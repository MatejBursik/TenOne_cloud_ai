2024-11-28 09:02:10,768:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:02:10,768:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:02:10,768:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:02:10,768:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:03:50,362:INFO:PyCaret ClassificationExperiment
2024-11-28 09:03:50,362:INFO:Logging name: clf-default-name
2024-11-28 09:03:50,363:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 09:03:50,363:INFO:version 3.3.2
2024-11-28 09:03:50,363:INFO:Initializing setup()
2024-11-28 09:03:50,363:INFO:self.USI: fab8
2024-11-28 09:03:50,363:INFO:self._variable_keys: {'fold_shuffle_param', 'idx', 'data', 'X_test', 'is_multiclass', 'y', 'pipeline', '_ml_usecase', 'gpu_n_jobs_param', 'logging_param', 'target_param', 'fold_groups_param', 'seed', 'fix_imbalance', 'gpu_param', 'X', 'X_train', 'fold_generator', 'USI', 'n_jobs_param', '_available_plots', 'exp_name_log', 'exp_id', 'log_plots_param', 'y_test', 'memory', 'y_train', 'html_param'}
2024-11-28 09:03:50,363:INFO:Checking environment
2024-11-28 09:03:50,363:INFO:python_version: 3.10.7
2024-11-28 09:03:50,363:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 09:03:50,363:INFO:machine: AMD64
2024-11-28 09:03:50,364:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 09:03:50,374:INFO:Memory: svmem(total=16934494208, available=2219737088, percent=86.9, used=14714757120, free=2219737088)
2024-11-28 09:03:50,374:INFO:Physical Core: 4
2024-11-28 09:03:50,374:INFO:Logical Core: 8
2024-11-28 09:03:50,374:INFO:Checking libraries
2024-11-28 09:03:50,374:INFO:System:
2024-11-28 09:03:50,374:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 09:03:50,374:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 09:03:50,374:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 09:03:50,374:INFO:PyCaret required dependencies:
2024-11-28 09:03:50,413:INFO:                 pip: 22.2.2
2024-11-28 09:03:50,413:INFO:          setuptools: 63.2.0
2024-11-28 09:03:50,413:INFO:             pycaret: 3.3.2
2024-11-28 09:03:50,413:INFO:             IPython: 8.28.0
2024-11-28 09:03:50,413:INFO:          ipywidgets: 8.1.5
2024-11-28 09:03:50,413:INFO:                tqdm: 4.66.6
2024-11-28 09:03:50,413:INFO:               numpy: 1.26.4
2024-11-28 09:03:50,413:INFO:              pandas: 2.1.4
2024-11-28 09:03:50,413:INFO:              jinja2: 3.1.4
2024-11-28 09:03:50,413:INFO:               scipy: 1.11.4
2024-11-28 09:03:50,413:INFO:              joblib: 1.4.2
2024-11-28 09:03:50,413:INFO:             sklearn: 1.4.2
2024-11-28 09:03:50,414:INFO:                pyod: 2.0.2
2024-11-28 09:03:50,414:INFO:            imblearn: 0.12.4
2024-11-28 09:03:50,414:INFO:   category_encoders: 2.6.4
2024-11-28 09:03:50,414:INFO:            lightgbm: 4.5.0
2024-11-28 09:03:50,414:INFO:               numba: 0.60.0
2024-11-28 09:03:50,414:INFO:            requests: 2.32.3
2024-11-28 09:03:50,414:INFO:          matplotlib: 3.7.5
2024-11-28 09:03:50,414:INFO:          scikitplot: 0.3.7
2024-11-28 09:03:50,414:INFO:         yellowbrick: 1.5
2024-11-28 09:03:50,414:INFO:              plotly: 5.24.1
2024-11-28 09:03:50,414:INFO:    plotly-resampler: Not installed
2024-11-28 09:03:50,414:INFO:             kaleido: 0.2.1
2024-11-28 09:03:50,414:INFO:           schemdraw: 0.15
2024-11-28 09:03:50,414:INFO:         statsmodels: 0.14.4
2024-11-28 09:03:50,414:INFO:              sktime: 0.26.0
2024-11-28 09:03:50,414:INFO:               tbats: 1.1.3
2024-11-28 09:03:50,414:INFO:            pmdarima: 2.0.4
2024-11-28 09:03:50,414:INFO:              psutil: 6.0.0
2024-11-28 09:03:50,414:INFO:          markupsafe: 3.0.2
2024-11-28 09:03:50,414:INFO:             pickle5: Not installed
2024-11-28 09:03:50,414:INFO:         cloudpickle: 3.1.0
2024-11-28 09:03:50,414:INFO:         deprecation: 2.1.0
2024-11-28 09:03:50,414:INFO:              xxhash: 3.5.0
2024-11-28 09:03:50,414:INFO:           wurlitzer: Not installed
2024-11-28 09:03:50,414:INFO:PyCaret optional dependencies:
2024-11-28 09:03:50,422:INFO:                shap: Not installed
2024-11-28 09:03:50,422:INFO:           interpret: Not installed
2024-11-28 09:03:50,422:INFO:                umap: Not installed
2024-11-28 09:03:50,422:INFO:     ydata_profiling: Not installed
2024-11-28 09:03:50,422:INFO:  explainerdashboard: Not installed
2024-11-28 09:03:50,422:INFO:             autoviz: Not installed
2024-11-28 09:03:50,422:INFO:           fairlearn: Not installed
2024-11-28 09:03:50,422:INFO:          deepchecks: Not installed
2024-11-28 09:03:50,422:INFO:             xgboost: Not installed
2024-11-28 09:03:50,422:INFO:            catboost: Not installed
2024-11-28 09:03:50,422:INFO:              kmodes: Not installed
2024-11-28 09:03:50,422:INFO:             mlxtend: Not installed
2024-11-28 09:03:50,422:INFO:       statsforecast: Not installed
2024-11-28 09:03:50,422:INFO:        tune_sklearn: Not installed
2024-11-28 09:03:50,422:INFO:                 ray: Not installed
2024-11-28 09:03:50,422:INFO:            hyperopt: Not installed
2024-11-28 09:03:50,422:INFO:              optuna: Not installed
2024-11-28 09:03:50,422:INFO:               skopt: Not installed
2024-11-28 09:03:50,422:INFO:              mlflow: Not installed
2024-11-28 09:03:50,422:INFO:              gradio: Not installed
2024-11-28 09:03:50,422:INFO:             fastapi: Not installed
2024-11-28 09:03:50,422:INFO:             uvicorn: Not installed
2024-11-28 09:03:50,422:INFO:              m2cgen: Not installed
2024-11-28 09:03:50,422:INFO:           evidently: Not installed
2024-11-28 09:03:50,422:INFO:               fugue: Not installed
2024-11-28 09:03:50,422:INFO:           streamlit: Not installed
2024-11-28 09:03:50,422:INFO:             prophet: Not installed
2024-11-28 09:03:50,422:INFO:None
2024-11-28 09:03:50,423:INFO:Set up data.
2024-11-28 09:03:50,430:INFO:Set up folding strategy.
2024-11-28 09:03:50,430:INFO:Set up train/test split.
2024-11-28 09:03:50,438:INFO:Set up index.
2024-11-28 09:03:50,438:INFO:Assigning column types.
2024-11-28 09:08:30,979:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:08:30,979:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:08:30,979:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 09:08:30,979:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:52:05,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:52:05,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:52:05,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:52:05,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:52:09,376:INFO:PyCaret ClassificationExperiment
2024-11-28 11:52:09,376:INFO:Logging name: clf-default-name
2024-11-28 11:52:09,376:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 11:52:09,376:INFO:version 3.3.2
2024-11-28 11:52:09,376:INFO:Initializing setup()
2024-11-28 11:52:09,376:INFO:self.USI: bad9
2024-11-28 11:52:09,376:INFO:self._variable_keys: {'_ml_usecase', 'X_train', 'y_train', 'is_multiclass', 'fold_groups_param', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'data', 'fix_imbalance', 'exp_id', 'fold_generator', '_available_plots', 'html_param', 'fold_shuffle_param', 'pipeline', 'log_plots_param', 'idx', 'X_test', 'memory', 'exp_name_log', 'seed', 'logging_param', 'X', 'y_test', 'gpu_param', 'y', 'target_param'}
2024-11-28 11:52:09,376:INFO:Checking environment
2024-11-28 11:52:09,376:INFO:python_version: 3.10.7
2024-11-28 11:52:09,376:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 11:52:09,376:INFO:machine: AMD64
2024-11-28 11:52:09,376:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 11:52:09,393:INFO:Memory: svmem(total=16934494208, available=2627231744, percent=84.5, used=14307262464, free=2627231744)
2024-11-28 11:52:09,393:INFO:Physical Core: 4
2024-11-28 11:52:09,393:INFO:Logical Core: 8
2024-11-28 11:52:09,394:INFO:Checking libraries
2024-11-28 11:52:09,394:INFO:System:
2024-11-28 11:52:09,394:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 11:52:09,394:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 11:52:09,394:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 11:52:09,394:INFO:PyCaret required dependencies:
2024-11-28 11:52:09,425:INFO:                 pip: 22.2.2
2024-11-28 11:52:09,425:INFO:          setuptools: 63.2.0
2024-11-28 11:52:09,425:INFO:             pycaret: 3.3.2
2024-11-28 11:52:09,425:INFO:             IPython: 8.28.0
2024-11-28 11:52:09,425:INFO:          ipywidgets: 8.1.5
2024-11-28 11:52:09,425:INFO:                tqdm: 4.66.6
2024-11-28 11:52:09,425:INFO:               numpy: 1.26.4
2024-11-28 11:52:09,425:INFO:              pandas: 2.1.4
2024-11-28 11:52:09,425:INFO:              jinja2: 3.1.4
2024-11-28 11:52:09,425:INFO:               scipy: 1.11.4
2024-11-28 11:52:09,425:INFO:              joblib: 1.4.2
2024-11-28 11:52:09,425:INFO:             sklearn: 1.4.2
2024-11-28 11:52:09,426:INFO:                pyod: 2.0.2
2024-11-28 11:52:09,426:INFO:            imblearn: 0.12.4
2024-11-28 11:52:09,426:INFO:   category_encoders: 2.6.4
2024-11-28 11:52:09,426:INFO:            lightgbm: 4.5.0
2024-11-28 11:52:09,426:INFO:               numba: 0.60.0
2024-11-28 11:52:09,426:INFO:            requests: 2.32.3
2024-11-28 11:52:09,426:INFO:          matplotlib: 3.7.5
2024-11-28 11:52:09,426:INFO:          scikitplot: 0.3.7
2024-11-28 11:52:09,426:INFO:         yellowbrick: 1.5
2024-11-28 11:52:09,426:INFO:              plotly: 5.24.1
2024-11-28 11:52:09,426:INFO:    plotly-resampler: Not installed
2024-11-28 11:52:09,426:INFO:             kaleido: 0.2.1
2024-11-28 11:52:09,426:INFO:           schemdraw: 0.15
2024-11-28 11:52:09,426:INFO:         statsmodels: 0.14.4
2024-11-28 11:52:09,426:INFO:              sktime: 0.26.0
2024-11-28 11:52:09,426:INFO:               tbats: 1.1.3
2024-11-28 11:52:09,426:INFO:            pmdarima: 2.0.4
2024-11-28 11:52:09,426:INFO:              psutil: 6.0.0
2024-11-28 11:52:09,426:INFO:          markupsafe: 3.0.2
2024-11-28 11:52:09,426:INFO:             pickle5: Not installed
2024-11-28 11:52:09,426:INFO:         cloudpickle: 3.1.0
2024-11-28 11:52:09,426:INFO:         deprecation: 2.1.0
2024-11-28 11:52:09,426:INFO:              xxhash: 3.5.0
2024-11-28 11:52:09,426:INFO:           wurlitzer: Not installed
2024-11-28 11:52:09,426:INFO:PyCaret optional dependencies:
2024-11-28 11:52:09,434:INFO:                shap: Not installed
2024-11-28 11:52:09,434:INFO:           interpret: Not installed
2024-11-28 11:52:09,434:INFO:                umap: Not installed
2024-11-28 11:52:09,434:INFO:     ydata_profiling: Not installed
2024-11-28 11:52:09,434:INFO:  explainerdashboard: Not installed
2024-11-28 11:52:09,434:INFO:             autoviz: Not installed
2024-11-28 11:52:09,434:INFO:           fairlearn: Not installed
2024-11-28 11:52:09,434:INFO:          deepchecks: Not installed
2024-11-28 11:52:09,434:INFO:             xgboost: Not installed
2024-11-28 11:52:09,434:INFO:            catboost: Not installed
2024-11-28 11:52:09,434:INFO:              kmodes: Not installed
2024-11-28 11:52:09,434:INFO:             mlxtend: Not installed
2024-11-28 11:52:09,434:INFO:       statsforecast: Not installed
2024-11-28 11:52:09,434:INFO:        tune_sklearn: Not installed
2024-11-28 11:52:09,434:INFO:                 ray: Not installed
2024-11-28 11:52:09,434:INFO:            hyperopt: Not installed
2024-11-28 11:52:09,434:INFO:              optuna: Not installed
2024-11-28 11:52:09,434:INFO:               skopt: Not installed
2024-11-28 11:52:09,434:INFO:              mlflow: Not installed
2024-11-28 11:52:09,434:INFO:              gradio: Not installed
2024-11-28 11:52:09,434:INFO:             fastapi: Not installed
2024-11-28 11:52:09,434:INFO:             uvicorn: Not installed
2024-11-28 11:52:09,434:INFO:              m2cgen: Not installed
2024-11-28 11:52:09,434:INFO:           evidently: Not installed
2024-11-28 11:52:09,434:INFO:               fugue: Not installed
2024-11-28 11:52:09,434:INFO:           streamlit: Not installed
2024-11-28 11:52:09,434:INFO:             prophet: Not installed
2024-11-28 11:52:09,434:INFO:None
2024-11-28 11:52:09,434:INFO:Set up data.
2024-11-28 11:52:09,441:INFO:Set up folding strategy.
2024-11-28 11:52:09,441:INFO:Set up train/test split.
2024-11-28 11:52:09,479:INFO:Set up index.
2024-11-28 11:52:09,479:INFO:Assigning column types.
2024-11-28 11:52:31,671:INFO:PyCaret ClassificationExperiment
2024-11-28 11:52:31,671:INFO:Logging name: clf-default-name
2024-11-28 11:52:31,671:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 11:52:31,671:INFO:version 3.3.2
2024-11-28 11:52:31,671:INFO:Initializing setup()
2024-11-28 11:52:31,671:INFO:self.USI: 6762
2024-11-28 11:52:31,671:INFO:self._variable_keys: {'_ml_usecase', 'X_train', 'y_train', 'is_multiclass', 'fold_groups_param', 'USI', 'n_jobs_param', 'gpu_n_jobs_param', 'data', 'fix_imbalance', 'exp_id', 'fold_generator', '_available_plots', 'html_param', 'fold_shuffle_param', 'pipeline', 'log_plots_param', 'idx', 'X_test', 'memory', 'exp_name_log', 'seed', 'logging_param', 'X', 'y_test', 'gpu_param', 'y', 'target_param'}
2024-11-28 11:52:31,671:INFO:Checking environment
2024-11-28 11:52:31,671:INFO:python_version: 3.10.7
2024-11-28 11:52:31,671:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 11:52:31,671:INFO:machine: AMD64
2024-11-28 11:52:31,671:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 11:52:31,681:INFO:Memory: svmem(total=16934494208, available=2582695936, percent=84.7, used=14351798272, free=2582695936)
2024-11-28 11:52:31,681:INFO:Physical Core: 4
2024-11-28 11:52:31,681:INFO:Logical Core: 8
2024-11-28 11:52:31,681:INFO:Checking libraries
2024-11-28 11:52:31,682:INFO:System:
2024-11-28 11:52:31,682:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 11:52:31,682:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 11:52:31,682:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 11:52:31,682:INFO:PyCaret required dependencies:
2024-11-28 11:52:31,682:INFO:                 pip: 22.2.2
2024-11-28 11:52:31,682:INFO:          setuptools: 63.2.0
2024-11-28 11:52:31,682:INFO:             pycaret: 3.3.2
2024-11-28 11:52:31,682:INFO:             IPython: 8.28.0
2024-11-28 11:52:31,682:INFO:          ipywidgets: 8.1.5
2024-11-28 11:52:31,682:INFO:                tqdm: 4.66.6
2024-11-28 11:52:31,682:INFO:               numpy: 1.26.4
2024-11-28 11:52:31,682:INFO:              pandas: 2.1.4
2024-11-28 11:52:31,683:INFO:              jinja2: 3.1.4
2024-11-28 11:52:31,683:INFO:               scipy: 1.11.4
2024-11-28 11:52:31,683:INFO:              joblib: 1.4.2
2024-11-28 11:52:31,683:INFO:             sklearn: 1.4.2
2024-11-28 11:52:31,683:INFO:                pyod: 2.0.2
2024-11-28 11:52:31,683:INFO:            imblearn: 0.12.4
2024-11-28 11:52:31,683:INFO:   category_encoders: 2.6.4
2024-11-28 11:52:31,683:INFO:            lightgbm: 4.5.0
2024-11-28 11:52:31,683:INFO:               numba: 0.60.0
2024-11-28 11:52:31,683:INFO:            requests: 2.32.3
2024-11-28 11:52:31,683:INFO:          matplotlib: 3.7.5
2024-11-28 11:52:31,683:INFO:          scikitplot: 0.3.7
2024-11-28 11:52:31,683:INFO:         yellowbrick: 1.5
2024-11-28 11:52:31,683:INFO:              plotly: 5.24.1
2024-11-28 11:52:31,683:INFO:    plotly-resampler: Not installed
2024-11-28 11:52:31,683:INFO:             kaleido: 0.2.1
2024-11-28 11:52:31,683:INFO:           schemdraw: 0.15
2024-11-28 11:52:31,683:INFO:         statsmodels: 0.14.4
2024-11-28 11:52:31,683:INFO:              sktime: 0.26.0
2024-11-28 11:52:31,683:INFO:               tbats: 1.1.3
2024-11-28 11:52:31,683:INFO:            pmdarima: 2.0.4
2024-11-28 11:52:31,683:INFO:              psutil: 6.0.0
2024-11-28 11:52:31,683:INFO:          markupsafe: 3.0.2
2024-11-28 11:52:31,683:INFO:             pickle5: Not installed
2024-11-28 11:52:31,683:INFO:         cloudpickle: 3.1.0
2024-11-28 11:52:31,683:INFO:         deprecation: 2.1.0
2024-11-28 11:52:31,683:INFO:              xxhash: 3.5.0
2024-11-28 11:52:31,683:INFO:           wurlitzer: Not installed
2024-11-28 11:52:31,683:INFO:PyCaret optional dependencies:
2024-11-28 11:52:31,683:INFO:                shap: Not installed
2024-11-28 11:52:31,683:INFO:           interpret: Not installed
2024-11-28 11:52:31,683:INFO:                umap: Not installed
2024-11-28 11:52:31,683:INFO:     ydata_profiling: Not installed
2024-11-28 11:52:31,683:INFO:  explainerdashboard: Not installed
2024-11-28 11:52:31,683:INFO:             autoviz: Not installed
2024-11-28 11:52:31,683:INFO:           fairlearn: Not installed
2024-11-28 11:52:31,683:INFO:          deepchecks: Not installed
2024-11-28 11:52:31,683:INFO:             xgboost: Not installed
2024-11-28 11:52:31,683:INFO:            catboost: Not installed
2024-11-28 11:52:31,683:INFO:              kmodes: Not installed
2024-11-28 11:52:31,683:INFO:             mlxtend: Not installed
2024-11-28 11:52:31,683:INFO:       statsforecast: Not installed
2024-11-28 11:52:31,683:INFO:        tune_sklearn: Not installed
2024-11-28 11:52:31,683:INFO:                 ray: Not installed
2024-11-28 11:52:31,683:INFO:            hyperopt: Not installed
2024-11-28 11:52:31,683:INFO:              optuna: Not installed
2024-11-28 11:52:31,683:INFO:               skopt: Not installed
2024-11-28 11:52:31,683:INFO:              mlflow: Not installed
2024-11-28 11:52:31,683:INFO:              gradio: Not installed
2024-11-28 11:52:31,683:INFO:             fastapi: Not installed
2024-11-28 11:52:31,683:INFO:             uvicorn: Not installed
2024-11-28 11:52:31,683:INFO:              m2cgen: Not installed
2024-11-28 11:52:31,683:INFO:           evidently: Not installed
2024-11-28 11:52:31,683:INFO:               fugue: Not installed
2024-11-28 11:52:31,683:INFO:           streamlit: Not installed
2024-11-28 11:52:31,683:INFO:             prophet: Not installed
2024-11-28 11:52:31,683:INFO:None
2024-11-28 11:52:31,683:INFO:Set up data.
2024-11-28 11:52:31,693:INFO:Set up folding strategy.
2024-11-28 11:52:31,693:INFO:Set up train/test split.
2024-11-28 11:52:31,697:INFO:Set up index.
2024-11-28 11:52:31,698:INFO:Assigning column types.
2024-11-28 11:59:46,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:59:46,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:59:46,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 11:59:46,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 12:15:03,909:INFO:PyCaret ClassificationExperiment
2024-11-28 12:15:03,909:INFO:Logging name: clf-default-name
2024-11-28 12:15:03,909:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 12:15:03,909:INFO:version 3.3.2
2024-11-28 12:15:03,910:INFO:Initializing setup()
2024-11-28 12:15:03,910:INFO:self.USI: b13b
2024-11-28 12:15:03,910:INFO:self._variable_keys: {'fold_groups_param', 'data', 'y', 'X_train', 'gpu_n_jobs_param', 'n_jobs_param', 'fix_imbalance', 'fold_shuffle_param', 'fold_generator', 'is_multiclass', 'X', 'y_test', 'html_param', 'pipeline', 'memory', 'seed', 'log_plots_param', 'gpu_param', 'idx', 'exp_name_log', 'logging_param', 'USI', 'X_test', '_ml_usecase', 'exp_id', 'target_param', 'y_train', '_available_plots'}
2024-11-28 12:15:03,910:INFO:Checking environment
2024-11-28 12:15:03,910:INFO:python_version: 3.10.7
2024-11-28 12:15:03,910:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 12:15:03,910:INFO:machine: AMD64
2024-11-28 12:15:03,910:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 12:15:03,922:INFO:Memory: svmem(total=16934494208, available=2265964544, percent=86.6, used=14668529664, free=2265964544)
2024-11-28 12:15:03,922:INFO:Physical Core: 4
2024-11-28 12:15:03,922:INFO:Logical Core: 8
2024-11-28 12:15:03,922:INFO:Checking libraries
2024-11-28 12:15:03,922:INFO:System:
2024-11-28 12:15:03,922:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 12:15:03,922:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 12:15:03,922:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 12:15:03,922:INFO:PyCaret required dependencies:
2024-11-28 12:15:03,959:INFO:                 pip: 22.2.2
2024-11-28 12:15:03,959:INFO:          setuptools: 63.2.0
2024-11-28 12:15:03,959:INFO:             pycaret: 3.3.2
2024-11-28 12:15:03,959:INFO:             IPython: 8.28.0
2024-11-28 12:15:03,959:INFO:          ipywidgets: 8.1.5
2024-11-28 12:15:03,959:INFO:                tqdm: 4.66.6
2024-11-28 12:15:03,959:INFO:               numpy: 1.26.4
2024-11-28 12:15:03,959:INFO:              pandas: 2.1.4
2024-11-28 12:15:03,959:INFO:              jinja2: 3.1.4
2024-11-28 12:15:03,959:INFO:               scipy: 1.11.4
2024-11-28 12:15:03,959:INFO:              joblib: 1.4.2
2024-11-28 12:15:03,959:INFO:             sklearn: 1.4.2
2024-11-28 12:15:03,959:INFO:                pyod: 2.0.2
2024-11-28 12:15:03,959:INFO:            imblearn: 0.12.4
2024-11-28 12:15:03,959:INFO:   category_encoders: 2.6.4
2024-11-28 12:15:03,959:INFO:            lightgbm: 4.5.0
2024-11-28 12:15:03,959:INFO:               numba: 0.60.0
2024-11-28 12:15:03,959:INFO:            requests: 2.32.3
2024-11-28 12:15:03,959:INFO:          matplotlib: 3.7.5
2024-11-28 12:15:03,959:INFO:          scikitplot: 0.3.7
2024-11-28 12:15:03,959:INFO:         yellowbrick: 1.5
2024-11-28 12:15:03,959:INFO:              plotly: 5.24.1
2024-11-28 12:15:03,959:INFO:    plotly-resampler: Not installed
2024-11-28 12:15:03,959:INFO:             kaleido: 0.2.1
2024-11-28 12:15:03,959:INFO:           schemdraw: 0.15
2024-11-28 12:15:03,959:INFO:         statsmodels: 0.14.4
2024-11-28 12:15:03,959:INFO:              sktime: 0.26.0
2024-11-28 12:15:03,959:INFO:               tbats: 1.1.3
2024-11-28 12:15:03,959:INFO:            pmdarima: 2.0.4
2024-11-28 12:15:03,959:INFO:              psutil: 6.0.0
2024-11-28 12:15:03,959:INFO:          markupsafe: 3.0.2
2024-11-28 12:15:03,959:INFO:             pickle5: Not installed
2024-11-28 12:15:03,959:INFO:         cloudpickle: 3.1.0
2024-11-28 12:15:03,959:INFO:         deprecation: 2.1.0
2024-11-28 12:15:03,959:INFO:              xxhash: 3.5.0
2024-11-28 12:15:03,959:INFO:           wurlitzer: Not installed
2024-11-28 12:15:03,959:INFO:PyCaret optional dependencies:
2024-11-28 12:15:03,980:INFO:                shap: Not installed
2024-11-28 12:15:03,980:INFO:           interpret: Not installed
2024-11-28 12:15:03,980:INFO:                umap: Not installed
2024-11-28 12:15:03,980:INFO:     ydata_profiling: Not installed
2024-11-28 12:15:03,980:INFO:  explainerdashboard: Not installed
2024-11-28 12:15:03,980:INFO:             autoviz: Not installed
2024-11-28 12:15:03,980:INFO:           fairlearn: Not installed
2024-11-28 12:15:03,980:INFO:          deepchecks: Not installed
2024-11-28 12:15:03,980:INFO:             xgboost: Not installed
2024-11-28 12:15:03,980:INFO:            catboost: Not installed
2024-11-28 12:15:03,980:INFO:              kmodes: Not installed
2024-11-28 12:15:03,980:INFO:             mlxtend: Not installed
2024-11-28 12:15:03,980:INFO:       statsforecast: Not installed
2024-11-28 12:15:03,980:INFO:        tune_sklearn: Not installed
2024-11-28 12:15:03,981:INFO:                 ray: Not installed
2024-11-28 12:15:03,981:INFO:            hyperopt: Not installed
2024-11-28 12:15:03,981:INFO:              optuna: Not installed
2024-11-28 12:15:03,981:INFO:               skopt: Not installed
2024-11-28 12:15:03,981:INFO:              mlflow: Not installed
2024-11-28 12:15:03,981:INFO:              gradio: Not installed
2024-11-28 12:15:03,981:INFO:             fastapi: Not installed
2024-11-28 12:15:03,981:INFO:             uvicorn: Not installed
2024-11-28 12:15:03,981:INFO:              m2cgen: Not installed
2024-11-28 12:15:03,981:INFO:           evidently: Not installed
2024-11-28 12:15:03,981:INFO:               fugue: Not installed
2024-11-28 12:15:03,981:INFO:           streamlit: Not installed
2024-11-28 12:15:03,981:INFO:             prophet: Not installed
2024-11-28 12:15:03,981:INFO:None
2024-11-28 12:15:03,981:INFO:Set up data.
2024-11-28 12:15:03,992:INFO:Set up folding strategy.
2024-11-28 12:15:03,993:INFO:Set up train/test split.
2024-11-28 12:15:04,001:INFO:Set up index.
2024-11-28 12:15:04,001:INFO:Assigning column types.
2024-11-28 12:15:04,004:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 12:15:04,038:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 12:15:04,059:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 12:15:04,087:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,153:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 12:15:04,156:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 12:15:04,172:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,172:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,172:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 12:15:04,235:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 12:15:04,271:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,271:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,319:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 12:15:04,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,350:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,350:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 12:15:04,429:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,430:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,512:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,512:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,512:INFO:Preparing preprocessing pipeline...
2024-11-28 12:15:04,522:INFO:Set up simple imputation.
2024-11-28 12:15:04,525:INFO:Set up encoding of categorical features.
2024-11-28 12:15:04,525:INFO:Set up column transformation.
2024-11-28 12:15:04,525:INFO:Set up feature normalization.
2024-11-28 12:15:04,670:INFO:Finished creating preprocessing pipeline.
2024-11-28 12:15:04,682:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 12:15:04,682:INFO:Creating final display dataframe.
2024-11-28 12:15:04,870:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 15)
4        Transformed data shape         (891, 18)
5   Transformed train set shape         (623, 18)
6    Transformed test set shape         (268, 18)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 1
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Transformation              True
17        Transformation method       yeo-johnson
18                    Normalize              True
19             Normalize method            zscore
20               Fold Generator   StratifiedKFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  clf-default-name
26                          USI              b13b
2024-11-28 12:15:04,962:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:04,963:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:05,016:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:05,016:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 12:15:05,016:INFO:setup() successfully completed in 1.12s...............
2024-11-28 12:15:05,055:INFO:Initializing compare_models()
2024-11-28 12:15:05,055:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 12:15:05,056:INFO:Checking exceptions
2024-11-28 12:15:05,056:INFO:Preparing display monitor
2024-11-28 12:15:05,099:INFO:Initializing Logistic Regression
2024-11-28 12:15:05,099:INFO:Total runtime is 0.0 minutes
2024-11-28 12:15:05,104:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:05,105:INFO:Initializing create_model()
2024-11-28 12:15:05,105:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:05,105:INFO:Checking exceptions
2024-11-28 12:15:05,105:INFO:Importing libraries
2024-11-28 12:15:05,105:INFO:Copying training dataset
2024-11-28 12:15:05,113:INFO:Defining folds
2024-11-28 12:15:05,113:INFO:Declaring metric variables
2024-11-28 12:15:05,116:INFO:Importing untrained model
2024-11-28 12:15:05,164:INFO:Logistic Regression Imported successfully
2024-11-28 12:15:05,193:INFO:Starting cross validation
2024-11-28 12:15:05,194:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:11,414:INFO:Calculating mean and std
2024-11-28 12:15:11,414:INFO:Creating metrics dataframe
2024-11-28 12:15:11,477:INFO:Uploading results into container
2024-11-28 12:15:11,477:INFO:Uploading model into container now
2024-11-28 12:15:11,477:INFO:_master_model_container: 1
2024-11-28 12:15:11,477:INFO:_display_container: 2
2024-11-28 12:15:11,477:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 12:15:11,477:INFO:create_model() successfully completed......................................
2024-11-28 12:15:11,556:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:11,556:INFO:Creating metrics dataframe
2024-11-28 12:15:11,571:INFO:Initializing K Neighbors Classifier
2024-11-28 12:15:11,575:INFO:Total runtime is 0.10786092678705851 minutes
2024-11-28 12:15:11,579:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:11,580:INFO:Initializing create_model()
2024-11-28 12:15:11,580:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:11,580:INFO:Checking exceptions
2024-11-28 12:15:11,580:INFO:Importing libraries
2024-11-28 12:15:11,580:INFO:Copying training dataset
2024-11-28 12:15:11,587:INFO:Defining folds
2024-11-28 12:15:11,587:INFO:Declaring metric variables
2024-11-28 12:15:11,591:INFO:Importing untrained model
2024-11-28 12:15:11,595:INFO:K Neighbors Classifier Imported successfully
2024-11-28 12:15:11,605:INFO:Starting cross validation
2024-11-28 12:15:11,607:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:12,458:INFO:Calculating mean and std
2024-11-28 12:15:12,459:INFO:Creating metrics dataframe
2024-11-28 12:15:12,461:INFO:Uploading results into container
2024-11-28 12:15:12,461:INFO:Uploading model into container now
2024-11-28 12:15:12,461:INFO:_master_model_container: 2
2024-11-28 12:15:12,461:INFO:_display_container: 2
2024-11-28 12:15:12,462:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 12:15:12,462:INFO:create_model() successfully completed......................................
2024-11-28 12:15:12,524:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:12,530:INFO:Creating metrics dataframe
2024-11-28 12:15:12,538:INFO:Initializing Naive Bayes
2024-11-28 12:15:12,538:INFO:Total runtime is 0.12398945887883504 minutes
2024-11-28 12:15:12,540:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:12,541:INFO:Initializing create_model()
2024-11-28 12:15:12,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:12,541:INFO:Checking exceptions
2024-11-28 12:15:12,541:INFO:Importing libraries
2024-11-28 12:15:12,541:INFO:Copying training dataset
2024-11-28 12:15:12,544:INFO:Defining folds
2024-11-28 12:15:12,544:INFO:Declaring metric variables
2024-11-28 12:15:12,548:INFO:Importing untrained model
2024-11-28 12:15:12,552:INFO:Naive Bayes Imported successfully
2024-11-28 12:15:12,557:INFO:Starting cross validation
2024-11-28 12:15:12,559:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:12,849:INFO:Calculating mean and std
2024-11-28 12:15:12,850:INFO:Creating metrics dataframe
2024-11-28 12:15:12,851:INFO:Uploading results into container
2024-11-28 12:15:12,852:INFO:Uploading model into container now
2024-11-28 12:15:12,852:INFO:_master_model_container: 3
2024-11-28 12:15:12,852:INFO:_display_container: 2
2024-11-28 12:15:12,852:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 12:15:12,852:INFO:create_model() successfully completed......................................
2024-11-28 12:15:12,923:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:12,923:INFO:Creating metrics dataframe
2024-11-28 12:15:12,929:INFO:Initializing Decision Tree Classifier
2024-11-28 12:15:12,929:INFO:Total runtime is 0.1305056651433309 minutes
2024-11-28 12:15:12,933:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:12,933:INFO:Initializing create_model()
2024-11-28 12:15:12,933:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:12,933:INFO:Checking exceptions
2024-11-28 12:15:12,933:INFO:Importing libraries
2024-11-28 12:15:12,933:INFO:Copying training dataset
2024-11-28 12:15:12,933:INFO:Defining folds
2024-11-28 12:15:12,933:INFO:Declaring metric variables
2024-11-28 12:15:12,933:INFO:Importing untrained model
2024-11-28 12:15:12,933:INFO:Decision Tree Classifier Imported successfully
2024-11-28 12:15:12,948:INFO:Starting cross validation
2024-11-28 12:15:12,948:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:13,244:INFO:Calculating mean and std
2024-11-28 12:15:13,245:INFO:Creating metrics dataframe
2024-11-28 12:15:13,249:INFO:Uploading results into container
2024-11-28 12:15:13,250:INFO:Uploading model into container now
2024-11-28 12:15:13,251:INFO:_master_model_container: 4
2024-11-28 12:15:13,251:INFO:_display_container: 2
2024-11-28 12:15:13,251:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 12:15:13,252:INFO:create_model() successfully completed......................................
2024-11-28 12:15:13,323:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:13,323:INFO:Creating metrics dataframe
2024-11-28 12:15:13,329:INFO:Initializing SVM - Linear Kernel
2024-11-28 12:15:13,329:INFO:Total runtime is 0.13716889222462972 minutes
2024-11-28 12:15:13,332:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:13,333:INFO:Initializing create_model()
2024-11-28 12:15:13,333:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:13,333:INFO:Checking exceptions
2024-11-28 12:15:13,333:INFO:Importing libraries
2024-11-28 12:15:13,333:INFO:Copying training dataset
2024-11-28 12:15:13,336:INFO:Defining folds
2024-11-28 12:15:13,337:INFO:Declaring metric variables
2024-11-28 12:15:13,339:INFO:Importing untrained model
2024-11-28 12:15:13,342:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 12:15:13,347:INFO:Starting cross validation
2024-11-28 12:15:13,347:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:13,626:INFO:Calculating mean and std
2024-11-28 12:15:13,628:INFO:Creating metrics dataframe
2024-11-28 12:15:13,628:INFO:Uploading results into container
2024-11-28 12:15:13,628:INFO:Uploading model into container now
2024-11-28 12:15:13,628:INFO:_master_model_container: 5
2024-11-28 12:15:13,628:INFO:_display_container: 2
2024-11-28 12:15:13,628:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 12:15:13,628:INFO:create_model() successfully completed......................................
2024-11-28 12:15:13,701:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:13,701:INFO:Creating metrics dataframe
2024-11-28 12:15:13,711:INFO:Initializing Ridge Classifier
2024-11-28 12:15:13,711:INFO:Total runtime is 0.14353988965352377 minutes
2024-11-28 12:15:13,712:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:13,712:INFO:Initializing create_model()
2024-11-28 12:15:13,712:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:13,712:INFO:Checking exceptions
2024-11-28 12:15:13,712:INFO:Importing libraries
2024-11-28 12:15:13,712:INFO:Copying training dataset
2024-11-28 12:15:13,712:INFO:Defining folds
2024-11-28 12:15:13,712:INFO:Declaring metric variables
2024-11-28 12:15:13,712:INFO:Importing untrained model
2024-11-28 12:15:13,724:INFO:Ridge Classifier Imported successfully
2024-11-28 12:15:13,732:INFO:Starting cross validation
2024-11-28 12:15:13,733:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:14,043:INFO:Calculating mean and std
2024-11-28 12:15:14,045:INFO:Creating metrics dataframe
2024-11-28 12:15:14,047:INFO:Uploading results into container
2024-11-28 12:15:14,047:INFO:Uploading model into container now
2024-11-28 12:15:14,047:INFO:_master_model_container: 6
2024-11-28 12:15:14,047:INFO:_display_container: 2
2024-11-28 12:15:14,047:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 12:15:14,047:INFO:create_model() successfully completed......................................
2024-11-28 12:15:14,109:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:14,109:INFO:Creating metrics dataframe
2024-11-28 12:15:14,109:INFO:Initializing Random Forest Classifier
2024-11-28 12:15:14,109:INFO:Total runtime is 0.15016132593154907 minutes
2024-11-28 12:15:14,109:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:14,109:INFO:Initializing create_model()
2024-11-28 12:15:14,109:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:14,109:INFO:Checking exceptions
2024-11-28 12:15:14,109:INFO:Importing libraries
2024-11-28 12:15:14,109:INFO:Copying training dataset
2024-11-28 12:15:14,125:INFO:Defining folds
2024-11-28 12:15:14,125:INFO:Declaring metric variables
2024-11-28 12:15:14,125:INFO:Importing untrained model
2024-11-28 12:15:14,125:INFO:Random Forest Classifier Imported successfully
2024-11-28 12:15:14,125:INFO:Starting cross validation
2024-11-28 12:15:14,140:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:15,003:INFO:Calculating mean and std
2024-11-28 12:15:15,003:INFO:Creating metrics dataframe
2024-11-28 12:15:15,007:INFO:Uploading results into container
2024-11-28 12:15:15,008:INFO:Uploading model into container now
2024-11-28 12:15:15,008:INFO:_master_model_container: 7
2024-11-28 12:15:15,008:INFO:_display_container: 2
2024-11-28 12:15:15,008:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 12:15:15,008:INFO:create_model() successfully completed......................................
2024-11-28 12:15:15,078:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:15,078:INFO:Creating metrics dataframe
2024-11-28 12:15:15,078:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 12:15:15,078:INFO:Total runtime is 0.1663216511408488 minutes
2024-11-28 12:15:15,093:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:15,093:INFO:Initializing create_model()
2024-11-28 12:15:15,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:15,094:INFO:Checking exceptions
2024-11-28 12:15:15,094:INFO:Importing libraries
2024-11-28 12:15:15,094:INFO:Copying training dataset
2024-11-28 12:15:15,097:INFO:Defining folds
2024-11-28 12:15:15,097:INFO:Declaring metric variables
2024-11-28 12:15:15,100:INFO:Importing untrained model
2024-11-28 12:15:15,102:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 12:15:15,111:INFO:Starting cross validation
2024-11-28 12:15:15,113:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:15,279:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,279:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,279:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,279:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,279:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,280:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,428:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,429:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 12:15:15,455:INFO:Calculating mean and std
2024-11-28 12:15:15,456:INFO:Creating metrics dataframe
2024-11-28 12:15:15,459:INFO:Uploading results into container
2024-11-28 12:15:15,460:INFO:Uploading model into container now
2024-11-28 12:15:15,461:INFO:_master_model_container: 8
2024-11-28 12:15:15,461:INFO:_display_container: 2
2024-11-28 12:15:15,461:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 12:15:15,461:INFO:create_model() successfully completed......................................
2024-11-28 12:15:15,532:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:15,532:INFO:Creating metrics dataframe
2024-11-28 12:15:15,537:INFO:Initializing Ada Boost Classifier
2024-11-28 12:15:15,538:INFO:Total runtime is 0.17399186293284097 minutes
2024-11-28 12:15:15,542:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:15,543:INFO:Initializing create_model()
2024-11-28 12:15:15,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:15,543:INFO:Checking exceptions
2024-11-28 12:15:15,543:INFO:Importing libraries
2024-11-28 12:15:15,543:INFO:Copying training dataset
2024-11-28 12:15:15,547:INFO:Defining folds
2024-11-28 12:15:15,547:INFO:Declaring metric variables
2024-11-28 12:15:15,549:INFO:Importing untrained model
2024-11-28 12:15:15,553:INFO:Ada Boost Classifier Imported successfully
2024-11-28 12:15:15,560:INFO:Starting cross validation
2024-11-28 12:15:15,562:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:15,703:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:15,703:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:15,703:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:15,703:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:15,703:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:15,991:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:15,991:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 12:15:16,121:INFO:Calculating mean and std
2024-11-28 12:15:16,122:INFO:Creating metrics dataframe
2024-11-28 12:15:16,124:INFO:Uploading results into container
2024-11-28 12:15:16,125:INFO:Uploading model into container now
2024-11-28 12:15:16,125:INFO:_master_model_container: 9
2024-11-28 12:15:16,125:INFO:_display_container: 2
2024-11-28 12:15:16,126:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 12:15:16,126:INFO:create_model() successfully completed......................................
2024-11-28 12:15:16,190:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:16,190:INFO:Creating metrics dataframe
2024-11-28 12:15:16,199:INFO:Initializing Gradient Boosting Classifier
2024-11-28 12:15:16,199:INFO:Total runtime is 0.184997022151947 minutes
2024-11-28 12:15:16,201:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:16,202:INFO:Initializing create_model()
2024-11-28 12:15:16,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:16,202:INFO:Checking exceptions
2024-11-28 12:15:16,202:INFO:Importing libraries
2024-11-28 12:15:16,202:INFO:Copying training dataset
2024-11-28 12:15:16,205:INFO:Defining folds
2024-11-28 12:15:16,205:INFO:Declaring metric variables
2024-11-28 12:15:16,205:INFO:Importing untrained model
2024-11-28 12:15:16,205:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 12:15:16,205:INFO:Starting cross validation
2024-11-28 12:15:16,205:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:16,971:INFO:Calculating mean and std
2024-11-28 12:15:16,973:INFO:Creating metrics dataframe
2024-11-28 12:15:16,975:INFO:Uploading results into container
2024-11-28 12:15:16,975:INFO:Uploading model into container now
2024-11-28 12:15:16,976:INFO:_master_model_container: 10
2024-11-28 12:15:16,976:INFO:_display_container: 2
2024-11-28 12:15:16,976:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 12:15:16,976:INFO:create_model() successfully completed......................................
2024-11-28 12:15:17,126:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:17,126:INFO:Creating metrics dataframe
2024-11-28 12:15:17,145:INFO:Initializing Linear Discriminant Analysis
2024-11-28 12:15:17,146:INFO:Total runtime is 0.20079175631205237 minutes
2024-11-28 12:15:17,155:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:17,156:INFO:Initializing create_model()
2024-11-28 12:15:17,156:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:17,156:INFO:Checking exceptions
2024-11-28 12:15:17,156:INFO:Importing libraries
2024-11-28 12:15:17,156:INFO:Copying training dataset
2024-11-28 12:15:17,167:INFO:Defining folds
2024-11-28 12:15:17,168:INFO:Declaring metric variables
2024-11-28 12:15:17,176:INFO:Importing untrained model
2024-11-28 12:15:17,187:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 12:15:17,199:INFO:Starting cross validation
2024-11-28 12:15:17,202:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:17,586:INFO:Calculating mean and std
2024-11-28 12:15:17,587:INFO:Creating metrics dataframe
2024-11-28 12:15:17,589:INFO:Uploading results into container
2024-11-28 12:15:17,590:INFO:Uploading model into container now
2024-11-28 12:15:17,590:INFO:_master_model_container: 11
2024-11-28 12:15:17,590:INFO:_display_container: 2
2024-11-28 12:15:17,591:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 12:15:17,591:INFO:create_model() successfully completed......................................
2024-11-28 12:15:17,674:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:17,674:INFO:Creating metrics dataframe
2024-11-28 12:15:17,683:INFO:Initializing Extra Trees Classifier
2024-11-28 12:15:17,683:INFO:Total runtime is 0.2097358187039693 minutes
2024-11-28 12:15:17,688:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:17,689:INFO:Initializing create_model()
2024-11-28 12:15:17,689:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:17,689:INFO:Checking exceptions
2024-11-28 12:15:17,689:INFO:Importing libraries
2024-11-28 12:15:17,689:INFO:Copying training dataset
2024-11-28 12:15:17,693:INFO:Defining folds
2024-11-28 12:15:17,693:INFO:Declaring metric variables
2024-11-28 12:15:17,698:INFO:Importing untrained model
2024-11-28 12:15:17,702:INFO:Extra Trees Classifier Imported successfully
2024-11-28 12:15:17,702:INFO:Starting cross validation
2024-11-28 12:15:17,702:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:18,612:INFO:Calculating mean and std
2024-11-28 12:15:18,614:INFO:Creating metrics dataframe
2024-11-28 12:15:18,620:INFO:Uploading results into container
2024-11-28 12:15:18,621:INFO:Uploading model into container now
2024-11-28 12:15:18,621:INFO:_master_model_container: 12
2024-11-28 12:15:18,621:INFO:_display_container: 2
2024-11-28 12:15:18,622:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 12:15:18,622:INFO:create_model() successfully completed......................................
2024-11-28 12:15:18,698:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:18,699:INFO:Creating metrics dataframe
2024-11-28 12:15:18,710:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 12:15:18,710:INFO:Total runtime is 0.22685591379801429 minutes
2024-11-28 12:15:18,714:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:18,714:INFO:Initializing create_model()
2024-11-28 12:15:18,714:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:18,714:INFO:Checking exceptions
2024-11-28 12:15:18,715:INFO:Importing libraries
2024-11-28 12:15:18,715:INFO:Copying training dataset
2024-11-28 12:15:18,718:INFO:Defining folds
2024-11-28 12:15:18,718:INFO:Declaring metric variables
2024-11-28 12:15:18,766:INFO:Importing untrained model
2024-11-28 12:15:18,852:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 12:15:18,899:INFO:Starting cross validation
2024-11-28 12:15:18,901:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:20,723:INFO:Calculating mean and std
2024-11-28 12:15:20,726:INFO:Creating metrics dataframe
2024-11-28 12:15:20,730:INFO:Uploading results into container
2024-11-28 12:15:20,731:INFO:Uploading model into container now
2024-11-28 12:15:20,731:INFO:_master_model_container: 13
2024-11-28 12:15:20,732:INFO:_display_container: 2
2024-11-28 12:15:20,732:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 12:15:20,733:INFO:create_model() successfully completed......................................
2024-11-28 12:15:20,845:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:20,845:INFO:Creating metrics dataframe
2024-11-28 12:15:20,853:INFO:Initializing Dummy Classifier
2024-11-28 12:15:20,853:INFO:Total runtime is 0.26256681680679317 minutes
2024-11-28 12:15:20,856:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:20,857:INFO:Initializing create_model()
2024-11-28 12:15:20,857:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002115C330A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:20,857:INFO:Checking exceptions
2024-11-28 12:15:20,857:INFO:Importing libraries
2024-11-28 12:15:20,858:INFO:Copying training dataset
2024-11-28 12:15:20,864:INFO:Defining folds
2024-11-28 12:15:20,864:INFO:Declaring metric variables
2024-11-28 12:15:20,868:INFO:Importing untrained model
2024-11-28 12:15:20,875:INFO:Dummy Classifier Imported successfully
2024-11-28 12:15:20,883:INFO:Starting cross validation
2024-11-28 12:15:20,885:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:21,112:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,112:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,113:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,126:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,140:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,225:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,225:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 12:15:21,239:INFO:Calculating mean and std
2024-11-28 12:15:21,240:INFO:Creating metrics dataframe
2024-11-28 12:15:21,243:INFO:Uploading results into container
2024-11-28 12:15:21,243:INFO:Uploading model into container now
2024-11-28 12:15:21,244:INFO:_master_model_container: 14
2024-11-28 12:15:21,244:INFO:_display_container: 2
2024-11-28 12:15:21,244:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 12:15:21,244:INFO:create_model() successfully completed......................................
2024-11-28 12:15:21,328:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:21,328:INFO:Creating metrics dataframe
2024-11-28 12:15:21,389:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 12:15:21,406:INFO:Initializing create_model()
2024-11-28 12:15:21,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:21,406:INFO:Checking exceptions
2024-11-28 12:15:21,409:INFO:Importing libraries
2024-11-28 12:15:21,410:INFO:Copying training dataset
2024-11-28 12:15:21,419:INFO:Defining folds
2024-11-28 12:15:21,419:INFO:Declaring metric variables
2024-11-28 12:15:21,419:INFO:Importing untrained model
2024-11-28 12:15:21,419:INFO:Declaring custom model
2024-11-28 12:15:21,420:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 12:15:21,421:INFO:Cross validation set to False
2024-11-28 12:15:21,422:INFO:Fitting Model
2024-11-28 12:15:21,633:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 12:15:21,633:INFO:create_model() successfully completed......................................
2024-11-28 12:15:21,741:INFO:_master_model_container: 14
2024-11-28 12:15:21,741:INFO:_display_container: 2
2024-11-28 12:15:21,742:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 12:15:21,742:INFO:compare_models() successfully completed......................................
2024-11-28 12:15:21,862:INFO:Initializing tune_model()
2024-11-28 12:15:21,862:INFO:tune_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>)
2024-11-28 12:15:21,862:INFO:Checking exceptions
2024-11-28 12:15:21,882:INFO:Copying training dataset
2024-11-28 12:15:21,886:INFO:Checking base model
2024-11-28 12:15:21,887:INFO:Base model : Gradient Boosting Classifier
2024-11-28 12:15:21,916:INFO:Declaring metric variables
2024-11-28 12:15:21,951:INFO:Defining Hyperparameters
2024-11-28 12:15:22,115:INFO:Tuning with n_jobs=-1
2024-11-28 12:15:22,115:INFO:Initializing RandomizedSearchCV
2024-11-28 12:15:30,748:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__n_estimators': 270, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.3, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 6, 'actual_estimator__learning_rate': 0.05}
2024-11-28 12:15:30,750:INFO:Hyperparameter search completed
2024-11-28 12:15:30,750:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:30,751:INFO:Initializing create_model()
2024-11-28 12:15:30,751:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002117F415660>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'n_estimators': 270, 'min_samples_split': 10, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.3, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.05})
2024-11-28 12:15:30,751:INFO:Checking exceptions
2024-11-28 12:15:30,751:INFO:Importing libraries
2024-11-28 12:15:30,751:INFO:Copying training dataset
2024-11-28 12:15:30,757:INFO:Defining folds
2024-11-28 12:15:30,757:INFO:Declaring metric variables
2024-11-28 12:15:30,762:INFO:Importing untrained model
2024-11-28 12:15:30,762:INFO:Declaring custom model
2024-11-28 12:15:30,768:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 12:15:30,775:INFO:Starting cross validation
2024-11-28 12:15:30,777:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:31,760:INFO:Calculating mean and std
2024-11-28 12:15:31,761:INFO:Creating metrics dataframe
2024-11-28 12:15:31,766:INFO:Finalizing model
2024-11-28 12:15:32,038:INFO:Uploading results into container
2024-11-28 12:15:32,040:INFO:Uploading model into container now
2024-11-28 12:15:32,041:INFO:_master_model_container: 15
2024-11-28 12:15:32,041:INFO:_display_container: 3
2024-11-28 12:15:32,041:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 12:15:32,041:INFO:create_model() successfully completed......................................
2024-11-28 12:15:32,124:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:32,124:INFO:choose_better activated
2024-11-28 12:15:32,127:INFO:SubProcess create_model() called ==================================
2024-11-28 12:15:32,128:INFO:Initializing create_model()
2024-11-28 12:15:32,129:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 12:15:32,129:INFO:Checking exceptions
2024-11-28 12:15:32,130:INFO:Importing libraries
2024-11-28 12:15:32,131:INFO:Copying training dataset
2024-11-28 12:15:32,136:INFO:Defining folds
2024-11-28 12:15:32,136:INFO:Declaring metric variables
2024-11-28 12:15:32,136:INFO:Importing untrained model
2024-11-28 12:15:32,136:INFO:Declaring custom model
2024-11-28 12:15:32,137:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 12:15:32,137:INFO:Starting cross validation
2024-11-28 12:15:32,139:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 12:15:32,927:INFO:Calculating mean and std
2024-11-28 12:15:32,927:INFO:Creating metrics dataframe
2024-11-28 12:15:32,929:INFO:Finalizing model
2024-11-28 12:15:33,115:INFO:Uploading results into container
2024-11-28 12:15:33,116:INFO:Uploading model into container now
2024-11-28 12:15:33,116:INFO:_master_model_container: 16
2024-11-28 12:15:33,116:INFO:_display_container: 4
2024-11-28 12:15:33,116:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 12:15:33,116:INFO:create_model() successfully completed......................................
2024-11-28 12:15:33,199:INFO:SubProcess create_model() end ==================================
2024-11-28 12:15:33,199:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8331
2024-11-28 12:15:33,200:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8378
2024-11-28 12:15:33,200:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2024-11-28 12:15:33,200:INFO:choose_better completed
2024-11-28 12:15:33,210:INFO:_master_model_container: 16
2024-11-28 12:15:33,210:INFO:_display_container: 3
2024-11-28 12:15:33,210:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 12:15:33,211:INFO:tune_model() successfully completed......................................
2024-11-28 12:15:38,463:INFO:Initializing evaluate_model()
2024-11-28 12:15:38,463:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-28 12:15:38,508:INFO:Initializing plot_model()
2024-11-28 12:15:38,508:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002115C330730>, system=True)
2024-11-28 12:15:38,508:INFO:Checking exceptions
2024-11-28 12:15:38,515:INFO:Preloading libraries
2024-11-28 12:15:38,535:INFO:Copying training dataset
2024-11-28 12:15:38,535:INFO:Plot type: pipeline
2024-11-28 12:15:38,939:INFO:Visual Rendered Successfully
2024-11-28 12:15:39,017:INFO:plot_model() successfully completed......................................
2024-11-28 13:51:59,165:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 13:51:59,165:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 13:51:59,165:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 13:51:59,165:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 13:52:00,115:INFO:PyCaret ClassificationExperiment
2024-11-28 13:52:00,115:INFO:Logging name: clf-default-name
2024-11-28 13:52:00,115:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 13:52:00,115:INFO:version 3.3.2
2024-11-28 13:52:00,115:INFO:Initializing setup()
2024-11-28 13:52:00,115:INFO:self.USI: 582d
2024-11-28 13:52:00,115:INFO:self._variable_keys: {'is_multiclass', 'exp_name_log', 'X_test', 'USI', 'y_train', 'n_jobs_param', 'html_param', 'logging_param', 'data', '_ml_usecase', 'gpu_param', 'gpu_n_jobs_param', 'y_test', 'memory', 'pipeline', 'log_plots_param', 'fix_imbalance', 'fold_shuffle_param', 'seed', 'fold_groups_param', 'target_param', 'X', 'idx', 'y', 'X_train', 'exp_id', 'fold_generator', '_available_plots'}
2024-11-28 13:52:00,115:INFO:Checking environment
2024-11-28 13:52:00,115:INFO:python_version: 3.10.7
2024-11-28 13:52:00,115:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 13:52:00,115:INFO:machine: AMD64
2024-11-28 13:52:00,116:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 13:52:00,127:INFO:Memory: svmem(total=16934494208, available=1602678784, percent=90.5, used=15331815424, free=1602678784)
2024-11-28 13:52:00,127:INFO:Physical Core: 4
2024-11-28 13:52:00,127:INFO:Logical Core: 8
2024-11-28 13:52:00,127:INFO:Checking libraries
2024-11-28 13:52:00,127:INFO:System:
2024-11-28 13:52:00,127:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 13:52:00,127:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 13:52:00,127:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 13:52:00,127:INFO:PyCaret required dependencies:
2024-11-28 13:52:00,156:INFO:                 pip: 22.2.2
2024-11-28 13:52:00,157:INFO:          setuptools: 63.2.0
2024-11-28 13:52:00,157:INFO:             pycaret: 3.3.2
2024-11-28 13:52:00,157:INFO:             IPython: 8.28.0
2024-11-28 13:52:00,157:INFO:          ipywidgets: 8.1.5
2024-11-28 13:52:00,157:INFO:                tqdm: 4.66.6
2024-11-28 13:52:00,157:INFO:               numpy: 1.26.4
2024-11-28 13:52:00,157:INFO:              pandas: 2.1.4
2024-11-28 13:52:00,157:INFO:              jinja2: 3.1.4
2024-11-28 13:52:00,157:INFO:               scipy: 1.11.4
2024-11-28 13:52:00,157:INFO:              joblib: 1.4.2
2024-11-28 13:52:00,157:INFO:             sklearn: 1.4.2
2024-11-28 13:52:00,157:INFO:                pyod: 2.0.2
2024-11-28 13:52:00,157:INFO:            imblearn: 0.12.4
2024-11-28 13:52:00,157:INFO:   category_encoders: 2.6.4
2024-11-28 13:52:00,157:INFO:            lightgbm: 4.5.0
2024-11-28 13:52:00,157:INFO:               numba: 0.60.0
2024-11-28 13:52:00,157:INFO:            requests: 2.32.3
2024-11-28 13:52:00,157:INFO:          matplotlib: 3.7.5
2024-11-28 13:52:00,157:INFO:          scikitplot: 0.3.7
2024-11-28 13:52:00,157:INFO:         yellowbrick: 1.5
2024-11-28 13:52:00,157:INFO:              plotly: 5.24.1
2024-11-28 13:52:00,157:INFO:    plotly-resampler: Not installed
2024-11-28 13:52:00,157:INFO:             kaleido: 0.2.1
2024-11-28 13:52:00,157:INFO:           schemdraw: 0.15
2024-11-28 13:52:00,157:INFO:         statsmodels: 0.14.4
2024-11-28 13:52:00,157:INFO:              sktime: 0.26.0
2024-11-28 13:52:00,157:INFO:               tbats: 1.1.3
2024-11-28 13:52:00,157:INFO:            pmdarima: 2.0.4
2024-11-28 13:52:00,157:INFO:              psutil: 6.0.0
2024-11-28 13:52:00,158:INFO:          markupsafe: 3.0.2
2024-11-28 13:52:00,158:INFO:             pickle5: Not installed
2024-11-28 13:52:00,158:INFO:         cloudpickle: 3.1.0
2024-11-28 13:52:00,158:INFO:         deprecation: 2.1.0
2024-11-28 13:52:00,158:INFO:              xxhash: 3.5.0
2024-11-28 13:52:00,158:INFO:           wurlitzer: Not installed
2024-11-28 13:52:00,158:INFO:PyCaret optional dependencies:
2024-11-28 13:52:00,166:INFO:                shap: Not installed
2024-11-28 13:52:00,166:INFO:           interpret: Not installed
2024-11-28 13:52:00,166:INFO:                umap: Not installed
2024-11-28 13:52:00,166:INFO:     ydata_profiling: Not installed
2024-11-28 13:52:00,166:INFO:  explainerdashboard: Not installed
2024-11-28 13:52:00,166:INFO:             autoviz: Not installed
2024-11-28 13:52:00,166:INFO:           fairlearn: Not installed
2024-11-28 13:52:00,166:INFO:          deepchecks: Not installed
2024-11-28 13:52:00,166:INFO:             xgboost: Not installed
2024-11-28 13:52:00,166:INFO:            catboost: Not installed
2024-11-28 13:52:00,166:INFO:              kmodes: Not installed
2024-11-28 13:52:00,166:INFO:             mlxtend: Not installed
2024-11-28 13:52:00,166:INFO:       statsforecast: Not installed
2024-11-28 13:52:00,166:INFO:        tune_sklearn: Not installed
2024-11-28 13:52:00,166:INFO:                 ray: Not installed
2024-11-28 13:52:00,166:INFO:            hyperopt: Not installed
2024-11-28 13:52:00,166:INFO:              optuna: Not installed
2024-11-28 13:52:00,167:INFO:               skopt: Not installed
2024-11-28 13:52:00,167:INFO:              mlflow: Not installed
2024-11-28 13:52:00,167:INFO:              gradio: Not installed
2024-11-28 13:52:00,167:INFO:             fastapi: Not installed
2024-11-28 13:52:00,167:INFO:             uvicorn: Not installed
2024-11-28 13:52:00,167:INFO:              m2cgen: Not installed
2024-11-28 13:52:00,167:INFO:           evidently: Not installed
2024-11-28 13:52:00,167:INFO:               fugue: Not installed
2024-11-28 13:52:00,167:INFO:           streamlit: Not installed
2024-11-28 13:52:00,167:INFO:             prophet: Not installed
2024-11-28 13:52:00,167:INFO:None
2024-11-28 13:52:00,167:INFO:Set up data.
2024-11-28 13:52:00,174:INFO:Set up folding strategy.
2024-11-28 13:52:00,174:INFO:Set up train/test split.
2024-11-28 13:52:00,183:INFO:Set up index.
2024-11-28 13:52:00,183:INFO:Assigning column types.
2024-11-28 13:52:00,187:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 13:52:00,217:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 13:52:00,217:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 13:52:00,248:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,248:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,295:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 13:52:00,295:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 13:52:00,311:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,311:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,311:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 13:52:00,338:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 13:52:00,369:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,369:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,401:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 13:52:00,432:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,432:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,432:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 13:52:00,487:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,548:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,548:INFO:Preparing preprocessing pipeline...
2024-11-28 13:52:00,563:INFO:Set up simple imputation.
2024-11-28 13:52:00,563:INFO:Set up encoding of categorical features.
2024-11-28 13:52:00,563:INFO:Set up column transformation.
2024-11-28 13:52:00,563:INFO:Set up feature normalization.
2024-11-28 13:52:00,658:INFO:Finished creating preprocessing pipeline.
2024-11-28 13:52:00,673:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 13:52:00,673:INFO:Creating final display dataframe.
2024-11-28 13:52:00,862:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 17)
4        Transformed data shape         (891, 24)
5   Transformed train set shape         (623, 24)
6    Transformed test set shape         (268, 24)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 3
10     Rows with missing values              3.9%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Transformation              True
18        Transformation method       yeo-johnson
19                    Normalize              True
20             Normalize method            zscore
21               Fold Generator   StratifiedKFold
22                  Fold Number                10
23                     CPU Jobs                -1
24                      Use GPU             False
25               Log Experiment             False
26              Experiment Name  clf-default-name
27                          USI              582d
2024-11-28 13:52:00,938:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:00,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:01,010:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:01,010:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 13:52:01,011:INFO:setup() successfully completed in 0.91s...............
2024-11-28 13:52:01,035:INFO:Initializing compare_models()
2024-11-28 13:52:01,035:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 13:52:01,035:INFO:Checking exceptions
2024-11-28 13:52:01,042:INFO:Preparing display monitor
2024-11-28 13:52:01,077:INFO:Initializing Logistic Regression
2024-11-28 13:52:01,077:INFO:Total runtime is 1.6641616821289062e-05 minutes
2024-11-28 13:52:01,082:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:01,082:INFO:Initializing create_model()
2024-11-28 13:52:01,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:01,082:INFO:Checking exceptions
2024-11-28 13:52:01,082:INFO:Importing libraries
2024-11-28 13:52:01,082:INFO:Copying training dataset
2024-11-28 13:52:01,089:INFO:Defining folds
2024-11-28 13:52:01,090:INFO:Declaring metric variables
2024-11-28 13:52:01,148:INFO:Importing untrained model
2024-11-28 13:52:01,158:INFO:Logistic Regression Imported successfully
2024-11-28 13:52:01,170:INFO:Starting cross validation
2024-11-28 13:52:01,174:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:06,783:INFO:Calculating mean and std
2024-11-28 13:52:06,785:INFO:Creating metrics dataframe
2024-11-28 13:52:06,791:INFO:Uploading results into container
2024-11-28 13:52:06,792:INFO:Uploading model into container now
2024-11-28 13:52:06,792:INFO:_master_model_container: 1
2024-11-28 13:52:06,792:INFO:_display_container: 2
2024-11-28 13:52:06,793:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 13:52:06,793:INFO:create_model() successfully completed......................................
2024-11-28 13:52:06,852:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:06,852:INFO:Creating metrics dataframe
2024-11-28 13:52:06,852:INFO:Initializing K Neighbors Classifier
2024-11-28 13:52:06,852:INFO:Total runtime is 0.09625343481699625 minutes
2024-11-28 13:52:06,867:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:06,867:INFO:Initializing create_model()
2024-11-28 13:52:06,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:06,867:INFO:Checking exceptions
2024-11-28 13:52:06,867:INFO:Importing libraries
2024-11-28 13:52:06,867:INFO:Copying training dataset
2024-11-28 13:52:06,867:INFO:Defining folds
2024-11-28 13:52:06,867:INFO:Declaring metric variables
2024-11-28 13:52:06,867:INFO:Importing untrained model
2024-11-28 13:52:06,867:INFO:K Neighbors Classifier Imported successfully
2024-11-28 13:52:06,883:INFO:Starting cross validation
2024-11-28 13:52:06,883:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:07,445:INFO:Calculating mean and std
2024-11-28 13:52:07,446:INFO:Creating metrics dataframe
2024-11-28 13:52:07,448:INFO:Uploading results into container
2024-11-28 13:52:07,448:INFO:Uploading model into container now
2024-11-28 13:52:07,449:INFO:_master_model_container: 2
2024-11-28 13:52:07,449:INFO:_display_container: 2
2024-11-28 13:52:07,450:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 13:52:07,450:INFO:create_model() successfully completed......................................
2024-11-28 13:52:07,515:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:07,515:INFO:Creating metrics dataframe
2024-11-28 13:52:07,515:INFO:Initializing Naive Bayes
2024-11-28 13:52:07,515:INFO:Total runtime is 0.10730429887771606 minutes
2024-11-28 13:52:07,515:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:07,515:INFO:Initializing create_model()
2024-11-28 13:52:07,515:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:07,515:INFO:Checking exceptions
2024-11-28 13:52:07,515:INFO:Importing libraries
2024-11-28 13:52:07,515:INFO:Copying training dataset
2024-11-28 13:52:07,530:INFO:Defining folds
2024-11-28 13:52:07,530:INFO:Declaring metric variables
2024-11-28 13:52:07,530:INFO:Importing untrained model
2024-11-28 13:52:07,530:INFO:Naive Bayes Imported successfully
2024-11-28 13:52:07,530:INFO:Starting cross validation
2024-11-28 13:52:07,546:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:08,013:INFO:Calculating mean and std
2024-11-28 13:52:08,015:INFO:Creating metrics dataframe
2024-11-28 13:52:08,017:INFO:Uploading results into container
2024-11-28 13:52:08,018:INFO:Uploading model into container now
2024-11-28 13:52:08,018:INFO:_master_model_container: 3
2024-11-28 13:52:08,018:INFO:_display_container: 2
2024-11-28 13:52:08,018:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 13:52:08,018:INFO:create_model() successfully completed......................................
2024-11-28 13:52:08,081:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:08,082:INFO:Creating metrics dataframe
2024-11-28 13:52:08,082:INFO:Initializing Decision Tree Classifier
2024-11-28 13:52:08,082:INFO:Total runtime is 0.11675264835357665 minutes
2024-11-28 13:52:08,082:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:08,082:INFO:Initializing create_model()
2024-11-28 13:52:08,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:08,082:INFO:Checking exceptions
2024-11-28 13:52:08,082:INFO:Importing libraries
2024-11-28 13:52:08,082:INFO:Copying training dataset
2024-11-28 13:52:08,093:INFO:Defining folds
2024-11-28 13:52:08,093:INFO:Declaring metric variables
2024-11-28 13:52:08,099:INFO:Importing untrained model
2024-11-28 13:52:08,099:INFO:Decision Tree Classifier Imported successfully
2024-11-28 13:52:08,109:INFO:Starting cross validation
2024-11-28 13:52:08,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:08,468:INFO:Calculating mean and std
2024-11-28 13:52:08,468:INFO:Creating metrics dataframe
2024-11-28 13:52:08,468:INFO:Uploading results into container
2024-11-28 13:52:08,468:INFO:Uploading model into container now
2024-11-28 13:52:08,468:INFO:_master_model_container: 4
2024-11-28 13:52:08,468:INFO:_display_container: 2
2024-11-28 13:52:08,468:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 13:52:08,468:INFO:create_model() successfully completed......................................
2024-11-28 13:52:08,540:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:08,540:INFO:Creating metrics dataframe
2024-11-28 13:52:08,550:INFO:Initializing SVM - Linear Kernel
2024-11-28 13:52:08,550:INFO:Total runtime is 0.12455153465270996 minutes
2024-11-28 13:52:08,553:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:08,553:INFO:Initializing create_model()
2024-11-28 13:52:08,553:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:08,553:INFO:Checking exceptions
2024-11-28 13:52:08,553:INFO:Importing libraries
2024-11-28 13:52:08,553:INFO:Copying training dataset
2024-11-28 13:52:08,558:INFO:Defining folds
2024-11-28 13:52:08,558:INFO:Declaring metric variables
2024-11-28 13:52:08,560:INFO:Importing untrained model
2024-11-28 13:52:08,566:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 13:52:08,572:INFO:Starting cross validation
2024-11-28 13:52:08,575:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:08,926:INFO:Calculating mean and std
2024-11-28 13:52:08,926:INFO:Creating metrics dataframe
2024-11-28 13:52:08,926:INFO:Uploading results into container
2024-11-28 13:52:08,926:INFO:Uploading model into container now
2024-11-28 13:52:08,926:INFO:_master_model_container: 5
2024-11-28 13:52:08,926:INFO:_display_container: 2
2024-11-28 13:52:08,926:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 13:52:08,926:INFO:create_model() successfully completed......................................
2024-11-28 13:52:08,983:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:08,983:INFO:Creating metrics dataframe
2024-11-28 13:52:08,996:INFO:Initializing Ridge Classifier
2024-11-28 13:52:08,997:INFO:Total runtime is 0.13201017379760743 minutes
2024-11-28 13:52:08,999:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:08,999:INFO:Initializing create_model()
2024-11-28 13:52:08,999:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:08,999:INFO:Checking exceptions
2024-11-28 13:52:08,999:INFO:Importing libraries
2024-11-28 13:52:08,999:INFO:Copying training dataset
2024-11-28 13:52:08,999:INFO:Defining folds
2024-11-28 13:52:08,999:INFO:Declaring metric variables
2024-11-28 13:52:08,999:INFO:Importing untrained model
2024-11-28 13:52:08,999:INFO:Ridge Classifier Imported successfully
2024-11-28 13:52:09,015:INFO:Starting cross validation
2024-11-28 13:52:09,015:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:09,361:INFO:Calculating mean and std
2024-11-28 13:52:09,361:INFO:Creating metrics dataframe
2024-11-28 13:52:09,365:INFO:Uploading results into container
2024-11-28 13:52:09,365:INFO:Uploading model into container now
2024-11-28 13:52:09,366:INFO:_master_model_container: 6
2024-11-28 13:52:09,367:INFO:_display_container: 2
2024-11-28 13:52:09,367:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 13:52:09,367:INFO:create_model() successfully completed......................................
2024-11-28 13:52:09,412:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:09,412:INFO:Creating metrics dataframe
2024-11-28 13:52:09,428:INFO:Initializing Random Forest Classifier
2024-11-28 13:52:09,428:INFO:Total runtime is 0.13919645150502524 minutes
2024-11-28 13:52:09,428:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:09,428:INFO:Initializing create_model()
2024-11-28 13:52:09,428:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:09,428:INFO:Checking exceptions
2024-11-28 13:52:09,428:INFO:Importing libraries
2024-11-28 13:52:09,428:INFO:Copying training dataset
2024-11-28 13:52:09,428:INFO:Defining folds
2024-11-28 13:52:09,428:INFO:Declaring metric variables
2024-11-28 13:52:09,443:INFO:Importing untrained model
2024-11-28 13:52:09,443:INFO:Random Forest Classifier Imported successfully
2024-11-28 13:52:09,453:INFO:Starting cross validation
2024-11-28 13:52:09,453:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:10,358:INFO:Calculating mean and std
2024-11-28 13:52:10,361:INFO:Creating metrics dataframe
2024-11-28 13:52:10,363:INFO:Uploading results into container
2024-11-28 13:52:10,363:INFO:Uploading model into container now
2024-11-28 13:52:10,365:INFO:_master_model_container: 7
2024-11-28 13:52:10,365:INFO:_display_container: 2
2024-11-28 13:52:10,365:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 13:52:10,365:INFO:create_model() successfully completed......................................
2024-11-28 13:52:10,409:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:10,409:INFO:Creating metrics dataframe
2024-11-28 13:52:10,425:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 13:52:10,425:INFO:Total runtime is 0.155813471476237 minutes
2024-11-28 13:52:10,425:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:10,425:INFO:Initializing create_model()
2024-11-28 13:52:10,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:10,425:INFO:Checking exceptions
2024-11-28 13:52:10,425:INFO:Importing libraries
2024-11-28 13:52:10,425:INFO:Copying training dataset
2024-11-28 13:52:10,425:INFO:Defining folds
2024-11-28 13:52:10,425:INFO:Declaring metric variables
2024-11-28 13:52:10,425:INFO:Importing untrained model
2024-11-28 13:52:10,441:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 13:52:10,449:INFO:Starting cross validation
2024-11-28 13:52:10,449:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:10,612:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,612:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,643:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,643:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,643:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,643:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,643:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,759:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,759:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 13:52:10,821:INFO:Calculating mean and std
2024-11-28 13:52:10,823:INFO:Creating metrics dataframe
2024-11-28 13:52:10,825:INFO:Uploading results into container
2024-11-28 13:52:10,826:INFO:Uploading model into container now
2024-11-28 13:52:10,826:INFO:_master_model_container: 8
2024-11-28 13:52:10,826:INFO:_display_container: 2
2024-11-28 13:52:10,827:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 13:52:10,827:INFO:create_model() successfully completed......................................
2024-11-28 13:52:10,874:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:10,874:INFO:Creating metrics dataframe
2024-11-28 13:52:10,890:INFO:Initializing Ada Boost Classifier
2024-11-28 13:52:10,891:INFO:Total runtime is 0.16356860399246217 minutes
2024-11-28 13:52:10,891:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:10,891:INFO:Initializing create_model()
2024-11-28 13:52:10,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:10,891:INFO:Checking exceptions
2024-11-28 13:52:10,891:INFO:Importing libraries
2024-11-28 13:52:10,891:INFO:Copying training dataset
2024-11-28 13:52:10,897:INFO:Defining folds
2024-11-28 13:52:10,897:INFO:Declaring metric variables
2024-11-28 13:52:10,897:INFO:Importing untrained model
2024-11-28 13:52:10,897:INFO:Ada Boost Classifier Imported successfully
2024-11-28 13:52:10,911:INFO:Starting cross validation
2024-11-28 13:52:10,913:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:11,059:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,059:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,059:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,075:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,075:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,075:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,075:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,075:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,359:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,359:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 13:52:11,490:INFO:Calculating mean and std
2024-11-28 13:52:11,490:INFO:Creating metrics dataframe
2024-11-28 13:52:11,490:INFO:Uploading results into container
2024-11-28 13:52:11,490:INFO:Uploading model into container now
2024-11-28 13:52:11,490:INFO:_master_model_container: 9
2024-11-28 13:52:11,490:INFO:_display_container: 2
2024-11-28 13:52:11,490:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 13:52:11,490:INFO:create_model() successfully completed......................................
2024-11-28 13:52:11,539:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:11,539:INFO:Creating metrics dataframe
2024-11-28 13:52:11,558:INFO:Initializing Gradient Boosting Classifier
2024-11-28 13:52:11,558:INFO:Total runtime is 0.17469135522842408 minutes
2024-11-28 13:52:11,560:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:11,560:INFO:Initializing create_model()
2024-11-28 13:52:11,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:11,560:INFO:Checking exceptions
2024-11-28 13:52:11,560:INFO:Importing libraries
2024-11-28 13:52:11,560:INFO:Copying training dataset
2024-11-28 13:52:11,560:INFO:Defining folds
2024-11-28 13:52:11,560:INFO:Declaring metric variables
2024-11-28 13:52:11,560:INFO:Importing untrained model
2024-11-28 13:52:11,560:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 13:52:11,578:INFO:Starting cross validation
2024-11-28 13:52:11,580:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:12,271:INFO:Calculating mean and std
2024-11-28 13:52:12,271:INFO:Creating metrics dataframe
2024-11-28 13:52:12,271:INFO:Uploading results into container
2024-11-28 13:52:12,271:INFO:Uploading model into container now
2024-11-28 13:52:12,277:INFO:_master_model_container: 10
2024-11-28 13:52:12,277:INFO:_display_container: 2
2024-11-28 13:52:12,277:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 13:52:12,277:INFO:create_model() successfully completed......................................
2024-11-28 13:52:12,321:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:12,321:INFO:Creating metrics dataframe
2024-11-28 13:52:12,337:INFO:Initializing Linear Discriminant Analysis
2024-11-28 13:52:12,337:INFO:Total runtime is 0.18767379919687907 minutes
2024-11-28 13:52:12,337:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:12,337:INFO:Initializing create_model()
2024-11-28 13:52:12,337:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:12,337:INFO:Checking exceptions
2024-11-28 13:52:12,337:INFO:Importing libraries
2024-11-28 13:52:12,337:INFO:Copying training dataset
2024-11-28 13:52:12,337:INFO:Defining folds
2024-11-28 13:52:12,337:INFO:Declaring metric variables
2024-11-28 13:52:12,337:INFO:Importing untrained model
2024-11-28 13:52:12,352:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 13:52:12,361:INFO:Starting cross validation
2024-11-28 13:52:12,362:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:12,702:INFO:Calculating mean and std
2024-11-28 13:52:12,702:INFO:Creating metrics dataframe
2024-11-28 13:52:12,702:INFO:Uploading results into container
2024-11-28 13:52:12,707:INFO:Uploading model into container now
2024-11-28 13:52:12,707:INFO:_master_model_container: 11
2024-11-28 13:52:12,707:INFO:_display_container: 2
2024-11-28 13:52:12,707:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 13:52:12,707:INFO:create_model() successfully completed......................................
2024-11-28 13:52:12,800:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:12,800:INFO:Creating metrics dataframe
2024-11-28 13:52:12,816:INFO:Initializing Extra Trees Classifier
2024-11-28 13:52:12,816:INFO:Total runtime is 0.19566030502319337 minutes
2024-11-28 13:52:12,816:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:12,816:INFO:Initializing create_model()
2024-11-28 13:52:12,816:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:12,816:INFO:Checking exceptions
2024-11-28 13:52:12,816:INFO:Importing libraries
2024-11-28 13:52:12,816:INFO:Copying training dataset
2024-11-28 13:52:12,816:INFO:Defining folds
2024-11-28 13:52:12,816:INFO:Declaring metric variables
2024-11-28 13:52:12,835:INFO:Importing untrained model
2024-11-28 13:52:12,840:INFO:Extra Trees Classifier Imported successfully
2024-11-28 13:52:12,840:INFO:Starting cross validation
2024-11-28 13:52:12,840:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:13,666:INFO:Calculating mean and std
2024-11-28 13:52:13,666:INFO:Creating metrics dataframe
2024-11-28 13:52:13,666:INFO:Uploading results into container
2024-11-28 13:52:13,666:INFO:Uploading model into container now
2024-11-28 13:52:13,666:INFO:_master_model_container: 12
2024-11-28 13:52:13,666:INFO:_display_container: 2
2024-11-28 13:52:13,666:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 13:52:13,666:INFO:create_model() successfully completed......................................
2024-11-28 13:52:13,719:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:13,719:INFO:Creating metrics dataframe
2024-11-28 13:52:13,736:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 13:52:13,736:INFO:Total runtime is 0.21099142233530682 minutes
2024-11-28 13:52:13,736:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:13,736:INFO:Initializing create_model()
2024-11-28 13:52:13,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:13,736:INFO:Checking exceptions
2024-11-28 13:52:13,736:INFO:Importing libraries
2024-11-28 13:52:13,736:INFO:Copying training dataset
2024-11-28 13:52:13,736:INFO:Defining folds
2024-11-28 13:52:13,736:INFO:Declaring metric variables
2024-11-28 13:52:13,752:INFO:Importing untrained model
2024-11-28 13:52:13,752:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:13,752:INFO:Starting cross validation
2024-11-28 13:52:13,752:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:14,932:INFO:Calculating mean and std
2024-11-28 13:52:14,932:INFO:Creating metrics dataframe
2024-11-28 13:52:14,932:INFO:Uploading results into container
2024-11-28 13:52:14,932:INFO:Uploading model into container now
2024-11-28 13:52:14,932:INFO:_master_model_container: 13
2024-11-28 13:52:14,932:INFO:_display_container: 2
2024-11-28 13:52:14,932:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:14,932:INFO:create_model() successfully completed......................................
2024-11-28 13:52:15,013:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:15,013:INFO:Creating metrics dataframe
2024-11-28 13:52:15,020:INFO:Initializing Dummy Classifier
2024-11-28 13:52:15,020:INFO:Total runtime is 0.2323983430862427 minutes
2024-11-28 13:52:15,020:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:15,029:INFO:Initializing create_model()
2024-11-28 13:52:15,029:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE394268F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:15,029:INFO:Checking exceptions
2024-11-28 13:52:15,029:INFO:Importing libraries
2024-11-28 13:52:15,029:INFO:Copying training dataset
2024-11-28 13:52:15,031:INFO:Defining folds
2024-11-28 13:52:15,031:INFO:Declaring metric variables
2024-11-28 13:52:15,031:INFO:Importing untrained model
2024-11-28 13:52:15,031:INFO:Dummy Classifier Imported successfully
2024-11-28 13:52:15,031:INFO:Starting cross validation
2024-11-28 13:52:15,046:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:15,264:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,264:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,264:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,264:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,264:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,264:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,279:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,379:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,379:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 13:52:15,411:INFO:Calculating mean and std
2024-11-28 13:52:15,412:INFO:Creating metrics dataframe
2024-11-28 13:52:15,415:INFO:Uploading results into container
2024-11-28 13:52:15,415:INFO:Uploading model into container now
2024-11-28 13:52:15,416:INFO:_master_model_container: 14
2024-11-28 13:52:15,416:INFO:_display_container: 2
2024-11-28 13:52:15,416:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 13:52:15,416:INFO:create_model() successfully completed......................................
2024-11-28 13:52:15,464:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:15,464:INFO:Creating metrics dataframe
2024-11-28 13:52:15,480:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 13:52:15,480:INFO:Initializing create_model()
2024-11-28 13:52:15,480:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:15,480:INFO:Checking exceptions
2024-11-28 13:52:15,496:INFO:Importing libraries
2024-11-28 13:52:15,496:INFO:Copying training dataset
2024-11-28 13:52:15,500:INFO:Defining folds
2024-11-28 13:52:15,500:INFO:Declaring metric variables
2024-11-28 13:52:15,500:INFO:Importing untrained model
2024-11-28 13:52:15,500:INFO:Declaring custom model
2024-11-28 13:52:15,500:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:15,501:INFO:Cross validation set to False
2024-11-28 13:52:15,501:INFO:Fitting Model
2024-11-28 13:52:15,583:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:15,584:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 13:52:15,585:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000242 seconds.
2024-11-28 13:52:15,585:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 13:52:15,585:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 13:52:15,585:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 13:52:15,585:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 13:52:15,585:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 13:52:15,586:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 13:52:15,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:15,648:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:15,648:INFO:create_model() successfully completed......................................
2024-11-28 13:52:15,762:INFO:_master_model_container: 14
2024-11-28 13:52:15,762:INFO:_display_container: 2
2024-11-28 13:52:15,763:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:15,763:INFO:compare_models() successfully completed......................................
2024-11-28 13:52:15,813:INFO:Initializing tune_model()
2024-11-28 13:52:15,813:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>)
2024-11-28 13:52:15,813:INFO:Checking exceptions
2024-11-28 13:52:15,830:INFO:Copying training dataset
2024-11-28 13:52:15,834:INFO:Checking base model
2024-11-28 13:52:15,834:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 13:52:15,837:INFO:Declaring metric variables
2024-11-28 13:52:15,841:INFO:Defining Hyperparameters
2024-11-28 13:52:15,907:INFO:Tuning with n_jobs=-1
2024-11-28 13:52:15,907:INFO:Initializing RandomizedSearchCV
2024-11-28 13:52:24,024:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 13:52:24,024:INFO:Hyperparameter search completed
2024-11-28 13:52:24,024:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:24,024:INFO:Initializing create_model()
2024-11-28 13:52:24,024:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE157E8F70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 13:52:24,024:INFO:Checking exceptions
2024-11-28 13:52:24,024:INFO:Importing libraries
2024-11-28 13:52:24,024:INFO:Copying training dataset
2024-11-28 13:52:24,033:INFO:Defining folds
2024-11-28 13:52:24,033:INFO:Declaring metric variables
2024-11-28 13:52:24,042:INFO:Importing untrained model
2024-11-28 13:52:24,042:INFO:Declaring custom model
2024-11-28 13:52:24,047:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:24,056:INFO:Starting cross validation
2024-11-28 13:52:24,060:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:24,944:INFO:Calculating mean and std
2024-11-28 13:52:24,944:INFO:Creating metrics dataframe
2024-11-28 13:52:24,957:INFO:Finalizing model
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 13:52:25,137:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 13:52:25,137:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000298 seconds.
2024-11-28 13:52:25,137:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 13:52:25,137:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 13:52:25,137:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 13:52:25,137:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 13:52:25,137:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 13:52:25,137:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,153:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:25,168:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:25,179:INFO:Uploading results into container
2024-11-28 13:52:25,181:INFO:Uploading model into container now
2024-11-28 13:52:25,182:INFO:_master_model_container: 15
2024-11-28 13:52:25,182:INFO:_display_container: 3
2024-11-28 13:52:25,183:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:25,183:INFO:create_model() successfully completed......................................
2024-11-28 13:52:25,270:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:25,270:INFO:choose_better activated
2024-11-28 13:52:25,274:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:25,274:INFO:Initializing create_model()
2024-11-28 13:52:25,275:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:25,275:INFO:Checking exceptions
2024-11-28 13:52:25,277:INFO:Importing libraries
2024-11-28 13:52:25,277:INFO:Copying training dataset
2024-11-28 13:52:25,281:INFO:Defining folds
2024-11-28 13:52:25,282:INFO:Declaring metric variables
2024-11-28 13:52:25,282:INFO:Importing untrained model
2024-11-28 13:52:25,282:INFO:Declaring custom model
2024-11-28 13:52:25,282:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:25,283:INFO:Starting cross validation
2024-11-28 13:52:25,284:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:26,704:INFO:Calculating mean and std
2024-11-28 13:52:26,704:INFO:Creating metrics dataframe
2024-11-28 13:52:26,704:INFO:Finalizing model
2024-11-28 13:52:26,797:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:26,797:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 13:52:26,797:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.
2024-11-28 13:52:26,797:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-28 13:52:26,797:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 13:52:26,797:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 13:52:26,797:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 13:52:26,797:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 13:52:26,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:26,860:INFO:Uploading results into container
2024-11-28 13:52:26,860:INFO:Uploading model into container now
2024-11-28 13:52:26,860:INFO:_master_model_container: 16
2024-11-28 13:52:26,860:INFO:_display_container: 4
2024-11-28 13:52:26,860:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:26,860:INFO:create_model() successfully completed......................................
2024-11-28 13:52:26,961:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:26,961:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 13:52:26,962:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 13:52:26,962:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 13:52:26,962:INFO:choose_better completed
2024-11-28 13:52:26,963:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 13:52:26,974:INFO:_master_model_container: 16
2024-11-28 13:52:26,974:INFO:_display_container: 3
2024-11-28 13:52:26,975:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:26,975:INFO:tune_model() successfully completed......................................
2024-11-28 13:52:27,047:INFO:Initializing evaluate_model()
2024-11-28 13:52:27,047:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-28 13:52:27,064:INFO:Initializing plot_model()
2024-11-28 13:52:27,064:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, system=True)
2024-11-28 13:52:27,064:INFO:Checking exceptions
2024-11-28 13:52:27,080:INFO:Preloading libraries
2024-11-28 13:52:27,090:INFO:Copying training dataset
2024-11-28 13:52:27,091:INFO:Plot type: pipeline
2024-11-28 13:52:27,409:INFO:Visual Rendered Successfully
2024-11-28 13:52:27,485:INFO:plot_model() successfully completed......................................
2024-11-28 13:52:27,521:INFO:Initializing plot_model()
2024-11-28 13:52:27,522:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, system=True)
2024-11-28 13:52:27,522:INFO:Checking exceptions
2024-11-28 13:52:27,526:INFO:Preloading libraries
2024-11-28 13:52:27,536:INFO:Copying training dataset
2024-11-28 13:52:27,536:INFO:Plot type: feature
2024-11-28 13:52:27,537:WARNING:No coef_ found. Trying feature_importances_
2024-11-28 13:52:27,746:INFO:Visual Rendered Successfully
2024-11-28 13:52:27,813:INFO:plot_model() successfully completed......................................
2024-11-28 13:52:27,840:INFO:Initializing create_model()
2024-11-28 13:52:27,840:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:27,840:INFO:Checking exceptions
2024-11-28 13:52:27,857:INFO:Importing libraries
2024-11-28 13:52:27,857:INFO:Copying training dataset
2024-11-28 13:52:27,865:INFO:Defining folds
2024-11-28 13:52:27,865:INFO:Declaring metric variables
2024-11-28 13:52:27,868:INFO:Importing untrained model
2024-11-28 13:52:27,872:INFO:Logistic Regression Imported successfully
2024-11-28 13:52:27,879:INFO:Starting cross validation
2024-11-28 13:52:27,881:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:28,304:INFO:Calculating mean and std
2024-11-28 13:52:28,304:INFO:Creating metrics dataframe
2024-11-28 13:52:28,304:INFO:Finalizing model
2024-11-28 13:52:28,415:INFO:Uploading results into container
2024-11-28 13:52:28,417:INFO:Uploading model into container now
2024-11-28 13:52:28,426:INFO:_master_model_container: 17
2024-11-28 13:52:28,427:INFO:_display_container: 4
2024-11-28 13:52:28,427:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 13:52:28,427:INFO:create_model() successfully completed......................................
2024-11-28 13:52:28,501:INFO:Initializing create_model()
2024-11-28 13:52:28,501:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:28,501:INFO:Checking exceptions
2024-11-28 13:52:28,514:INFO:Importing libraries
2024-11-28 13:52:28,515:INFO:Copying training dataset
2024-11-28 13:52:28,521:INFO:Defining folds
2024-11-28 13:52:28,521:INFO:Declaring metric variables
2024-11-28 13:52:28,523:INFO:Importing untrained model
2024-11-28 13:52:28,527:INFO:Random Forest Classifier Imported successfully
2024-11-28 13:52:28,534:INFO:Starting cross validation
2024-11-28 13:52:28,535:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:29,886:INFO:Calculating mean and std
2024-11-28 13:52:29,886:INFO:Creating metrics dataframe
2024-11-28 13:52:29,886:INFO:Finalizing model
2024-11-28 13:52:30,147:INFO:Uploading results into container
2024-11-28 13:52:30,147:INFO:Uploading model into container now
2024-11-28 13:52:30,162:INFO:_master_model_container: 18
2024-11-28 13:52:30,162:INFO:_display_container: 5
2024-11-28 13:52:30,162:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 13:52:30,162:INFO:create_model() successfully completed......................................
2024-11-28 13:52:30,230:INFO:Initializing create_model()
2024-11-28 13:52:30,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:30,230:INFO:Checking exceptions
2024-11-28 13:52:30,244:INFO:Importing libraries
2024-11-28 13:52:30,244:INFO:Copying training dataset
2024-11-28 13:52:30,249:INFO:Defining folds
2024-11-28 13:52:30,249:INFO:Declaring metric variables
2024-11-28 13:52:30,252:INFO:Importing untrained model
2024-11-28 13:52:30,256:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 13:52:30,264:INFO:Starting cross validation
2024-11-28 13:52:30,267:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:31,103:INFO:Calculating mean and std
2024-11-28 13:52:31,103:INFO:Creating metrics dataframe
2024-11-28 13:52:31,103:INFO:Finalizing model
2024-11-28 13:52:31,304:INFO:Uploading results into container
2024-11-28 13:52:31,305:INFO:Uploading model into container now
2024-11-28 13:52:31,316:INFO:_master_model_container: 19
2024-11-28 13:52:31,316:INFO:_display_container: 6
2024-11-28 13:52:31,316:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 13:52:31,316:INFO:create_model() successfully completed......................................
2024-11-28 13:52:31,368:INFO:Initializing create_model()
2024-11-28 13:52:31,368:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:31,368:INFO:Checking exceptions
2024-11-28 13:52:31,385:INFO:Importing libraries
2024-11-28 13:52:31,385:INFO:Copying training dataset
2024-11-28 13:52:31,400:INFO:Defining folds
2024-11-28 13:52:31,400:INFO:Declaring metric variables
2024-11-28 13:52:31,403:INFO:Importing untrained model
2024-11-28 13:52:31,407:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:31,413:INFO:Starting cross validation
2024-11-28 13:52:31,416:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:32,744:INFO:Calculating mean and std
2024-11-28 13:52:32,744:INFO:Creating metrics dataframe
2024-11-28 13:52:32,744:INFO:Finalizing model
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:32,869:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 13:52:32,869:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000313 seconds.
2024-11-28 13:52:32,869:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-28 13:52:32,869:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 13:52:32,869:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 13:52:32,869:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 13:52:32,869:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:32,932:INFO:Uploading results into container
2024-11-28 13:52:32,949:INFO:Uploading model into container now
2024-11-28 13:52:32,961:INFO:_master_model_container: 20
2024-11-28 13:52:32,961:INFO:_display_container: 7
2024-11-28 13:52:32,962:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:32,962:INFO:create_model() successfully completed......................................
2024-11-28 13:52:33,059:INFO:Initializing tune_model()
2024-11-28 13:52:33,059:INFO:tune_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>)
2024-11-28 13:52:33,059:INFO:Checking exceptions
2024-11-28 13:52:33,091:INFO:Copying training dataset
2024-11-28 13:52:33,096:INFO:Checking base model
2024-11-28 13:52:33,096:INFO:Base model : Gradient Boosting Classifier
2024-11-28 13:52:33,100:INFO:Declaring metric variables
2024-11-28 13:52:33,104:INFO:Defining Hyperparameters
2024-11-28 13:52:33,163:INFO:Tuning with n_jobs=-1
2024-11-28 13:52:33,163:INFO:Initializing RandomizedSearchCV
2024-11-28 13:52:41,507:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__n_estimators': 270, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.3, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 6, 'actual_estimator__learning_rate': 0.05}
2024-11-28 13:52:41,507:INFO:Hyperparameter search completed
2024-11-28 13:52:41,507:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:41,507:INFO:Initializing create_model()
2024-11-28 13:52:41,507:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39508850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'n_estimators': 270, 'min_samples_split': 10, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.3, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.05})
2024-11-28 13:52:41,507:INFO:Checking exceptions
2024-11-28 13:52:41,507:INFO:Importing libraries
2024-11-28 13:52:41,507:INFO:Copying training dataset
2024-11-28 13:52:41,507:INFO:Defining folds
2024-11-28 13:52:41,507:INFO:Declaring metric variables
2024-11-28 13:52:41,507:INFO:Importing untrained model
2024-11-28 13:52:41,507:INFO:Declaring custom model
2024-11-28 13:52:41,507:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 13:52:41,523:INFO:Starting cross validation
2024-11-28 13:52:41,523:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:42,504:INFO:Calculating mean and std
2024-11-28 13:52:42,504:INFO:Creating metrics dataframe
2024-11-28 13:52:42,504:INFO:Finalizing model
2024-11-28 13:52:42,751:INFO:Uploading results into container
2024-11-28 13:52:42,751:INFO:Uploading model into container now
2024-11-28 13:52:42,751:INFO:_master_model_container: 21
2024-11-28 13:52:42,751:INFO:_display_container: 8
2024-11-28 13:52:42,767:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 13:52:42,767:INFO:create_model() successfully completed......................................
2024-11-28 13:52:42,819:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:42,819:INFO:choose_better activated
2024-11-28 13:52:42,819:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:42,819:INFO:Initializing create_model()
2024-11-28 13:52:42,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:42,819:INFO:Checking exceptions
2024-11-28 13:52:42,819:INFO:Importing libraries
2024-11-28 13:52:42,819:INFO:Copying training dataset
2024-11-28 13:52:42,835:INFO:Defining folds
2024-11-28 13:52:42,835:INFO:Declaring metric variables
2024-11-28 13:52:42,835:INFO:Importing untrained model
2024-11-28 13:52:42,835:INFO:Declaring custom model
2024-11-28 13:52:42,835:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 13:52:42,835:INFO:Starting cross validation
2024-11-28 13:52:42,835:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:43,608:INFO:Calculating mean and std
2024-11-28 13:52:43,609:INFO:Creating metrics dataframe
2024-11-28 13:52:43,611:INFO:Finalizing model
2024-11-28 13:52:43,798:INFO:Uploading results into container
2024-11-28 13:52:43,798:INFO:Uploading model into container now
2024-11-28 13:52:43,798:INFO:_master_model_container: 22
2024-11-28 13:52:43,798:INFO:_display_container: 9
2024-11-28 13:52:43,798:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 13:52:43,798:INFO:create_model() successfully completed......................................
2024-11-28 13:52:43,861:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:43,861:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8234
2024-11-28 13:52:43,861:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8395
2024-11-28 13:52:43,861:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2024-11-28 13:52:43,861:INFO:choose_better completed
2024-11-28 13:52:43,861:INFO:_master_model_container: 22
2024-11-28 13:52:43,861:INFO:_display_container: 8
2024-11-28 13:52:43,861:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 13:52:43,861:INFO:tune_model() successfully completed......................................
2024-11-28 13:52:43,924:INFO:Initializing tune_model()
2024-11-28 13:52:43,924:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>)
2024-11-28 13:52:43,937:INFO:Checking exceptions
2024-11-28 13:52:43,940:INFO:Copying training dataset
2024-11-28 13:52:43,940:INFO:Checking base model
2024-11-28 13:52:43,940:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 13:52:43,955:INFO:Declaring metric variables
2024-11-28 13:52:43,961:INFO:Defining Hyperparameters
2024-11-28 13:52:44,014:INFO:Tuning with n_jobs=-1
2024-11-28 13:52:44,014:INFO:Initializing RandomizedSearchCV
2024-11-28 13:52:52,559:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 13:52:52,562:INFO:Hyperparameter search completed
2024-11-28 13:52:52,563:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:52,566:INFO:Initializing create_model()
2024-11-28 13:52:52,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3968FF70>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 13:52:52,566:INFO:Checking exceptions
2024-11-28 13:52:52,566:INFO:Importing libraries
2024-11-28 13:52:52,566:INFO:Copying training dataset
2024-11-28 13:52:52,573:INFO:Defining folds
2024-11-28 13:52:52,573:INFO:Declaring metric variables
2024-11-28 13:52:52,578:INFO:Importing untrained model
2024-11-28 13:52:52,579:INFO:Declaring custom model
2024-11-28 13:52:52,584:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:52,594:INFO:Starting cross validation
2024-11-28 13:52:52,598:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:53,472:INFO:Calculating mean and std
2024-11-28 13:52:53,472:INFO:Creating metrics dataframe
2024-11-28 13:52:53,472:INFO:Finalizing model
2024-11-28 13:52:53,588:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 13:52:53,588:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 13:52:53,588:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 13:52:53,592:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:53,593:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 13:52:53,593:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 13:52:53,593:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 13:52:53,593:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 13:52:53,594:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000293 seconds.
2024-11-28 13:52:53,594:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 13:52:53,594:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 13:52:53,594:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 13:52:53,594:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 13:52:53,595:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 13:52:53,595:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 13:52:53,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,612:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,613:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,615:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,616:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,617:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:53,618:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 13:52:53,627:INFO:Uploading results into container
2024-11-28 13:52:53,627:INFO:Uploading model into container now
2024-11-28 13:52:53,627:INFO:_master_model_container: 23
2024-11-28 13:52:53,627:INFO:_display_container: 9
2024-11-28 13:52:53,627:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:53,627:INFO:create_model() successfully completed......................................
2024-11-28 13:52:53,710:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:53,710:INFO:choose_better activated
2024-11-28 13:52:53,710:INFO:SubProcess create_model() called ==================================
2024-11-28 13:52:53,710:INFO:Initializing create_model()
2024-11-28 13:52:53,710:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:53,710:INFO:Checking exceptions
2024-11-28 13:52:53,726:INFO:Importing libraries
2024-11-28 13:52:53,726:INFO:Copying training dataset
2024-11-28 13:52:53,726:INFO:Defining folds
2024-11-28 13:52:53,726:INFO:Declaring metric variables
2024-11-28 13:52:53,726:INFO:Importing untrained model
2024-11-28 13:52:53,726:INFO:Declaring custom model
2024-11-28 13:52:53,726:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:53,726:INFO:Starting cross validation
2024-11-28 13:52:53,726:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 13:52:55,050:INFO:Calculating mean and std
2024-11-28 13:52:55,050:INFO:Creating metrics dataframe
2024-11-28 13:52:55,050:INFO:Finalizing model
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:55,151:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 13:52:55,151:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.
2024-11-28 13:52:55,151:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-28 13:52:55,151:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 13:52:55,151:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 13:52:55,151:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 13:52:55,151:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,207:INFO:Uploading results into container
2024-11-28 13:52:55,207:INFO:Uploading model into container now
2024-11-28 13:52:55,207:INFO:_master_model_container: 24
2024-11-28 13:52:55,207:INFO:_display_container: 10
2024-11-28 13:52:55,207:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:55,207:INFO:create_model() successfully completed......................................
2024-11-28 13:52:55,286:INFO:SubProcess create_model() end ==================================
2024-11-28 13:52:55,286:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 13:52:55,286:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 13:52:55,286:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 13:52:55,286:INFO:choose_better completed
2024-11-28 13:52:55,286:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 13:52:55,301:INFO:_master_model_container: 24
2024-11-28 13:52:55,301:INFO:_display_container: 9
2024-11-28 13:52:55,301:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:55,301:INFO:tune_model() successfully completed......................................
2024-11-28 13:52:55,411:INFO:Initializing predict_model()
2024-11-28 13:52:55,411:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE391BBD90>)
2024-11-28 13:52:55,411:INFO:Checking exceptions
2024-11-28 13:52:55,411:INFO:Preloading libraries
2024-11-28 13:52:55,411:INFO:Set up data.
2024-11-28 13:52:55,427:INFO:Set up index.
2024-11-28 13:52:55,602:INFO:Initializing finalize_model()
2024-11-28 13:52:55,602:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-28 13:52:55,602:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 13:52:55,602:INFO:Initializing create_model()
2024-11-28 13:52:55,602:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 13:52:55,602:INFO:Checking exceptions
2024-11-28 13:52:55,602:INFO:Importing libraries
2024-11-28 13:52:55,602:INFO:Copying training dataset
2024-11-28 13:52:55,602:INFO:Defining folds
2024-11-28 13:52:55,602:INFO:Declaring metric variables
2024-11-28 13:52:55,602:INFO:Importing untrained model
2024-11-28 13:52:55,602:INFO:Declaring custom model
2024-11-28 13:52:55,602:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 13:52:55,617:INFO:Cross validation set to False
2024-11-28 13:52:55,617:INFO:Fitting Model
2024-11-28 13:52:55,700:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 13:52:55,700:INFO:[LightGBM] [Info] Number of positive: 342, number of negative: 549
2024-11-28 13:52:55,701:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000224 seconds.
2024-11-28 13:52:55,701:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 13:52:55,701:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 13:52:55,701:INFO:[LightGBM] [Info] Total Bins 423
2024-11-28 13:52:55,702:INFO:[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 23
2024-11-28 13:52:55,702:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288
2024-11-28 13:52:55,702:INFO:[LightGBM] [Info] Start training from score -0.473288
2024-11-28 13:52:55,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 13:52:55,768:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 13:52:55,768:INFO:create_model() successfully completed......................................
2024-11-28 13:52:55,852:INFO:_master_model_container: 24
2024-11-28 13:52:55,852:INFO:_display_container: 10
2024-11-28 13:52:55,867:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 13:52:55,867:INFO:finalize_model() successfully completed......................................
2024-11-28 13:52:55,941:INFO:Initializing save_model()
2024-11-28 13:52:55,941:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), model_name=../Titanic/data/titanic/final_tuned_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-28 13:52:55,941:INFO:Adding model into prep_pipe
2024-11-28 13:52:55,941:WARNING:Only Model saved as it was a pipeline.
2024-11-28 13:52:55,952:INFO:../Titanic/data/titanic/final_tuned_model.pkl saved in current working directory
2024-11-28 13:52:55,967:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 13:52:55,967:INFO:save_model() successfully completed......................................
2024-11-28 13:52:56,085:INFO:Initializing predict_model()
2024-11-28 13:52:56,085:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE15953340>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE394541F0>)
2024-11-28 13:52:56,085:INFO:Checking exceptions
2024-11-28 13:52:56,085:INFO:Preloading libraries
2024-11-28 13:52:56,085:INFO:Set up data.
2024-11-28 13:52:56,096:INFO:Set up index.
2024-11-28 14:05:57,922:INFO:PyCaret ClassificationExperiment
2024-11-28 14:05:57,922:INFO:Logging name: clf-default-name
2024-11-28 14:05:57,922:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 14:05:57,922:INFO:version 3.3.2
2024-11-28 14:05:57,922:INFO:Initializing setup()
2024-11-28 14:05:57,922:INFO:self.USI: cb49
2024-11-28 14:05:57,922:INFO:self._variable_keys: {'is_multiclass', 'exp_name_log', 'X_test', 'USI', 'y_train', 'n_jobs_param', 'html_param', 'logging_param', 'data', '_ml_usecase', 'gpu_param', 'gpu_n_jobs_param', 'y_test', 'memory', 'pipeline', 'log_plots_param', 'fix_imbalance', 'fold_shuffle_param', 'seed', 'fold_groups_param', 'target_param', 'X', 'idx', 'y', 'X_train', 'exp_id', 'fold_generator', '_available_plots'}
2024-11-28 14:05:57,922:INFO:Checking environment
2024-11-28 14:05:57,922:INFO:python_version: 3.10.7
2024-11-28 14:05:57,922:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 14:05:57,922:INFO:machine: AMD64
2024-11-28 14:05:57,923:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 14:05:57,937:INFO:Memory: svmem(total=16934494208, available=1655521280, percent=90.2, used=15278972928, free=1655521280)
2024-11-28 14:05:57,937:INFO:Physical Core: 4
2024-11-28 14:05:57,937:INFO:Logical Core: 8
2024-11-28 14:05:57,937:INFO:Checking libraries
2024-11-28 14:05:57,937:INFO:System:
2024-11-28 14:05:57,937:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 14:05:57,937:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 14:05:57,937:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 14:05:57,937:INFO:PyCaret required dependencies:
2024-11-28 14:05:57,937:INFO:                 pip: 22.2.2
2024-11-28 14:05:57,937:INFO:          setuptools: 63.2.0
2024-11-28 14:05:57,937:INFO:             pycaret: 3.3.2
2024-11-28 14:05:57,937:INFO:             IPython: 8.28.0
2024-11-28 14:05:57,937:INFO:          ipywidgets: 8.1.5
2024-11-28 14:05:57,937:INFO:                tqdm: 4.66.6
2024-11-28 14:05:57,937:INFO:               numpy: 1.26.4
2024-11-28 14:05:57,937:INFO:              pandas: 2.1.4
2024-11-28 14:05:57,938:INFO:              jinja2: 3.1.4
2024-11-28 14:05:57,938:INFO:               scipy: 1.11.4
2024-11-28 14:05:57,938:INFO:              joblib: 1.4.2
2024-11-28 14:05:57,938:INFO:             sklearn: 1.4.2
2024-11-28 14:05:57,938:INFO:                pyod: 2.0.2
2024-11-28 14:05:57,938:INFO:            imblearn: 0.12.4
2024-11-28 14:05:57,938:INFO:   category_encoders: 2.6.4
2024-11-28 14:05:57,938:INFO:            lightgbm: 4.5.0
2024-11-28 14:05:57,938:INFO:               numba: 0.60.0
2024-11-28 14:05:57,938:INFO:            requests: 2.32.3
2024-11-28 14:05:57,938:INFO:          matplotlib: 3.7.5
2024-11-28 14:05:57,938:INFO:          scikitplot: 0.3.7
2024-11-28 14:05:57,938:INFO:         yellowbrick: 1.5
2024-11-28 14:05:57,938:INFO:              plotly: 5.24.1
2024-11-28 14:05:57,938:INFO:    plotly-resampler: Not installed
2024-11-28 14:05:57,938:INFO:             kaleido: 0.2.1
2024-11-28 14:05:57,938:INFO:           schemdraw: 0.15
2024-11-28 14:05:57,938:INFO:         statsmodels: 0.14.4
2024-11-28 14:05:57,938:INFO:              sktime: 0.26.0
2024-11-28 14:05:57,938:INFO:               tbats: 1.1.3
2024-11-28 14:05:57,938:INFO:            pmdarima: 2.0.4
2024-11-28 14:05:57,938:INFO:              psutil: 6.0.0
2024-11-28 14:05:57,938:INFO:          markupsafe: 3.0.2
2024-11-28 14:05:57,938:INFO:             pickle5: Not installed
2024-11-28 14:05:57,938:INFO:         cloudpickle: 3.1.0
2024-11-28 14:05:57,938:INFO:         deprecation: 2.1.0
2024-11-28 14:05:57,938:INFO:              xxhash: 3.5.0
2024-11-28 14:05:57,938:INFO:           wurlitzer: Not installed
2024-11-28 14:05:57,938:INFO:PyCaret optional dependencies:
2024-11-28 14:05:57,938:INFO:                shap: Not installed
2024-11-28 14:05:57,938:INFO:           interpret: Not installed
2024-11-28 14:05:57,939:INFO:                umap: Not installed
2024-11-28 14:05:57,939:INFO:     ydata_profiling: Not installed
2024-11-28 14:05:57,939:INFO:  explainerdashboard: Not installed
2024-11-28 14:05:57,939:INFO:             autoviz: Not installed
2024-11-28 14:05:57,939:INFO:           fairlearn: Not installed
2024-11-28 14:05:57,939:INFO:          deepchecks: Not installed
2024-11-28 14:05:57,939:INFO:             xgboost: Not installed
2024-11-28 14:05:57,939:INFO:            catboost: Not installed
2024-11-28 14:05:57,939:INFO:              kmodes: Not installed
2024-11-28 14:05:57,939:INFO:             mlxtend: Not installed
2024-11-28 14:05:57,939:INFO:       statsforecast: Not installed
2024-11-28 14:05:57,939:INFO:        tune_sklearn: Not installed
2024-11-28 14:05:57,939:INFO:                 ray: Not installed
2024-11-28 14:05:57,939:INFO:            hyperopt: Not installed
2024-11-28 14:05:57,939:INFO:              optuna: Not installed
2024-11-28 14:05:57,939:INFO:               skopt: Not installed
2024-11-28 14:05:57,939:INFO:              mlflow: Not installed
2024-11-28 14:05:57,939:INFO:              gradio: Not installed
2024-11-28 14:05:57,940:INFO:             fastapi: Not installed
2024-11-28 14:05:57,940:INFO:             uvicorn: Not installed
2024-11-28 14:05:57,940:INFO:              m2cgen: Not installed
2024-11-28 14:05:57,940:INFO:           evidently: Not installed
2024-11-28 14:05:57,940:INFO:               fugue: Not installed
2024-11-28 14:05:57,940:INFO:           streamlit: Not installed
2024-11-28 14:05:57,940:INFO:             prophet: Not installed
2024-11-28 14:05:57,940:INFO:None
2024-11-28 14:05:57,940:INFO:Set up data.
2024-11-28 14:05:57,949:INFO:Set up folding strategy.
2024-11-28 14:05:57,949:INFO:Set up train/test split.
2024-11-28 14:05:57,956:INFO:Set up index.
2024-11-28 14:05:57,956:INFO:Assigning column types.
2024-11-28 14:05:57,959:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 14:05:58,068:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 14:05:58,068:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:05:58,149:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,149:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,256:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 14:05:58,257:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:05:58,317:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,317:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 14:05:58,417:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:05:58,484:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,484:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,583:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:05:58,633:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,633:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,633:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 14:05:58,799:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,799:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,967:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,967:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:58,967:INFO:Preparing preprocessing pipeline...
2024-11-28 14:05:58,967:INFO:Set up simple imputation.
2024-11-28 14:05:58,967:INFO:Set up encoding of categorical features.
2024-11-28 14:05:58,967:INFO:Set up column transformation.
2024-11-28 14:05:58,967:INFO:Set up feature normalization.
2024-11-28 14:05:59,066:INFO:Finished creating preprocessing pipeline.
2024-11-28 14:05:59,066:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 14:05:59,066:INFO:Creating final display dataframe.
2024-11-28 14:05:59,248:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 17)
4        Transformed data shape         (891, 24)
5   Transformed train set shape         (623, 24)
6    Transformed test set shape         (268, 24)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 3
10     Rows with missing values              3.9%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Transformation              True
18        Transformation method       yeo-johnson
19                    Normalize              True
20             Normalize method            zscore
21               Fold Generator   StratifiedKFold
22                  Fold Number                10
23                     CPU Jobs                -1
24                      Use GPU             False
25               Log Experiment             False
26              Experiment Name  clf-default-name
27                          USI              cb49
2024-11-28 14:05:59,318:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:59,318:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:59,364:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:59,364:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:05:59,364:INFO:setup() successfully completed in 1.45s...............
2024-11-28 14:05:59,397:INFO:Initializing compare_models()
2024-11-28 14:05:59,397:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 14:05:59,397:INFO:Checking exceptions
2024-11-28 14:05:59,416:INFO:Preparing display monitor
2024-11-28 14:05:59,440:INFO:Initializing Logistic Regression
2024-11-28 14:05:59,440:INFO:Total runtime is 0.0 minutes
2024-11-28 14:05:59,443:INFO:SubProcess create_model() called ==================================
2024-11-28 14:05:59,443:INFO:Initializing create_model()
2024-11-28 14:05:59,443:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:05:59,443:INFO:Checking exceptions
2024-11-28 14:05:59,443:INFO:Importing libraries
2024-11-28 14:05:59,443:INFO:Copying training dataset
2024-11-28 14:05:59,447:INFO:Defining folds
2024-11-28 14:05:59,447:INFO:Declaring metric variables
2024-11-28 14:05:59,452:INFO:Importing untrained model
2024-11-28 14:05:59,456:INFO:Logistic Regression Imported successfully
2024-11-28 14:05:59,462:INFO:Starting cross validation
2024-11-28 14:05:59,464:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:05,147:INFO:Calculating mean and std
2024-11-28 14:06:05,147:INFO:Creating metrics dataframe
2024-11-28 14:06:05,147:INFO:Uploading results into container
2024-11-28 14:06:05,147:INFO:Uploading model into container now
2024-11-28 14:06:05,147:INFO:_master_model_container: 1
2024-11-28 14:06:05,147:INFO:_display_container: 2
2024-11-28 14:06:05,147:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 14:06:05,147:INFO:create_model() successfully completed......................................
2024-11-28 14:06:05,242:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:05,242:INFO:Creating metrics dataframe
2024-11-28 14:06:05,258:INFO:Initializing K Neighbors Classifier
2024-11-28 14:06:05,258:INFO:Total runtime is 0.09696654081344605 minutes
2024-11-28 14:06:05,258:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:05,258:INFO:Initializing create_model()
2024-11-28 14:06:05,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:05,258:INFO:Checking exceptions
2024-11-28 14:06:05,258:INFO:Importing libraries
2024-11-28 14:06:05,258:INFO:Copying training dataset
2024-11-28 14:06:05,258:INFO:Defining folds
2024-11-28 14:06:05,258:INFO:Declaring metric variables
2024-11-28 14:06:05,258:INFO:Importing untrained model
2024-11-28 14:06:05,273:INFO:K Neighbors Classifier Imported successfully
2024-11-28 14:06:05,273:INFO:Starting cross validation
2024-11-28 14:06:05,273:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:05,760:INFO:Calculating mean and std
2024-11-28 14:06:05,760:INFO:Creating metrics dataframe
2024-11-28 14:06:05,760:INFO:Uploading results into container
2024-11-28 14:06:05,760:INFO:Uploading model into container now
2024-11-28 14:06:05,760:INFO:_master_model_container: 2
2024-11-28 14:06:05,760:INFO:_display_container: 2
2024-11-28 14:06:05,760:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 14:06:05,760:INFO:create_model() successfully completed......................................
2024-11-28 14:06:05,839:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:05,839:INFO:Creating metrics dataframe
2024-11-28 14:06:05,854:INFO:Initializing Naive Bayes
2024-11-28 14:06:05,854:INFO:Total runtime is 0.10691253741582235 minutes
2024-11-28 14:06:05,854:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:05,854:INFO:Initializing create_model()
2024-11-28 14:06:05,854:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:05,854:INFO:Checking exceptions
2024-11-28 14:06:05,854:INFO:Importing libraries
2024-11-28 14:06:05,854:INFO:Copying training dataset
2024-11-28 14:06:05,854:INFO:Defining folds
2024-11-28 14:06:05,854:INFO:Declaring metric variables
2024-11-28 14:06:05,870:INFO:Importing untrained model
2024-11-28 14:06:05,870:INFO:Naive Bayes Imported successfully
2024-11-28 14:06:05,870:INFO:Starting cross validation
2024-11-28 14:06:05,870:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:06,228:INFO:Calculating mean and std
2024-11-28 14:06:06,228:INFO:Creating metrics dataframe
2024-11-28 14:06:06,229:INFO:Uploading results into container
2024-11-28 14:06:06,229:INFO:Uploading model into container now
2024-11-28 14:06:06,229:INFO:_master_model_container: 3
2024-11-28 14:06:06,229:INFO:_display_container: 2
2024-11-28 14:06:06,229:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 14:06:06,229:INFO:create_model() successfully completed......................................
2024-11-28 14:06:06,306:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:06,306:INFO:Creating metrics dataframe
2024-11-28 14:06:06,322:INFO:Initializing Decision Tree Classifier
2024-11-28 14:06:06,322:INFO:Total runtime is 0.11470427910486858 minutes
2024-11-28 14:06:06,329:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:06,329:INFO:Initializing create_model()
2024-11-28 14:06:06,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:06,329:INFO:Checking exceptions
2024-11-28 14:06:06,329:INFO:Importing libraries
2024-11-28 14:06:06,329:INFO:Copying training dataset
2024-11-28 14:06:06,329:INFO:Defining folds
2024-11-28 14:06:06,329:INFO:Declaring metric variables
2024-11-28 14:06:06,329:INFO:Importing untrained model
2024-11-28 14:06:06,338:INFO:Decision Tree Classifier Imported successfully
2024-11-28 14:06:06,338:INFO:Starting cross validation
2024-11-28 14:06:06,338:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:06,683:INFO:Calculating mean and std
2024-11-28 14:06:06,683:INFO:Creating metrics dataframe
2024-11-28 14:06:06,683:INFO:Uploading results into container
2024-11-28 14:06:06,683:INFO:Uploading model into container now
2024-11-28 14:06:06,683:INFO:_master_model_container: 4
2024-11-28 14:06:06,683:INFO:_display_container: 2
2024-11-28 14:06:06,683:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 14:06:06,683:INFO:create_model() successfully completed......................................
2024-11-28 14:06:06,762:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:06,777:INFO:Creating metrics dataframe
2024-11-28 14:06:06,777:INFO:Initializing SVM - Linear Kernel
2024-11-28 14:06:06,777:INFO:Total runtime is 0.12229575316111248 minutes
2024-11-28 14:06:06,777:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:06,777:INFO:Initializing create_model()
2024-11-28 14:06:06,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:06,777:INFO:Checking exceptions
2024-11-28 14:06:06,777:INFO:Importing libraries
2024-11-28 14:06:06,777:INFO:Copying training dataset
2024-11-28 14:06:06,793:INFO:Defining folds
2024-11-28 14:06:06,793:INFO:Declaring metric variables
2024-11-28 14:06:06,793:INFO:Importing untrained model
2024-11-28 14:06:06,793:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 14:06:06,793:INFO:Starting cross validation
2024-11-28 14:06:06,793:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:07,155:INFO:Calculating mean and std
2024-11-28 14:06:07,155:INFO:Creating metrics dataframe
2024-11-28 14:06:07,155:INFO:Uploading results into container
2024-11-28 14:06:07,155:INFO:Uploading model into container now
2024-11-28 14:06:07,155:INFO:_master_model_container: 5
2024-11-28 14:06:07,155:INFO:_display_container: 2
2024-11-28 14:06:07,155:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 14:06:07,155:INFO:create_model() successfully completed......................................
2024-11-28 14:06:07,233:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:07,249:INFO:Creating metrics dataframe
2024-11-28 14:06:07,249:INFO:Initializing Ridge Classifier
2024-11-28 14:06:07,249:INFO:Total runtime is 0.13014950752258303 minutes
2024-11-28 14:06:07,249:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:07,249:INFO:Initializing create_model()
2024-11-28 14:06:07,249:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:07,249:INFO:Checking exceptions
2024-11-28 14:06:07,249:INFO:Importing libraries
2024-11-28 14:06:07,249:INFO:Copying training dataset
2024-11-28 14:06:07,249:INFO:Defining folds
2024-11-28 14:06:07,249:INFO:Declaring metric variables
2024-11-28 14:06:07,264:INFO:Importing untrained model
2024-11-28 14:06:07,264:INFO:Ridge Classifier Imported successfully
2024-11-28 14:06:07,264:INFO:Starting cross validation
2024-11-28 14:06:07,280:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:07,640:INFO:Calculating mean and std
2024-11-28 14:06:07,640:INFO:Creating metrics dataframe
2024-11-28 14:06:07,640:INFO:Uploading results into container
2024-11-28 14:06:07,640:INFO:Uploading model into container now
2024-11-28 14:06:07,640:INFO:_master_model_container: 6
2024-11-28 14:06:07,640:INFO:_display_container: 2
2024-11-28 14:06:07,640:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 14:06:07,640:INFO:create_model() successfully completed......................................
2024-11-28 14:06:07,734:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:07,734:INFO:Creating metrics dataframe
2024-11-28 14:06:07,734:INFO:Initializing Random Forest Classifier
2024-11-28 14:06:07,734:INFO:Total runtime is 0.1382454832394918 minutes
2024-11-28 14:06:07,734:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:07,734:INFO:Initializing create_model()
2024-11-28 14:06:07,734:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:07,734:INFO:Checking exceptions
2024-11-28 14:06:07,734:INFO:Importing libraries
2024-11-28 14:06:07,734:INFO:Copying training dataset
2024-11-28 14:06:07,750:INFO:Defining folds
2024-11-28 14:06:07,750:INFO:Declaring metric variables
2024-11-28 14:06:07,750:INFO:Importing untrained model
2024-11-28 14:06:07,750:INFO:Random Forest Classifier Imported successfully
2024-11-28 14:06:07,750:INFO:Starting cross validation
2024-11-28 14:06:07,766:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:08,631:INFO:Calculating mean and std
2024-11-28 14:06:08,631:INFO:Creating metrics dataframe
2024-11-28 14:06:08,633:INFO:Uploading results into container
2024-11-28 14:06:08,633:INFO:Uploading model into container now
2024-11-28 14:06:08,633:INFO:_master_model_container: 7
2024-11-28 14:06:08,633:INFO:_display_container: 2
2024-11-28 14:06:08,633:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 14:06:08,633:INFO:create_model() successfully completed......................................
2024-11-28 14:06:08,725:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:08,725:INFO:Creating metrics dataframe
2024-11-28 14:06:08,733:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 14:06:08,733:INFO:Total runtime is 0.15489604870478313 minutes
2024-11-28 14:06:08,733:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:08,733:INFO:Initializing create_model()
2024-11-28 14:06:08,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:08,733:INFO:Checking exceptions
2024-11-28 14:06:08,733:INFO:Importing libraries
2024-11-28 14:06:08,733:INFO:Copying training dataset
2024-11-28 14:06:08,741:INFO:Defining folds
2024-11-28 14:06:08,741:INFO:Declaring metric variables
2024-11-28 14:06:08,741:INFO:Importing untrained model
2024-11-28 14:06:08,741:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 14:06:08,757:INFO:Starting cross validation
2024-11-28 14:06:08,757:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:08,901:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:08,901:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:08,916:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:08,916:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:08,916:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:08,916:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:08,932:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:09,042:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:09,042:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:06:09,089:INFO:Calculating mean and std
2024-11-28 14:06:09,089:INFO:Creating metrics dataframe
2024-11-28 14:06:09,089:INFO:Uploading results into container
2024-11-28 14:06:09,089:INFO:Uploading model into container now
2024-11-28 14:06:09,089:INFO:_master_model_container: 8
2024-11-28 14:06:09,089:INFO:_display_container: 2
2024-11-28 14:06:09,089:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 14:06:09,089:INFO:create_model() successfully completed......................................
2024-11-28 14:06:09,168:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:09,168:INFO:Creating metrics dataframe
2024-11-28 14:06:09,183:INFO:Initializing Ada Boost Classifier
2024-11-28 14:06:09,183:INFO:Total runtime is 0.1623908321062724 minutes
2024-11-28 14:06:09,183:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:09,183:INFO:Initializing create_model()
2024-11-28 14:06:09,183:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:09,183:INFO:Checking exceptions
2024-11-28 14:06:09,183:INFO:Importing libraries
2024-11-28 14:06:09,183:INFO:Copying training dataset
2024-11-28 14:06:09,183:INFO:Defining folds
2024-11-28 14:06:09,183:INFO:Declaring metric variables
2024-11-28 14:06:09,199:INFO:Importing untrained model
2024-11-28 14:06:09,199:INFO:Ada Boost Classifier Imported successfully
2024-11-28 14:06:09,199:INFO:Starting cross validation
2024-11-28 14:06:09,199:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:09,356:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,356:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,356:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,356:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,356:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,371:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,371:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,371:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,653:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,653:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:06:09,799:INFO:Calculating mean and std
2024-11-28 14:06:09,800:INFO:Creating metrics dataframe
2024-11-28 14:06:09,802:INFO:Uploading results into container
2024-11-28 14:06:09,803:INFO:Uploading model into container now
2024-11-28 14:06:09,804:INFO:_master_model_container: 9
2024-11-28 14:06:09,804:INFO:_display_container: 2
2024-11-28 14:06:09,804:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 14:06:09,804:INFO:create_model() successfully completed......................................
2024-11-28 14:06:09,885:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:09,885:INFO:Creating metrics dataframe
2024-11-28 14:06:09,885:INFO:Initializing Gradient Boosting Classifier
2024-11-28 14:06:09,901:INFO:Total runtime is 0.1743517518043518 minutes
2024-11-28 14:06:09,902:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:09,902:INFO:Initializing create_model()
2024-11-28 14:06:09,902:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:09,902:INFO:Checking exceptions
2024-11-28 14:06:09,902:INFO:Importing libraries
2024-11-28 14:06:09,902:INFO:Copying training dataset
2024-11-28 14:06:09,902:INFO:Defining folds
2024-11-28 14:06:09,902:INFO:Declaring metric variables
2024-11-28 14:06:09,902:INFO:Importing untrained model
2024-11-28 14:06:09,902:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:06:09,919:INFO:Starting cross validation
2024-11-28 14:06:09,919:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:10,566:INFO:Calculating mean and std
2024-11-28 14:06:10,567:INFO:Creating metrics dataframe
2024-11-28 14:06:10,567:INFO:Uploading results into container
2024-11-28 14:06:10,567:INFO:Uploading model into container now
2024-11-28 14:06:10,567:INFO:_master_model_container: 10
2024-11-28 14:06:10,567:INFO:_display_container: 2
2024-11-28 14:06:10,567:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:06:10,567:INFO:create_model() successfully completed......................................
2024-11-28 14:06:10,667:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:10,667:INFO:Creating metrics dataframe
2024-11-28 14:06:10,667:INFO:Initializing Linear Discriminant Analysis
2024-11-28 14:06:10,667:INFO:Total runtime is 0.18711787859598797 minutes
2024-11-28 14:06:10,667:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:10,667:INFO:Initializing create_model()
2024-11-28 14:06:10,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:10,667:INFO:Checking exceptions
2024-11-28 14:06:10,667:INFO:Importing libraries
2024-11-28 14:06:10,667:INFO:Copying training dataset
2024-11-28 14:06:10,683:INFO:Defining folds
2024-11-28 14:06:10,683:INFO:Declaring metric variables
2024-11-28 14:06:10,683:INFO:Importing untrained model
2024-11-28 14:06:10,683:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 14:06:10,683:INFO:Starting cross validation
2024-11-28 14:06:10,699:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:11,031:INFO:Calculating mean and std
2024-11-28 14:06:11,031:INFO:Creating metrics dataframe
2024-11-28 14:06:11,032:INFO:Uploading results into container
2024-11-28 14:06:11,032:INFO:Uploading model into container now
2024-11-28 14:06:11,032:INFO:_master_model_container: 11
2024-11-28 14:06:11,032:INFO:_display_container: 2
2024-11-28 14:06:11,032:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 14:06:11,032:INFO:create_model() successfully completed......................................
2024-11-28 14:06:11,123:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:11,123:INFO:Creating metrics dataframe
2024-11-28 14:06:11,131:INFO:Initializing Extra Trees Classifier
2024-11-28 14:06:11,131:INFO:Total runtime is 0.19485977093378704 minutes
2024-11-28 14:06:11,132:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:11,132:INFO:Initializing create_model()
2024-11-28 14:06:11,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:11,132:INFO:Checking exceptions
2024-11-28 14:06:11,132:INFO:Importing libraries
2024-11-28 14:06:11,132:INFO:Copying training dataset
2024-11-28 14:06:11,137:INFO:Defining folds
2024-11-28 14:06:11,137:INFO:Declaring metric variables
2024-11-28 14:06:11,137:INFO:Importing untrained model
2024-11-28 14:06:11,137:INFO:Extra Trees Classifier Imported successfully
2024-11-28 14:06:11,148:INFO:Starting cross validation
2024-11-28 14:06:11,148:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:11,929:INFO:Calculating mean and std
2024-11-28 14:06:11,929:INFO:Creating metrics dataframe
2024-11-28 14:06:11,930:INFO:Uploading results into container
2024-11-28 14:06:11,930:INFO:Uploading model into container now
2024-11-28 14:06:11,930:INFO:_master_model_container: 12
2024-11-28 14:06:11,930:INFO:_display_container: 2
2024-11-28 14:06:11,930:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 14:06:11,930:INFO:create_model() successfully completed......................................
2024-11-28 14:06:12,035:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:12,035:INFO:Creating metrics dataframe
2024-11-28 14:06:12,042:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 14:06:12,042:INFO:Total runtime is 0.21003789107004803 minutes
2024-11-28 14:06:12,046:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:12,046:INFO:Initializing create_model()
2024-11-28 14:06:12,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:12,047:INFO:Checking exceptions
2024-11-28 14:06:12,047:INFO:Importing libraries
2024-11-28 14:06:12,047:INFO:Copying training dataset
2024-11-28 14:06:12,048:INFO:Defining folds
2024-11-28 14:06:12,048:INFO:Declaring metric variables
2024-11-28 14:06:12,048:INFO:Importing untrained model
2024-11-28 14:06:12,048:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:12,063:INFO:Starting cross validation
2024-11-28 14:06:12,063:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:13,291:INFO:Calculating mean and std
2024-11-28 14:06:13,292:INFO:Creating metrics dataframe
2024-11-28 14:06:13,292:INFO:Uploading results into container
2024-11-28 14:06:13,292:INFO:Uploading model into container now
2024-11-28 14:06:13,292:INFO:_master_model_container: 13
2024-11-28 14:06:13,292:INFO:_display_container: 2
2024-11-28 14:06:13,292:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:13,292:INFO:create_model() successfully completed......................................
2024-11-28 14:06:13,409:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:13,409:INFO:Creating metrics dataframe
2024-11-28 14:06:13,409:INFO:Initializing Dummy Classifier
2024-11-28 14:06:13,409:INFO:Total runtime is 0.23282035589218142 minutes
2024-11-28 14:06:13,409:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:13,409:INFO:Initializing create_model()
2024-11-28 14:06:13,409:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FB83E20>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:13,425:INFO:Checking exceptions
2024-11-28 14:06:13,425:INFO:Importing libraries
2024-11-28 14:06:13,425:INFO:Copying training dataset
2024-11-28 14:06:13,425:INFO:Defining folds
2024-11-28 14:06:13,425:INFO:Declaring metric variables
2024-11-28 14:06:13,425:INFO:Importing untrained model
2024-11-28 14:06:13,425:INFO:Dummy Classifier Imported successfully
2024-11-28 14:06:13,441:INFO:Starting cross validation
2024-11-28 14:06:13,442:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:13,642:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,642:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,642:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,658:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,658:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,642:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,658:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,776:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:06:13,790:INFO:Calculating mean and std
2024-11-28 14:06:13,791:INFO:Creating metrics dataframe
2024-11-28 14:06:13,791:INFO:Uploading results into container
2024-11-28 14:06:13,791:INFO:Uploading model into container now
2024-11-28 14:06:13,791:INFO:_master_model_container: 14
2024-11-28 14:06:13,791:INFO:_display_container: 2
2024-11-28 14:06:13,791:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 14:06:13,791:INFO:create_model() successfully completed......................................
2024-11-28 14:06:13,881:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:13,881:INFO:Creating metrics dataframe
2024-11-28 14:06:13,888:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 14:06:13,891:INFO:Initializing create_model()
2024-11-28 14:06:13,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:13,891:INFO:Checking exceptions
2024-11-28 14:06:13,891:INFO:Importing libraries
2024-11-28 14:06:13,891:INFO:Copying training dataset
2024-11-28 14:06:13,891:INFO:Defining folds
2024-11-28 14:06:13,891:INFO:Declaring metric variables
2024-11-28 14:06:13,891:INFO:Importing untrained model
2024-11-28 14:06:13,891:INFO:Declaring custom model
2024-11-28 14:06:13,891:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:13,906:INFO:Cross validation set to False
2024-11-28 14:06:13,906:INFO:Fitting Model
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:13,974:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:06:13,974:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000188 seconds.
2024-11-28 14:06:13,974:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:13,974:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:13,974:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:06:13,974:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:06:13,974:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:06:13,974:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:13,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:14,007:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:14,007:INFO:create_model() successfully completed......................................
2024-11-28 14:06:14,160:INFO:_master_model_container: 14
2024-11-28 14:06:14,160:INFO:_display_container: 2
2024-11-28 14:06:14,161:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:14,161:INFO:compare_models() successfully completed......................................
2024-11-28 14:06:14,228:INFO:Initializing tune_model()
2024-11-28 14:06:14,228:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>)
2024-11-28 14:06:14,228:INFO:Checking exceptions
2024-11-28 14:06:14,248:INFO:Copying training dataset
2024-11-28 14:06:14,248:INFO:Checking base model
2024-11-28 14:06:14,248:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 14:06:14,256:INFO:Declaring metric variables
2024-11-28 14:06:14,259:INFO:Defining Hyperparameters
2024-11-28 14:06:14,369:INFO:Tuning with n_jobs=-1
2024-11-28 14:06:14,369:INFO:Initializing RandomizedSearchCV
2024-11-28 14:06:21,843:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 14:06:21,843:INFO:Hyperparameter search completed
2024-11-28 14:06:21,843:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:21,843:INFO:Initializing create_model()
2024-11-28 14:06:21,843:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B7AC0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 14:06:21,843:INFO:Checking exceptions
2024-11-28 14:06:21,843:INFO:Importing libraries
2024-11-28 14:06:21,843:INFO:Copying training dataset
2024-11-28 14:06:21,843:INFO:Defining folds
2024-11-28 14:06:21,843:INFO:Declaring metric variables
2024-11-28 14:06:21,854:INFO:Importing untrained model
2024-11-28 14:06:21,854:INFO:Declaring custom model
2024-11-28 14:06:21,859:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:21,859:INFO:Starting cross validation
2024-11-28 14:06:21,859:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:22,565:INFO:Calculating mean and std
2024-11-28 14:06:22,565:INFO:Creating metrics dataframe
2024-11-28 14:06:22,565:INFO:Finalizing model
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:06:22,659:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:06:22,659:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000228 seconds.
2024-11-28 14:06:22,659:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:22,659:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:22,659:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 14:06:22,659:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 14:06:22,659:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:06:22,659:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:22,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:22,690:INFO:Uploading results into container
2024-11-28 14:06:22,690:INFO:Uploading model into container now
2024-11-28 14:06:22,690:INFO:_master_model_container: 15
2024-11-28 14:06:22,690:INFO:_display_container: 3
2024-11-28 14:06:22,690:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:22,690:INFO:create_model() successfully completed......................................
2024-11-28 14:06:22,816:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:22,816:INFO:choose_better activated
2024-11-28 14:06:22,816:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:22,816:INFO:Initializing create_model()
2024-11-28 14:06:22,816:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:22,816:INFO:Checking exceptions
2024-11-28 14:06:22,816:INFO:Importing libraries
2024-11-28 14:06:22,816:INFO:Copying training dataset
2024-11-28 14:06:22,816:INFO:Defining folds
2024-11-28 14:06:22,816:INFO:Declaring metric variables
2024-11-28 14:06:22,816:INFO:Importing untrained model
2024-11-28 14:06:22,816:INFO:Declaring custom model
2024-11-28 14:06:22,816:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:22,816:INFO:Starting cross validation
2024-11-28 14:06:22,816:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:23,993:INFO:Calculating mean and std
2024-11-28 14:06:23,993:INFO:Creating metrics dataframe
2024-11-28 14:06:23,993:INFO:Finalizing model
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:24,088:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:06:24,088:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000276 seconds.
2024-11-28 14:06:24,088:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:24,088:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:24,088:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:06:24,088:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:06:24,088:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:06:24,088:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:24,150:INFO:Uploading results into container
2024-11-28 14:06:24,150:INFO:Uploading model into container now
2024-11-28 14:06:24,150:INFO:_master_model_container: 16
2024-11-28 14:06:24,150:INFO:_display_container: 4
2024-11-28 14:06:24,158:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:24,158:INFO:create_model() successfully completed......................................
2024-11-28 14:06:24,276:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:24,276:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 14:06:24,276:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8411
2024-11-28 14:06:24,276:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 14:06:24,276:INFO:choose_better completed
2024-11-28 14:06:24,276:INFO:_master_model_container: 16
2024-11-28 14:06:24,276:INFO:_display_container: 3
2024-11-28 14:06:24,276:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:24,276:INFO:tune_model() successfully completed......................................
2024-11-28 14:06:24,428:INFO:Initializing evaluate_model()
2024-11-28 14:06:24,428:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-28 14:06:24,430:INFO:Initializing plot_model()
2024-11-28 14:06:24,430:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, system=True)
2024-11-28 14:06:24,430:INFO:Checking exceptions
2024-11-28 14:06:24,430:INFO:Preloading libraries
2024-11-28 14:06:24,445:INFO:Copying training dataset
2024-11-28 14:06:24,445:INFO:Plot type: pipeline
2024-11-28 14:06:24,595:INFO:Visual Rendered Successfully
2024-11-28 14:06:24,745:INFO:plot_model() successfully completed......................................
2024-11-28 14:06:24,786:INFO:Initializing plot_model()
2024-11-28 14:06:24,786:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, system=True)
2024-11-28 14:06:24,786:INFO:Checking exceptions
2024-11-28 14:06:24,791:INFO:Preloading libraries
2024-11-28 14:06:24,794:INFO:Copying training dataset
2024-11-28 14:06:24,794:INFO:Plot type: feature
2024-11-28 14:06:24,794:WARNING:No coef_ found. Trying feature_importances_
2024-11-28 14:06:25,005:INFO:Visual Rendered Successfully
2024-11-28 14:06:25,113:INFO:plot_model() successfully completed......................................
2024-11-28 14:06:25,128:INFO:Initializing create_model()
2024-11-28 14:06:25,128:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:25,128:INFO:Checking exceptions
2024-11-28 14:06:25,148:INFO:Importing libraries
2024-11-28 14:06:25,148:INFO:Copying training dataset
2024-11-28 14:06:25,150:INFO:Defining folds
2024-11-28 14:06:25,150:INFO:Declaring metric variables
2024-11-28 14:06:25,150:INFO:Importing untrained model
2024-11-28 14:06:25,163:INFO:Logistic Regression Imported successfully
2024-11-28 14:06:25,192:INFO:Starting cross validation
2024-11-28 14:06:25,195:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:25,625:INFO:Calculating mean and std
2024-11-28 14:06:25,625:INFO:Creating metrics dataframe
2024-11-28 14:06:25,625:INFO:Finalizing model
2024-11-28 14:06:25,719:INFO:Uploading results into container
2024-11-28 14:06:25,719:INFO:Uploading model into container now
2024-11-28 14:06:25,719:INFO:_master_model_container: 17
2024-11-28 14:06:25,719:INFO:_display_container: 4
2024-11-28 14:06:25,719:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 14:06:25,719:INFO:create_model() successfully completed......................................
2024-11-28 14:06:25,813:INFO:Initializing create_model()
2024-11-28 14:06:25,813:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:25,813:INFO:Checking exceptions
2024-11-28 14:06:25,837:INFO:Importing libraries
2024-11-28 14:06:25,837:INFO:Copying training dataset
2024-11-28 14:06:25,841:INFO:Defining folds
2024-11-28 14:06:25,841:INFO:Declaring metric variables
2024-11-28 14:06:25,844:INFO:Importing untrained model
2024-11-28 14:06:25,847:INFO:Random Forest Classifier Imported successfully
2024-11-28 14:06:25,854:INFO:Starting cross validation
2024-11-28 14:06:25,855:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:26,799:INFO:Calculating mean and std
2024-11-28 14:06:26,799:INFO:Creating metrics dataframe
2024-11-28 14:06:26,799:INFO:Finalizing model
2024-11-28 14:06:26,987:INFO:Uploading results into container
2024-11-28 14:06:26,987:INFO:Uploading model into container now
2024-11-28 14:06:26,987:INFO:_master_model_container: 18
2024-11-28 14:06:26,987:INFO:_display_container: 5
2024-11-28 14:06:26,987:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 14:06:26,987:INFO:create_model() successfully completed......................................
2024-11-28 14:06:27,082:INFO:Initializing create_model()
2024-11-28 14:06:27,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:27,082:INFO:Checking exceptions
2024-11-28 14:06:27,097:INFO:Importing libraries
2024-11-28 14:06:27,097:INFO:Copying training dataset
2024-11-28 14:06:27,110:INFO:Defining folds
2024-11-28 14:06:27,110:INFO:Declaring metric variables
2024-11-28 14:06:27,112:INFO:Importing untrained model
2024-11-28 14:06:27,116:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:06:27,123:INFO:Starting cross validation
2024-11-28 14:06:27,126:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:27,895:INFO:Calculating mean and std
2024-11-28 14:06:27,895:INFO:Creating metrics dataframe
2024-11-28 14:06:27,895:INFO:Finalizing model
2024-11-28 14:06:28,068:INFO:Uploading results into container
2024-11-28 14:06:28,068:INFO:Uploading model into container now
2024-11-28 14:06:28,068:INFO:_master_model_container: 19
2024-11-28 14:06:28,068:INFO:_display_container: 6
2024-11-28 14:06:28,068:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:06:28,068:INFO:create_model() successfully completed......................................
2024-11-28 14:06:28,165:INFO:Initializing create_model()
2024-11-28 14:06:28,165:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:28,165:INFO:Checking exceptions
2024-11-28 14:06:28,178:INFO:Importing libraries
2024-11-28 14:06:28,187:INFO:Copying training dataset
2024-11-28 14:06:28,192:INFO:Defining folds
2024-11-28 14:06:28,192:INFO:Declaring metric variables
2024-11-28 14:06:28,195:INFO:Importing untrained model
2024-11-28 14:06:28,198:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:28,205:INFO:Starting cross validation
2024-11-28 14:06:28,207:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:29,419:INFO:Calculating mean and std
2024-11-28 14:06:29,419:INFO:Creating metrics dataframe
2024-11-28 14:06:29,419:INFO:Finalizing model
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:29,529:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:06:29,529:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000225 seconds.
2024-11-28 14:06:29,529:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:29,529:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:29,529:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:06:29,529:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:06:29,529:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:06:29,529:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:29,576:INFO:Uploading results into container
2024-11-28 14:06:29,576:INFO:Uploading model into container now
2024-11-28 14:06:29,592:INFO:_master_model_container: 20
2024-11-28 14:06:29,592:INFO:_display_container: 7
2024-11-28 14:06:29,592:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:29,592:INFO:create_model() successfully completed......................................
2024-11-28 14:06:29,767:INFO:Initializing tune_model()
2024-11-28 14:06:29,767:INFO:tune_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>)
2024-11-28 14:06:29,767:INFO:Checking exceptions
2024-11-28 14:06:29,784:INFO:Copying training dataset
2024-11-28 14:06:29,784:INFO:Checking base model
2024-11-28 14:06:29,784:INFO:Base model : Gradient Boosting Classifier
2024-11-28 14:06:29,799:INFO:Declaring metric variables
2024-11-28 14:06:29,802:INFO:Defining Hyperparameters
2024-11-28 14:06:29,896:INFO:Tuning with n_jobs=-1
2024-11-28 14:06:29,896:INFO:Initializing RandomizedSearchCV
2024-11-28 14:06:37,564:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__n_estimators': 270, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.3, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 6, 'actual_estimator__learning_rate': 0.05}
2024-11-28 14:06:37,564:INFO:Hyperparameter search completed
2024-11-28 14:06:37,564:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:37,564:INFO:Initializing create_model()
2024-11-28 14:06:37,564:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39425840>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'n_estimators': 270, 'min_samples_split': 10, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.3, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.05})
2024-11-28 14:06:37,564:INFO:Checking exceptions
2024-11-28 14:06:37,564:INFO:Importing libraries
2024-11-28 14:06:37,564:INFO:Copying training dataset
2024-11-28 14:06:37,570:INFO:Defining folds
2024-11-28 14:06:37,570:INFO:Declaring metric variables
2024-11-28 14:06:37,570:INFO:Importing untrained model
2024-11-28 14:06:37,576:INFO:Declaring custom model
2024-11-28 14:06:37,578:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:06:37,582:INFO:Starting cross validation
2024-11-28 14:06:37,582:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:38,526:INFO:Calculating mean and std
2024-11-28 14:06:38,526:INFO:Creating metrics dataframe
2024-11-28 14:06:38,526:INFO:Finalizing model
2024-11-28 14:06:38,770:INFO:Uploading results into container
2024-11-28 14:06:38,771:INFO:Uploading model into container now
2024-11-28 14:06:38,771:INFO:_master_model_container: 21
2024-11-28 14:06:38,771:INFO:_display_container: 8
2024-11-28 14:06:38,773:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:06:38,773:INFO:create_model() successfully completed......................................
2024-11-28 14:06:38,867:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:38,867:INFO:choose_better activated
2024-11-28 14:06:38,870:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:38,870:INFO:Initializing create_model()
2024-11-28 14:06:38,871:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:38,871:INFO:Checking exceptions
2024-11-28 14:06:38,873:INFO:Importing libraries
2024-11-28 14:06:38,873:INFO:Copying training dataset
2024-11-28 14:06:38,873:INFO:Defining folds
2024-11-28 14:06:38,873:INFO:Declaring metric variables
2024-11-28 14:06:38,873:INFO:Importing untrained model
2024-11-28 14:06:38,873:INFO:Declaring custom model
2024-11-28 14:06:38,873:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:06:38,873:INFO:Starting cross validation
2024-11-28 14:06:38,873:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:39,560:INFO:Calculating mean and std
2024-11-28 14:06:39,561:INFO:Creating metrics dataframe
2024-11-28 14:06:39,562:INFO:Finalizing model
2024-11-28 14:06:39,749:INFO:Uploading results into container
2024-11-28 14:06:39,749:INFO:Uploading model into container now
2024-11-28 14:06:39,749:INFO:_master_model_container: 22
2024-11-28 14:06:39,749:INFO:_display_container: 9
2024-11-28 14:06:39,749:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:06:39,749:INFO:create_model() successfully completed......................................
2024-11-28 14:06:39,843:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:39,843:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8234
2024-11-28 14:06:39,843:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8379
2024-11-28 14:06:39,843:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2024-11-28 14:06:39,843:INFO:choose_better completed
2024-11-28 14:06:39,843:INFO:_master_model_container: 22
2024-11-28 14:06:39,843:INFO:_display_container: 8
2024-11-28 14:06:39,843:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:06:39,843:INFO:tune_model() successfully completed......................................
2024-11-28 14:06:39,937:INFO:Initializing tune_model()
2024-11-28 14:06:39,937:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>)
2024-11-28 14:06:39,937:INFO:Checking exceptions
2024-11-28 14:06:39,953:INFO:Copying training dataset
2024-11-28 14:06:39,953:INFO:Checking base model
2024-11-28 14:06:39,953:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 14:06:39,982:INFO:Declaring metric variables
2024-11-28 14:06:39,994:INFO:Defining Hyperparameters
2024-11-28 14:06:40,121:INFO:Tuning with n_jobs=-1
2024-11-28 14:06:40,121:INFO:Initializing RandomizedSearchCV
2024-11-28 14:06:48,196:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 14:06:48,196:INFO:Hyperparameter search completed
2024-11-28 14:06:48,196:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:48,196:INFO:Initializing create_model()
2024-11-28 14:06:48,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3D078790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 14:06:48,202:INFO:Checking exceptions
2024-11-28 14:06:48,202:INFO:Importing libraries
2024-11-28 14:06:48,202:INFO:Copying training dataset
2024-11-28 14:06:48,202:INFO:Defining folds
2024-11-28 14:06:48,202:INFO:Declaring metric variables
2024-11-28 14:06:48,202:INFO:Importing untrained model
2024-11-28 14:06:48,202:INFO:Declaring custom model
2024-11-28 14:06:48,212:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:48,212:INFO:Starting cross validation
2024-11-28 14:06:48,212:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:49,087:INFO:Calculating mean and std
2024-11-28 14:06:49,087:INFO:Creating metrics dataframe
2024-11-28 14:06:49,093:INFO:Finalizing model
2024-11-28 14:06:49,197:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:06:49,197:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:06:49,197:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:06:49,203:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:06:49,203:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000172 seconds.
2024-11-28 14:06:49,203:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:49,203:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:49,203:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 14:06:49,203:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 14:06:49,203:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:06:49,203:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,229:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,230:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,231:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:49,233:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:49,236:INFO:Uploading results into container
2024-11-28 14:06:49,236:INFO:Uploading model into container now
2024-11-28 14:06:49,236:INFO:_master_model_container: 23
2024-11-28 14:06:49,236:INFO:_display_container: 9
2024-11-28 14:06:49,236:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:49,236:INFO:create_model() successfully completed......................................
2024-11-28 14:06:49,382:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:49,382:INFO:choose_better activated
2024-11-28 14:06:49,382:INFO:SubProcess create_model() called ==================================
2024-11-28 14:06:49,382:INFO:Initializing create_model()
2024-11-28 14:06:49,382:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:49,382:INFO:Checking exceptions
2024-11-28 14:06:49,382:INFO:Importing libraries
2024-11-28 14:06:49,382:INFO:Copying training dataset
2024-11-28 14:06:49,382:INFO:Defining folds
2024-11-28 14:06:49,382:INFO:Declaring metric variables
2024-11-28 14:06:49,382:INFO:Importing untrained model
2024-11-28 14:06:49,382:INFO:Declaring custom model
2024-11-28 14:06:49,382:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:49,382:INFO:Starting cross validation
2024-11-28 14:06:49,396:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:06:50,740:INFO:Calculating mean and std
2024-11-28 14:06:50,740:INFO:Creating metrics dataframe
2024-11-28 14:06:50,741:INFO:Finalizing model
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:50,843:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:06:50,843:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000182 seconds.
2024-11-28 14:06:50,843:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:50,843:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:50,843:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:06:50,843:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:06:50,843:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:06:50,843:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:50,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,046:INFO:Uploading results into container
2024-11-28 14:06:51,048:INFO:Uploading model into container now
2024-11-28 14:06:51,048:INFO:_master_model_container: 24
2024-11-28 14:06:51,048:INFO:_display_container: 10
2024-11-28 14:06:51,049:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:51,049:INFO:create_model() successfully completed......................................
2024-11-28 14:06:51,193:INFO:SubProcess create_model() end ==================================
2024-11-28 14:06:51,193:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 14:06:51,194:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8411
2024-11-28 14:06:51,195:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 14:06:51,195:INFO:choose_better completed
2024-11-28 14:06:51,206:INFO:_master_model_container: 24
2024-11-28 14:06:51,206:INFO:_display_container: 9
2024-11-28 14:06:51,206:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:51,206:INFO:tune_model() successfully completed......................................
2024-11-28 14:06:51,321:INFO:Initializing predict_model()
2024-11-28 14:06:51,321:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE3FA07E20>)
2024-11-28 14:06:51,321:INFO:Checking exceptions
2024-11-28 14:06:51,321:INFO:Preloading libraries
2024-11-28 14:06:51,338:INFO:Set up data.
2024-11-28 14:06:51,345:INFO:Set up index.
2024-11-28 14:06:51,572:INFO:Initializing finalize_model()
2024-11-28 14:06:51,572:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-28 14:06:51,572:INFO:Finalizing LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:06:51,576:INFO:Initializing create_model()
2024-11-28 14:06:51,576:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:06:51,577:INFO:Checking exceptions
2024-11-28 14:06:51,578:INFO:Importing libraries
2024-11-28 14:06:51,578:INFO:Copying training dataset
2024-11-28 14:06:51,579:INFO:Defining folds
2024-11-28 14:06:51,579:INFO:Declaring metric variables
2024-11-28 14:06:51,580:INFO:Importing untrained model
2024-11-28 14:06:51,580:INFO:Declaring custom model
2024-11-28 14:06:51,580:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:06:51,582:INFO:Cross validation set to False
2024-11-28 14:06:51,582:INFO:Fitting Model
2024-11-28 14:06:51,668:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:06:51,668:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:06:51,668:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:06:51,670:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:06:51,670:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:06:51,670:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:06:51,670:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:06:51,670:INFO:[LightGBM] [Info] Number of positive: 342, number of negative: 549
2024-11-28 14:06:51,671:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000222 seconds.
2024-11-28 14:06:51,671:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:06:51,671:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:06:51,671:INFO:[LightGBM] [Info] Total Bins 423
2024-11-28 14:06:51,671:INFO:[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 23
2024-11-28 14:06:51,671:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288
2024-11-28 14:06:51,671:INFO:[LightGBM] [Info] Start training from score -0.473288
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,690:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,692:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:06:51,692:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:06:51,704:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:06:51,704:INFO:create_model() successfully completed......................................
2024-11-28 14:06:51,821:INFO:_master_model_container: 24
2024-11-28 14:06:51,821:INFO:_display_container: 10
2024-11-28 14:06:51,837:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:06:51,837:INFO:finalize_model() successfully completed......................................
2024-11-28 14:06:51,945:INFO:Initializing save_model()
2024-11-28 14:06:51,945:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), model_name=../Titanic/data/titanic/final_tuned_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-28 14:06:51,945:INFO:Adding model into prep_pipe
2024-11-28 14:06:51,945:WARNING:Only Model saved as it was a pipeline.
2024-11-28 14:06:51,957:INFO:../Titanic/data/titanic/final_tuned_model.pkl saved in current working directory
2024-11-28 14:06:51,962:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:06:51,962:INFO:save_model() successfully completed......................................
2024-11-28 14:07:08,901:INFO:Initializing predict_model()
2024-11-28 14:07:08,901:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE39454160>)
2024-11-28 14:07:08,901:INFO:Checking exceptions
2024-11-28 14:07:08,901:INFO:Preloading libraries
2024-11-28 14:07:08,901:INFO:Set up data.
2024-11-28 14:07:08,915:INFO:Set up index.
2024-11-28 14:07:11,557:INFO:Initializing finalize_model()
2024-11-28 14:07:11,557:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-28 14:07:11,557:INFO:Finalizing LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:07:11,557:INFO:Initializing create_model()
2024-11-28 14:07:11,557:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:07:11,557:INFO:Checking exceptions
2024-11-28 14:07:11,557:INFO:Importing libraries
2024-11-28 14:07:11,557:INFO:Copying training dataset
2024-11-28 14:07:11,557:INFO:Defining folds
2024-11-28 14:07:11,569:INFO:Declaring metric variables
2024-11-28 14:07:11,569:INFO:Importing untrained model
2024-11-28 14:07:11,569:INFO:Declaring custom model
2024-11-28 14:07:11,570:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:07:11,571:INFO:Cross validation set to False
2024-11-28 14:07:11,571:INFO:Fitting Model
2024-11-28 14:07:11,639:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:07:11,639:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:07:11,639:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:07:11,641:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:07:11,641:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:07:11,641:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:07:11,641:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:07:11,642:INFO:[LightGBM] [Info] Number of positive: 342, number of negative: 549
2024-11-28 14:07:11,642:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.
2024-11-28 14:07:11,642:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-28 14:07:11,642:INFO:[LightGBM] [Info] Total Bins 423
2024-11-28 14:07:11,642:INFO:[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 23
2024-11-28 14:07:11,643:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288
2024-11-28 14:07:11,643:INFO:[LightGBM] [Info] Start training from score -0.473288
2024-11-28 14:07:11,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:07:11,657:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:07:11,675:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:07:11,675:INFO:create_model() successfully completed......................................
2024-11-28 14:07:11,788:INFO:_master_model_container: 24
2024-11-28 14:07:11,788:INFO:_display_container: 11
2024-11-28 14:07:11,794:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:07:11,794:INFO:finalize_model() successfully completed......................................
2024-11-28 14:07:11,893:INFO:Initializing save_model()
2024-11-28 14:07:11,893:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), model_name=../Titanic/data/titanic/final_tuned_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-28 14:07:11,893:INFO:Adding model into prep_pipe
2024-11-28 14:07:11,893:WARNING:Only Model saved as it was a pipeline.
2024-11-28 14:07:11,900:INFO:../Titanic/data/titanic/final_tuned_model.pkl saved in current working directory
2024-11-28 14:07:11,900:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:07:11,900:INFO:save_model() successfully completed......................................
2024-11-28 14:07:14,297:INFO:Initializing predict_model()
2024-11-28 14:07:14,298:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FB83940>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.8,
                                importance_type='split', learning_rate=0.2,
                                max_depth=-1, min_child_samples=6,
                                min_child_weight=0.001, min_split_gain=0.6,
                                n_estimators=100, n_jobs=-1, num_leaves=30,
                                objective=None, random_state=42,
                                reg_alpha=0.001, reg_lambda=5, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE3FA07E20>)
2024-11-28 14:07:14,298:INFO:Checking exceptions
2024-11-28 14:07:14,298:INFO:Preloading libraries
2024-11-28 14:07:14,300:INFO:Set up data.
2024-11-28 14:07:14,302:INFO:Set up index.
2024-11-28 14:20:22,009:INFO:PyCaret ClassificationExperiment
2024-11-28 14:20:22,009:INFO:Logging name: clf-default-name
2024-11-28 14:20:22,009:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 14:20:22,009:INFO:version 3.3.2
2024-11-28 14:20:22,009:INFO:Initializing setup()
2024-11-28 14:20:22,009:INFO:self.USI: f649
2024-11-28 14:20:22,009:INFO:self._variable_keys: {'is_multiclass', 'exp_name_log', 'X_test', 'USI', 'y_train', 'n_jobs_param', 'html_param', 'logging_param', 'data', '_ml_usecase', 'gpu_param', 'gpu_n_jobs_param', 'y_test', 'memory', 'pipeline', 'log_plots_param', 'fix_imbalance', 'fold_shuffle_param', 'seed', 'fold_groups_param', 'target_param', 'X', 'idx', 'y', 'X_train', 'exp_id', 'fold_generator', '_available_plots'}
2024-11-28 14:20:22,009:INFO:Checking environment
2024-11-28 14:20:22,009:INFO:python_version: 3.10.7
2024-11-28 14:20:22,009:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 14:20:22,009:INFO:machine: AMD64
2024-11-28 14:20:22,009:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 14:20:22,036:INFO:Memory: svmem(total=16934494208, available=2057482240, percent=87.9, used=14877011968, free=2057482240)
2024-11-28 14:20:22,036:INFO:Physical Core: 4
2024-11-28 14:20:22,037:INFO:Logical Core: 8
2024-11-28 14:20:22,037:INFO:Checking libraries
2024-11-28 14:20:22,037:INFO:System:
2024-11-28 14:20:22,037:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 14:20:22,037:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 14:20:22,037:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 14:20:22,037:INFO:PyCaret required dependencies:
2024-11-28 14:20:22,037:INFO:                 pip: 22.2.2
2024-11-28 14:20:22,037:INFO:          setuptools: 63.2.0
2024-11-28 14:20:22,037:INFO:             pycaret: 3.3.2
2024-11-28 14:20:22,037:INFO:             IPython: 8.28.0
2024-11-28 14:20:22,037:INFO:          ipywidgets: 8.1.5
2024-11-28 14:20:22,037:INFO:                tqdm: 4.66.6
2024-11-28 14:20:22,037:INFO:               numpy: 1.26.4
2024-11-28 14:20:22,038:INFO:              pandas: 2.1.4
2024-11-28 14:20:22,038:INFO:              jinja2: 3.1.4
2024-11-28 14:20:22,038:INFO:               scipy: 1.11.4
2024-11-28 14:20:22,038:INFO:              joblib: 1.4.2
2024-11-28 14:20:22,038:INFO:             sklearn: 1.4.2
2024-11-28 14:20:22,038:INFO:                pyod: 2.0.2
2024-11-28 14:20:22,038:INFO:            imblearn: 0.12.4
2024-11-28 14:20:22,038:INFO:   category_encoders: 2.6.4
2024-11-28 14:20:22,038:INFO:            lightgbm: 4.5.0
2024-11-28 14:20:22,038:INFO:               numba: 0.60.0
2024-11-28 14:20:22,038:INFO:            requests: 2.32.3
2024-11-28 14:20:22,038:INFO:          matplotlib: 3.7.5
2024-11-28 14:20:22,038:INFO:          scikitplot: 0.3.7
2024-11-28 14:20:22,038:INFO:         yellowbrick: 1.5
2024-11-28 14:20:22,038:INFO:              plotly: 5.24.1
2024-11-28 14:20:22,038:INFO:    plotly-resampler: Not installed
2024-11-28 14:20:22,038:INFO:             kaleido: 0.2.1
2024-11-28 14:20:22,038:INFO:           schemdraw: 0.15
2024-11-28 14:20:22,038:INFO:         statsmodels: 0.14.4
2024-11-28 14:20:22,038:INFO:              sktime: 0.26.0
2024-11-28 14:20:22,038:INFO:               tbats: 1.1.3
2024-11-28 14:20:22,038:INFO:            pmdarima: 2.0.4
2024-11-28 14:20:22,039:INFO:              psutil: 6.0.0
2024-11-28 14:20:22,039:INFO:          markupsafe: 3.0.2
2024-11-28 14:20:22,039:INFO:             pickle5: Not installed
2024-11-28 14:20:22,039:INFO:         cloudpickle: 3.1.0
2024-11-28 14:20:22,039:INFO:         deprecation: 2.1.0
2024-11-28 14:20:22,039:INFO:              xxhash: 3.5.0
2024-11-28 14:20:22,039:INFO:           wurlitzer: Not installed
2024-11-28 14:20:22,039:INFO:PyCaret optional dependencies:
2024-11-28 14:20:22,039:INFO:                shap: Not installed
2024-11-28 14:20:22,039:INFO:           interpret: Not installed
2024-11-28 14:20:22,039:INFO:                umap: Not installed
2024-11-28 14:20:22,039:INFO:     ydata_profiling: Not installed
2024-11-28 14:20:22,039:INFO:  explainerdashboard: Not installed
2024-11-28 14:20:22,039:INFO:             autoviz: Not installed
2024-11-28 14:20:22,039:INFO:           fairlearn: Not installed
2024-11-28 14:20:22,039:INFO:          deepchecks: Not installed
2024-11-28 14:20:22,039:INFO:             xgboost: Not installed
2024-11-28 14:20:22,039:INFO:            catboost: Not installed
2024-11-28 14:20:22,040:INFO:              kmodes: Not installed
2024-11-28 14:20:22,040:INFO:             mlxtend: Not installed
2024-11-28 14:20:22,040:INFO:       statsforecast: Not installed
2024-11-28 14:20:22,040:INFO:        tune_sklearn: Not installed
2024-11-28 14:20:22,040:INFO:                 ray: Not installed
2024-11-28 14:20:22,040:INFO:            hyperopt: Not installed
2024-11-28 14:20:22,040:INFO:              optuna: Not installed
2024-11-28 14:20:22,040:INFO:               skopt: Not installed
2024-11-28 14:20:22,040:INFO:              mlflow: Not installed
2024-11-28 14:20:22,040:INFO:              gradio: Not installed
2024-11-28 14:20:22,040:INFO:             fastapi: Not installed
2024-11-28 14:20:22,040:INFO:             uvicorn: Not installed
2024-11-28 14:20:22,040:INFO:              m2cgen: Not installed
2024-11-28 14:20:22,040:INFO:           evidently: Not installed
2024-11-28 14:20:22,040:INFO:               fugue: Not installed
2024-11-28 14:20:22,040:INFO:           streamlit: Not installed
2024-11-28 14:20:22,040:INFO:             prophet: Not installed
2024-11-28 14:20:22,040:INFO:None
2024-11-28 14:20:22,040:INFO:Set up data.
2024-11-28 14:20:22,054:INFO:Set up folding strategy.
2024-11-28 14:20:22,054:INFO:Set up train/test split.
2024-11-28 14:20:22,084:INFO:Set up index.
2024-11-28 14:20:22,085:INFO:Assigning column types.
2024-11-28 14:20:22,088:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 14:20:22,128:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 14:20:22,128:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:20:22,161:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,161:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 14:20:22,202:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:20:22,224:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,224:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,226:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 14:20:22,261:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:20:22,282:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,283:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,311:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:20:22,344:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,344:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,345:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 14:20:22,391:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,391:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,454:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,454:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,454:INFO:Preparing preprocessing pipeline...
2024-11-28 14:20:22,454:INFO:Set up simple imputation.
2024-11-28 14:20:22,454:INFO:Set up encoding of categorical features.
2024-11-28 14:20:22,454:INFO:Set up column transformation.
2024-11-28 14:20:22,454:INFO:Set up feature normalization.
2024-11-28 14:20:22,558:INFO:Finished creating preprocessing pipeline.
2024-11-28 14:20:22,558:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 14:20:22,558:INFO:Creating final display dataframe.
2024-11-28 14:20:22,799:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 17)
4        Transformed data shape         (891, 24)
5   Transformed train set shape         (623, 24)
6    Transformed test set shape         (268, 24)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 3
10     Rows with missing values              3.9%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Transformation              True
18        Transformation method       yeo-johnson
19                    Normalize              True
20             Normalize method            zscore
21               Fold Generator   StratifiedKFold
22                  Fold Number                10
23                     CPU Jobs                -1
24                      Use GPU             False
25               Log Experiment             False
26              Experiment Name  clf-default-name
27                          USI              f649
2024-11-28 14:20:22,858:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,858:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,927:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,927:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:20:22,927:INFO:setup() successfully completed in 0.92s...............
2024-11-28 14:20:23,035:INFO:Initializing compare_models()
2024-11-28 14:20:23,035:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 14:20:23,035:INFO:Checking exceptions
2024-11-28 14:20:23,035:INFO:Preparing display monitor
2024-11-28 14:20:23,062:INFO:Initializing Logistic Regression
2024-11-28 14:20:23,062:INFO:Total runtime is 0.0 minutes
2024-11-28 14:20:23,066:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:23,066:INFO:Initializing create_model()
2024-11-28 14:20:23,066:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:23,066:INFO:Checking exceptions
2024-11-28 14:20:23,066:INFO:Importing libraries
2024-11-28 14:20:23,066:INFO:Copying training dataset
2024-11-28 14:20:23,073:INFO:Defining folds
2024-11-28 14:20:23,073:INFO:Declaring metric variables
2024-11-28 14:20:23,078:INFO:Importing untrained model
2024-11-28 14:20:23,083:INFO:Logistic Regression Imported successfully
2024-11-28 14:20:23,089:INFO:Starting cross validation
2024-11-28 14:20:23,092:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:29,311:INFO:Calculating mean and std
2024-11-28 14:20:29,311:INFO:Creating metrics dataframe
2024-11-28 14:20:29,311:INFO:Uploading results into container
2024-11-28 14:20:29,311:INFO:Uploading model into container now
2024-11-28 14:20:29,311:INFO:_master_model_container: 1
2024-11-28 14:20:29,311:INFO:_display_container: 2
2024-11-28 14:20:29,311:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 14:20:29,311:INFO:create_model() successfully completed......................................
2024-11-28 14:20:29,432:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:29,432:INFO:Creating metrics dataframe
2024-11-28 14:20:29,439:INFO:Initializing K Neighbors Classifier
2024-11-28 14:20:29,439:INFO:Total runtime is 0.10628393093744913 minutes
2024-11-28 14:20:29,444:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:29,444:INFO:Initializing create_model()
2024-11-28 14:20:29,444:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:29,445:INFO:Checking exceptions
2024-11-28 14:20:29,445:INFO:Importing libraries
2024-11-28 14:20:29,445:INFO:Copying training dataset
2024-11-28 14:20:29,449:INFO:Defining folds
2024-11-28 14:20:29,449:INFO:Declaring metric variables
2024-11-28 14:20:29,451:INFO:Importing untrained model
2024-11-28 14:20:29,451:INFO:K Neighbors Classifier Imported successfully
2024-11-28 14:20:29,462:INFO:Starting cross validation
2024-11-28 14:20:29,464:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:30,106:INFO:Calculating mean and std
2024-11-28 14:20:30,106:INFO:Creating metrics dataframe
2024-11-28 14:20:30,106:INFO:Uploading results into container
2024-11-28 14:20:30,106:INFO:Uploading model into container now
2024-11-28 14:20:30,106:INFO:_master_model_container: 2
2024-11-28 14:20:30,106:INFO:_display_container: 2
2024-11-28 14:20:30,106:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 14:20:30,106:INFO:create_model() successfully completed......................................
2024-11-28 14:20:30,205:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:30,205:INFO:Creating metrics dataframe
2024-11-28 14:20:30,227:INFO:Initializing Naive Bayes
2024-11-28 14:20:30,227:INFO:Total runtime is 0.11941194136937458 minutes
2024-11-28 14:20:30,227:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:30,227:INFO:Initializing create_model()
2024-11-28 14:20:30,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:30,227:INFO:Checking exceptions
2024-11-28 14:20:30,227:INFO:Importing libraries
2024-11-28 14:20:30,227:INFO:Copying training dataset
2024-11-28 14:20:30,233:INFO:Defining folds
2024-11-28 14:20:30,233:INFO:Declaring metric variables
2024-11-28 14:20:30,243:INFO:Importing untrained model
2024-11-28 14:20:30,243:INFO:Naive Bayes Imported successfully
2024-11-28 14:20:30,254:INFO:Starting cross validation
2024-11-28 14:20:30,257:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:30,674:INFO:Calculating mean and std
2024-11-28 14:20:30,674:INFO:Creating metrics dataframe
2024-11-28 14:20:30,674:INFO:Uploading results into container
2024-11-28 14:20:30,674:INFO:Uploading model into container now
2024-11-28 14:20:30,674:INFO:_master_model_container: 3
2024-11-28 14:20:30,674:INFO:_display_container: 2
2024-11-28 14:20:30,674:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 14:20:30,674:INFO:create_model() successfully completed......................................
2024-11-28 14:20:30,773:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:30,773:INFO:Creating metrics dataframe
2024-11-28 14:20:30,778:INFO:Initializing Decision Tree Classifier
2024-11-28 14:20:30,778:INFO:Total runtime is 0.12859785556793213 minutes
2024-11-28 14:20:30,806:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:30,807:INFO:Initializing create_model()
2024-11-28 14:20:30,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:30,807:INFO:Checking exceptions
2024-11-28 14:20:30,807:INFO:Importing libraries
2024-11-28 14:20:30,807:INFO:Copying training dataset
2024-11-28 14:20:30,823:INFO:Defining folds
2024-11-28 14:20:30,823:INFO:Declaring metric variables
2024-11-28 14:20:30,827:INFO:Importing untrained model
2024-11-28 14:20:30,833:INFO:Decision Tree Classifier Imported successfully
2024-11-28 14:20:30,842:INFO:Starting cross validation
2024-11-28 14:20:30,842:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:31,253:INFO:Calculating mean and std
2024-11-28 14:20:31,253:INFO:Creating metrics dataframe
2024-11-28 14:20:31,253:INFO:Uploading results into container
2024-11-28 14:20:31,253:INFO:Uploading model into container now
2024-11-28 14:20:31,253:INFO:_master_model_container: 4
2024-11-28 14:20:31,253:INFO:_display_container: 2
2024-11-28 14:20:31,253:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 14:20:31,253:INFO:create_model() successfully completed......................................
2024-11-28 14:20:31,354:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:31,354:INFO:Creating metrics dataframe
2024-11-28 14:20:31,354:INFO:Initializing SVM - Linear Kernel
2024-11-28 14:20:31,354:INFO:Total runtime is 0.13820541699727376 minutes
2024-11-28 14:20:31,366:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:31,366:INFO:Initializing create_model()
2024-11-28 14:20:31,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:31,366:INFO:Checking exceptions
2024-11-28 14:20:31,366:INFO:Importing libraries
2024-11-28 14:20:31,366:INFO:Copying training dataset
2024-11-28 14:20:31,374:INFO:Defining folds
2024-11-28 14:20:31,374:INFO:Declaring metric variables
2024-11-28 14:20:31,375:INFO:Importing untrained model
2024-11-28 14:20:31,375:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 14:20:31,388:INFO:Starting cross validation
2024-11-28 14:20:31,392:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:31,787:INFO:Calculating mean and std
2024-11-28 14:20:31,787:INFO:Creating metrics dataframe
2024-11-28 14:20:31,789:INFO:Uploading results into container
2024-11-28 14:20:31,790:INFO:Uploading model into container now
2024-11-28 14:20:31,790:INFO:_master_model_container: 5
2024-11-28 14:20:31,790:INFO:_display_container: 2
2024-11-28 14:20:31,790:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 14:20:31,790:INFO:create_model() successfully completed......................................
2024-11-28 14:20:31,882:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:31,882:INFO:Creating metrics dataframe
2024-11-28 14:20:31,893:INFO:Initializing Ridge Classifier
2024-11-28 14:20:31,893:INFO:Total runtime is 0.14718540906906127 minutes
2024-11-28 14:20:31,896:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:31,897:INFO:Initializing create_model()
2024-11-28 14:20:31,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:31,897:INFO:Checking exceptions
2024-11-28 14:20:31,897:INFO:Importing libraries
2024-11-28 14:20:31,897:INFO:Copying training dataset
2024-11-28 14:20:31,905:INFO:Defining folds
2024-11-28 14:20:31,906:INFO:Declaring metric variables
2024-11-28 14:20:31,909:INFO:Importing untrained model
2024-11-28 14:20:31,909:INFO:Ridge Classifier Imported successfully
2024-11-28 14:20:31,925:INFO:Starting cross validation
2024-11-28 14:20:31,927:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:32,303:INFO:Calculating mean and std
2024-11-28 14:20:32,304:INFO:Creating metrics dataframe
2024-11-28 14:20:32,305:INFO:Uploading results into container
2024-11-28 14:20:32,305:INFO:Uploading model into container now
2024-11-28 14:20:32,306:INFO:_master_model_container: 6
2024-11-28 14:20:32,306:INFO:_display_container: 2
2024-11-28 14:20:32,306:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 14:20:32,307:INFO:create_model() successfully completed......................................
2024-11-28 14:20:32,413:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:32,413:INFO:Creating metrics dataframe
2024-11-28 14:20:32,421:INFO:Initializing Random Forest Classifier
2024-11-28 14:20:32,421:INFO:Total runtime is 0.15598270893096924 minutes
2024-11-28 14:20:32,424:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:32,424:INFO:Initializing create_model()
2024-11-28 14:20:32,424:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:32,424:INFO:Checking exceptions
2024-11-28 14:20:32,424:INFO:Importing libraries
2024-11-28 14:20:32,424:INFO:Copying training dataset
2024-11-28 14:20:32,429:INFO:Defining folds
2024-11-28 14:20:32,430:INFO:Declaring metric variables
2024-11-28 14:20:32,434:INFO:Importing untrained model
2024-11-28 14:20:32,439:INFO:Random Forest Classifier Imported successfully
2024-11-28 14:20:32,439:INFO:Starting cross validation
2024-11-28 14:20:32,439:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:33,476:INFO:Calculating mean and std
2024-11-28 14:20:33,477:INFO:Creating metrics dataframe
2024-11-28 14:20:33,479:INFO:Uploading results into container
2024-11-28 14:20:33,480:INFO:Uploading model into container now
2024-11-28 14:20:33,481:INFO:_master_model_container: 7
2024-11-28 14:20:33,481:INFO:_display_container: 2
2024-11-28 14:20:33,481:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 14:20:33,481:INFO:create_model() successfully completed......................................
2024-11-28 14:20:33,575:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:33,575:INFO:Creating metrics dataframe
2024-11-28 14:20:33,583:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 14:20:33,583:INFO:Total runtime is 0.1753416657447815 minutes
2024-11-28 14:20:33,587:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:33,587:INFO:Initializing create_model()
2024-11-28 14:20:33,588:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:33,588:INFO:Checking exceptions
2024-11-28 14:20:33,588:INFO:Importing libraries
2024-11-28 14:20:33,588:INFO:Copying training dataset
2024-11-28 14:20:33,592:INFO:Defining folds
2024-11-28 14:20:33,592:INFO:Declaring metric variables
2024-11-28 14:20:33,595:INFO:Importing untrained model
2024-11-28 14:20:33,597:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 14:20:33,597:INFO:Starting cross validation
2024-11-28 14:20:33,597:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:33,802:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,812:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,828:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,845:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,845:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,878:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,911:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,980:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:33,992:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:20:34,027:INFO:Calculating mean and std
2024-11-28 14:20:34,028:INFO:Creating metrics dataframe
2024-11-28 14:20:34,028:INFO:Uploading results into container
2024-11-28 14:20:34,028:INFO:Uploading model into container now
2024-11-28 14:20:34,028:INFO:_master_model_container: 8
2024-11-28 14:20:34,028:INFO:_display_container: 2
2024-11-28 14:20:34,028:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 14:20:34,028:INFO:create_model() successfully completed......................................
2024-11-28 14:20:34,111:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:34,111:INFO:Creating metrics dataframe
2024-11-28 14:20:34,128:INFO:Initializing Ada Boost Classifier
2024-11-28 14:20:34,128:INFO:Total runtime is 0.18443696896235148 minutes
2024-11-28 14:20:34,136:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:34,136:INFO:Initializing create_model()
2024-11-28 14:20:34,136:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:34,136:INFO:Checking exceptions
2024-11-28 14:20:34,136:INFO:Importing libraries
2024-11-28 14:20:34,136:INFO:Copying training dataset
2024-11-28 14:20:34,144:INFO:Defining folds
2024-11-28 14:20:34,144:INFO:Declaring metric variables
2024-11-28 14:20:34,144:INFO:Importing untrained model
2024-11-28 14:20:34,152:INFO:Ada Boost Classifier Imported successfully
2024-11-28 14:20:34,152:INFO:Starting cross validation
2024-11-28 14:20:34,160:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:34,322:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,324:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,328:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,328:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,336:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,336:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,344:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,344:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,681:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,695:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:20:34,808:INFO:Calculating mean and std
2024-11-28 14:20:34,809:INFO:Creating metrics dataframe
2024-11-28 14:20:34,811:INFO:Uploading results into container
2024-11-28 14:20:34,812:INFO:Uploading model into container now
2024-11-28 14:20:34,812:INFO:_master_model_container: 9
2024-11-28 14:20:34,812:INFO:_display_container: 2
2024-11-28 14:20:34,813:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 14:20:34,813:INFO:create_model() successfully completed......................................
2024-11-28 14:20:34,915:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:34,916:INFO:Creating metrics dataframe
2024-11-28 14:20:34,921:INFO:Initializing Gradient Boosting Classifier
2024-11-28 14:20:34,921:INFO:Total runtime is 0.19765587250391642 minutes
2024-11-28 14:20:34,925:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:34,926:INFO:Initializing create_model()
2024-11-28 14:20:34,926:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:34,926:INFO:Checking exceptions
2024-11-28 14:20:34,926:INFO:Importing libraries
2024-11-28 14:20:34,926:INFO:Copying training dataset
2024-11-28 14:20:34,931:INFO:Defining folds
2024-11-28 14:20:34,932:INFO:Declaring metric variables
2024-11-28 14:20:34,935:INFO:Importing untrained model
2024-11-28 14:20:34,939:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:20:34,945:INFO:Starting cross validation
2024-11-28 14:20:34,948:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:35,726:INFO:Calculating mean and std
2024-11-28 14:20:35,727:INFO:Creating metrics dataframe
2024-11-28 14:20:35,728:INFO:Uploading results into container
2024-11-28 14:20:35,729:INFO:Uploading model into container now
2024-11-28 14:20:35,729:INFO:_master_model_container: 10
2024-11-28 14:20:35,729:INFO:_display_container: 2
2024-11-28 14:20:35,730:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:20:35,730:INFO:create_model() successfully completed......................................
2024-11-28 14:20:35,823:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:35,823:INFO:Creating metrics dataframe
2024-11-28 14:20:35,823:INFO:Initializing Linear Discriminant Analysis
2024-11-28 14:20:35,823:INFO:Total runtime is 0.21268248160680134 minutes
2024-11-28 14:20:35,837:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:35,837:INFO:Initializing create_model()
2024-11-28 14:20:35,837:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:35,838:INFO:Checking exceptions
2024-11-28 14:20:35,838:INFO:Importing libraries
2024-11-28 14:20:35,838:INFO:Copying training dataset
2024-11-28 14:20:35,842:INFO:Defining folds
2024-11-28 14:20:35,842:INFO:Declaring metric variables
2024-11-28 14:20:35,845:INFO:Importing untrained model
2024-11-28 14:20:35,850:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 14:20:35,855:INFO:Starting cross validation
2024-11-28 14:20:35,857:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:36,336:INFO:Calculating mean and std
2024-11-28 14:20:36,336:INFO:Creating metrics dataframe
2024-11-28 14:20:36,341:INFO:Uploading results into container
2024-11-28 14:20:36,342:INFO:Uploading model into container now
2024-11-28 14:20:36,343:INFO:_master_model_container: 11
2024-11-28 14:20:36,343:INFO:_display_container: 2
2024-11-28 14:20:36,344:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 14:20:36,344:INFO:create_model() successfully completed......................................
2024-11-28 14:20:36,470:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:36,470:INFO:Creating metrics dataframe
2024-11-28 14:20:36,480:INFO:Initializing Extra Trees Classifier
2024-11-28 14:20:36,480:INFO:Total runtime is 0.22363237142562864 minutes
2024-11-28 14:20:36,485:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:36,485:INFO:Initializing create_model()
2024-11-28 14:20:36,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:36,485:INFO:Checking exceptions
2024-11-28 14:20:36,485:INFO:Importing libraries
2024-11-28 14:20:36,485:INFO:Copying training dataset
2024-11-28 14:20:36,490:INFO:Defining folds
2024-11-28 14:20:36,490:INFO:Declaring metric variables
2024-11-28 14:20:36,494:INFO:Importing untrained model
2024-11-28 14:20:36,498:INFO:Extra Trees Classifier Imported successfully
2024-11-28 14:20:36,507:INFO:Starting cross validation
2024-11-28 14:20:36,510:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:37,800:INFO:Calculating mean and std
2024-11-28 14:20:37,802:INFO:Creating metrics dataframe
2024-11-28 14:20:37,804:INFO:Uploading results into container
2024-11-28 14:20:37,805:INFO:Uploading model into container now
2024-11-28 14:20:37,805:INFO:_master_model_container: 12
2024-11-28 14:20:37,805:INFO:_display_container: 2
2024-11-28 14:20:37,806:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 14:20:37,806:INFO:create_model() successfully completed......................................
2024-11-28 14:20:37,910:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:37,910:INFO:Creating metrics dataframe
2024-11-28 14:20:37,920:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 14:20:37,920:INFO:Total runtime is 0.24763752222061156 minutes
2024-11-28 14:20:37,924:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:37,924:INFO:Initializing create_model()
2024-11-28 14:20:37,924:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:37,924:INFO:Checking exceptions
2024-11-28 14:20:37,924:INFO:Importing libraries
2024-11-28 14:20:37,924:INFO:Copying training dataset
2024-11-28 14:20:37,927:INFO:Defining folds
2024-11-28 14:20:37,927:INFO:Declaring metric variables
2024-11-28 14:20:37,927:INFO:Importing untrained model
2024-11-28 14:20:37,938:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:20:37,943:INFO:Starting cross validation
2024-11-28 14:20:37,943:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:39,412:INFO:Calculating mean and std
2024-11-28 14:20:39,412:INFO:Creating metrics dataframe
2024-11-28 14:20:39,417:INFO:Uploading results into container
2024-11-28 14:20:39,418:INFO:Uploading model into container now
2024-11-28 14:20:39,419:INFO:_master_model_container: 13
2024-11-28 14:20:39,419:INFO:_display_container: 2
2024-11-28 14:20:39,420:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:20:39,420:INFO:create_model() successfully completed......................................
2024-11-28 14:20:39,547:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:39,548:INFO:Creating metrics dataframe
2024-11-28 14:20:39,555:INFO:Initializing Dummy Classifier
2024-11-28 14:20:39,555:INFO:Total runtime is 0.2748781442642212 minutes
2024-11-28 14:20:39,558:INFO:SubProcess create_model() called ==================================
2024-11-28 14:20:39,558:INFO:Initializing create_model()
2024-11-28 14:20:39,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3FEF9780>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:39,559:INFO:Checking exceptions
2024-11-28 14:20:39,559:INFO:Importing libraries
2024-11-28 14:20:39,559:INFO:Copying training dataset
2024-11-28 14:20:39,564:INFO:Defining folds
2024-11-28 14:20:39,564:INFO:Declaring metric variables
2024-11-28 14:20:39,568:INFO:Importing untrained model
2024-11-28 14:20:39,572:INFO:Dummy Classifier Imported successfully
2024-11-28 14:20:39,577:INFO:Starting cross validation
2024-11-28 14:20:39,579:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:20:39,806:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,812:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,812:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,813:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,829:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,845:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,845:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,845:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,944:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,945:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:20:39,963:INFO:Calculating mean and std
2024-11-28 14:20:39,964:INFO:Creating metrics dataframe
2024-11-28 14:20:39,966:INFO:Uploading results into container
2024-11-28 14:20:39,967:INFO:Uploading model into container now
2024-11-28 14:20:39,967:INFO:_master_model_container: 14
2024-11-28 14:20:39,968:INFO:_display_container: 2
2024-11-28 14:20:39,968:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 14:20:39,968:INFO:create_model() successfully completed......................................
2024-11-28 14:20:40,065:INFO:SubProcess create_model() end ==================================
2024-11-28 14:20:40,065:INFO:Creating metrics dataframe
2024-11-28 14:20:40,080:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 14:20:40,080:INFO:Initializing create_model()
2024-11-28 14:20:40,080:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEF9B70>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:20:40,080:INFO:Checking exceptions
2024-11-28 14:20:40,080:INFO:Importing libraries
2024-11-28 14:20:40,080:INFO:Copying training dataset
2024-11-28 14:20:40,097:INFO:Defining folds
2024-11-28 14:20:40,097:INFO:Declaring metric variables
2024-11-28 14:20:40,097:INFO:Importing untrained model
2024-11-28 14:20:40,097:INFO:Declaring custom model
2024-11-28 14:20:40,097:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:20:40,097:INFO:Cross validation set to False
2024-11-28 14:20:40,097:INFO:Fitting Model
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:20:40,161:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:20:40,161:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.
2024-11-28 14:20:40,161:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:20:40,161:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:20:40,161:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:20:40,161:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:20:40,161:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:20:40,161:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:20:40,211:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:20:40,211:INFO:create_model() successfully completed......................................
2024-11-28 14:20:40,362:INFO:_master_model_container: 14
2024-11-28 14:20:40,362:INFO:_display_container: 2
2024-11-28 14:20:40,362:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:20:40,362:INFO:compare_models() successfully completed......................................
2024-11-28 14:36:11,642:INFO:PyCaret ClassificationExperiment
2024-11-28 14:36:11,642:INFO:Logging name: clf-default-name
2024-11-28 14:36:11,642:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 14:36:11,642:INFO:version 3.3.2
2024-11-28 14:36:11,642:INFO:Initializing setup()
2024-11-28 14:36:11,643:INFO:self.USI: e60b
2024-11-28 14:36:11,643:INFO:self._variable_keys: {'is_multiclass', 'exp_name_log', 'X_test', 'USI', 'y_train', 'n_jobs_param', 'html_param', 'logging_param', 'data', '_ml_usecase', 'gpu_param', 'gpu_n_jobs_param', 'y_test', 'memory', 'pipeline', 'log_plots_param', 'fix_imbalance', 'fold_shuffle_param', 'seed', 'fold_groups_param', 'target_param', 'X', 'idx', 'y', 'X_train', 'exp_id', 'fold_generator', '_available_plots'}
2024-11-28 14:36:11,643:INFO:Checking environment
2024-11-28 14:36:11,643:INFO:python_version: 3.10.7
2024-11-28 14:36:11,643:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 14:36:11,643:INFO:machine: AMD64
2024-11-28 14:36:11,643:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 14:36:11,657:INFO:Memory: svmem(total=16934494208, available=2191122432, percent=87.1, used=14743371776, free=2191122432)
2024-11-28 14:36:11,657:INFO:Physical Core: 4
2024-11-28 14:36:11,657:INFO:Logical Core: 8
2024-11-28 14:36:11,657:INFO:Checking libraries
2024-11-28 14:36:11,658:INFO:System:
2024-11-28 14:36:11,658:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 14:36:11,658:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 14:36:11,658:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 14:36:11,658:INFO:PyCaret required dependencies:
2024-11-28 14:36:11,658:INFO:                 pip: 22.2.2
2024-11-28 14:36:11,658:INFO:          setuptools: 63.2.0
2024-11-28 14:36:11,658:INFO:             pycaret: 3.3.2
2024-11-28 14:36:11,658:INFO:             IPython: 8.28.0
2024-11-28 14:36:11,658:INFO:          ipywidgets: 8.1.5
2024-11-28 14:36:11,658:INFO:                tqdm: 4.66.6
2024-11-28 14:36:11,658:INFO:               numpy: 1.26.4
2024-11-28 14:36:11,658:INFO:              pandas: 2.1.4
2024-11-28 14:36:11,658:INFO:              jinja2: 3.1.4
2024-11-28 14:36:11,658:INFO:               scipy: 1.11.4
2024-11-28 14:36:11,658:INFO:              joblib: 1.4.2
2024-11-28 14:36:11,658:INFO:             sklearn: 1.4.2
2024-11-28 14:36:11,658:INFO:                pyod: 2.0.2
2024-11-28 14:36:11,658:INFO:            imblearn: 0.12.4
2024-11-28 14:36:11,658:INFO:   category_encoders: 2.6.4
2024-11-28 14:36:11,658:INFO:            lightgbm: 4.5.0
2024-11-28 14:36:11,658:INFO:               numba: 0.60.0
2024-11-28 14:36:11,659:INFO:            requests: 2.32.3
2024-11-28 14:36:11,659:INFO:          matplotlib: 3.7.5
2024-11-28 14:36:11,659:INFO:          scikitplot: 0.3.7
2024-11-28 14:36:11,659:INFO:         yellowbrick: 1.5
2024-11-28 14:36:11,659:INFO:              plotly: 5.24.1
2024-11-28 14:36:11,659:INFO:    plotly-resampler: Not installed
2024-11-28 14:36:11,659:INFO:             kaleido: 0.2.1
2024-11-28 14:36:11,659:INFO:           schemdraw: 0.15
2024-11-28 14:36:11,659:INFO:         statsmodels: 0.14.4
2024-11-28 14:36:11,659:INFO:              sktime: 0.26.0
2024-11-28 14:36:11,659:INFO:               tbats: 1.1.3
2024-11-28 14:36:11,659:INFO:            pmdarima: 2.0.4
2024-11-28 14:36:11,659:INFO:              psutil: 6.0.0
2024-11-28 14:36:11,659:INFO:          markupsafe: 3.0.2
2024-11-28 14:36:11,659:INFO:             pickle5: Not installed
2024-11-28 14:36:11,659:INFO:         cloudpickle: 3.1.0
2024-11-28 14:36:11,659:INFO:         deprecation: 2.1.0
2024-11-28 14:36:11,659:INFO:              xxhash: 3.5.0
2024-11-28 14:36:11,659:INFO:           wurlitzer: Not installed
2024-11-28 14:36:11,660:INFO:PyCaret optional dependencies:
2024-11-28 14:36:11,660:INFO:                shap: Not installed
2024-11-28 14:36:11,660:INFO:           interpret: Not installed
2024-11-28 14:36:11,660:INFO:                umap: Not installed
2024-11-28 14:36:11,660:INFO:     ydata_profiling: Not installed
2024-11-28 14:36:11,660:INFO:  explainerdashboard: Not installed
2024-11-28 14:36:11,660:INFO:             autoviz: Not installed
2024-11-28 14:36:11,660:INFO:           fairlearn: Not installed
2024-11-28 14:36:11,660:INFO:          deepchecks: Not installed
2024-11-28 14:36:11,660:INFO:             xgboost: Not installed
2024-11-28 14:36:11,660:INFO:            catboost: Not installed
2024-11-28 14:36:11,660:INFO:              kmodes: Not installed
2024-11-28 14:36:11,660:INFO:             mlxtend: Not installed
2024-11-28 14:36:11,660:INFO:       statsforecast: Not installed
2024-11-28 14:36:11,661:INFO:        tune_sklearn: Not installed
2024-11-28 14:36:11,661:INFO:                 ray: Not installed
2024-11-28 14:36:11,661:INFO:            hyperopt: Not installed
2024-11-28 14:36:11,661:INFO:              optuna: Not installed
2024-11-28 14:36:11,661:INFO:               skopt: Not installed
2024-11-28 14:36:11,661:INFO:              mlflow: Not installed
2024-11-28 14:36:11,661:INFO:              gradio: Not installed
2024-11-28 14:36:11,661:INFO:             fastapi: Not installed
2024-11-28 14:36:11,661:INFO:             uvicorn: Not installed
2024-11-28 14:36:11,661:INFO:              m2cgen: Not installed
2024-11-28 14:36:11,661:INFO:           evidently: Not installed
2024-11-28 14:36:11,661:INFO:               fugue: Not installed
2024-11-28 14:36:11,661:INFO:           streamlit: Not installed
2024-11-28 14:36:11,661:INFO:             prophet: Not installed
2024-11-28 14:36:11,661:INFO:None
2024-11-28 14:36:11,661:INFO:Set up data.
2024-11-28 14:36:11,668:INFO:Set up folding strategy.
2024-11-28 14:36:11,669:INFO:Set up train/test split.
2024-11-28 14:36:11,675:INFO:Set up index.
2024-11-28 14:36:11,675:INFO:Assigning column types.
2024-11-28 14:36:11,678:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 14:36:11,719:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 14:36:11,719:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:36:11,741:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,741:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,786:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 14:36:11,786:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:36:11,815:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,815:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,816:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 14:36:11,859:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:36:11,883:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,884:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,922:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 14:36:11,947:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,948:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:11,948:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 14:36:12,019:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,019:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,084:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,085:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,086:INFO:Preparing preprocessing pipeline...
2024-11-28 14:36:12,087:INFO:Set up simple imputation.
2024-11-28 14:36:12,089:INFO:Set up encoding of categorical features.
2024-11-28 14:36:12,089:INFO:Set up column transformation.
2024-11-28 14:36:12,089:INFO:Set up feature normalization.
2024-11-28 14:36:12,192:INFO:Finished creating preprocessing pipeline.
2024-11-28 14:36:12,201:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 14:36:12,201:INFO:Creating final display dataframe.
2024-11-28 14:36:12,385:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 17)
4        Transformed data shape         (891, 24)
5   Transformed train set shape         (623, 24)
6    Transformed test set shape         (268, 24)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 3
10     Rows with missing values              3.9%
11                   Preprocess              True
12              Imputation type            simple
13           Numeric imputation              mean
14       Categorical imputation              mode
15     Maximum one-hot encoding                25
16              Encoding method              None
17               Transformation              True
18        Transformation method       yeo-johnson
19                    Normalize              True
20             Normalize method            zscore
21               Fold Generator   StratifiedKFold
22                  Fold Number                10
23                     CPU Jobs                -1
24                      Use GPU             False
25               Log Experiment             False
26              Experiment Name  clf-default-name
27                          USI              e60b
2024-11-28 14:36:12,467:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,468:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,534:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,534:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 14:36:12,534:INFO:setup() successfully completed in 0.9s...............
2024-11-28 14:36:12,604:INFO:Initializing compare_models()
2024-11-28 14:36:12,604:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 14:36:12,604:INFO:Checking exceptions
2024-11-28 14:36:12,609:INFO:Preparing display monitor
2024-11-28 14:36:12,652:INFO:Initializing Logistic Regression
2024-11-28 14:36:12,653:INFO:Total runtime is 1.6673405965169272e-05 minutes
2024-11-28 14:36:12,659:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:12,660:INFO:Initializing create_model()
2024-11-28 14:36:12,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:12,660:INFO:Checking exceptions
2024-11-28 14:36:12,660:INFO:Importing libraries
2024-11-28 14:36:12,660:INFO:Copying training dataset
2024-11-28 14:36:12,667:INFO:Defining folds
2024-11-28 14:36:12,667:INFO:Declaring metric variables
2024-11-28 14:36:12,679:INFO:Importing untrained model
2024-11-28 14:36:12,686:INFO:Logistic Regression Imported successfully
2024-11-28 14:36:12,714:INFO:Starting cross validation
2024-11-28 14:36:12,718:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:18,466:INFO:Calculating mean and std
2024-11-28 14:36:18,468:INFO:Creating metrics dataframe
2024-11-28 14:36:18,472:INFO:Uploading results into container
2024-11-28 14:36:18,472:INFO:Uploading model into container now
2024-11-28 14:36:18,473:INFO:_master_model_container: 1
2024-11-28 14:36:18,473:INFO:_display_container: 2
2024-11-28 14:36:18,474:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 14:36:18,474:INFO:create_model() successfully completed......................................
2024-11-28 14:36:18,585:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:18,585:INFO:Creating metrics dataframe
2024-11-28 14:36:18,585:INFO:Initializing K Neighbors Classifier
2024-11-28 14:36:18,585:INFO:Total runtime is 0.09889460404713948 minutes
2024-11-28 14:36:18,601:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:18,601:INFO:Initializing create_model()
2024-11-28 14:36:18,601:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:18,601:INFO:Checking exceptions
2024-11-28 14:36:18,601:INFO:Importing libraries
2024-11-28 14:36:18,601:INFO:Copying training dataset
2024-11-28 14:36:18,601:INFO:Defining folds
2024-11-28 14:36:18,601:INFO:Declaring metric variables
2024-11-28 14:36:18,601:INFO:Importing untrained model
2024-11-28 14:36:18,617:INFO:K Neighbors Classifier Imported successfully
2024-11-28 14:36:18,624:INFO:Starting cross validation
2024-11-28 14:36:18,627:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:19,205:INFO:Calculating mean and std
2024-11-28 14:36:19,205:INFO:Creating metrics dataframe
2024-11-28 14:36:19,205:INFO:Uploading results into container
2024-11-28 14:36:19,205:INFO:Uploading model into container now
2024-11-28 14:36:19,205:INFO:_master_model_container: 2
2024-11-28 14:36:19,205:INFO:_display_container: 2
2024-11-28 14:36:19,205:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 14:36:19,205:INFO:create_model() successfully completed......................................
2024-11-28 14:36:19,304:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:19,304:INFO:Creating metrics dataframe
2024-11-28 14:36:19,317:INFO:Initializing Naive Bayes
2024-11-28 14:36:19,317:INFO:Total runtime is 0.11109209060668945 minutes
2024-11-28 14:36:19,317:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:19,317:INFO:Initializing create_model()
2024-11-28 14:36:19,317:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:19,317:INFO:Checking exceptions
2024-11-28 14:36:19,317:INFO:Importing libraries
2024-11-28 14:36:19,317:INFO:Copying training dataset
2024-11-28 14:36:19,317:INFO:Defining folds
2024-11-28 14:36:19,317:INFO:Declaring metric variables
2024-11-28 14:36:19,332:INFO:Importing untrained model
2024-11-28 14:36:19,335:INFO:Naive Bayes Imported successfully
2024-11-28 14:36:19,335:INFO:Starting cross validation
2024-11-28 14:36:19,335:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:19,744:INFO:Calculating mean and std
2024-11-28 14:36:19,744:INFO:Creating metrics dataframe
2024-11-28 14:36:19,749:INFO:Uploading results into container
2024-11-28 14:36:19,749:INFO:Uploading model into container now
2024-11-28 14:36:19,749:INFO:_master_model_container: 3
2024-11-28 14:36:19,749:INFO:_display_container: 2
2024-11-28 14:36:19,749:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 14:36:19,749:INFO:create_model() successfully completed......................................
2024-11-28 14:36:19,849:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:19,849:INFO:Creating metrics dataframe
2024-11-28 14:36:19,849:INFO:Initializing Decision Tree Classifier
2024-11-28 14:36:19,849:INFO:Total runtime is 0.1199548323949178 minutes
2024-11-28 14:36:19,849:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:19,849:INFO:Initializing create_model()
2024-11-28 14:36:19,849:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:19,849:INFO:Checking exceptions
2024-11-28 14:36:19,849:INFO:Importing libraries
2024-11-28 14:36:19,849:INFO:Copying training dataset
2024-11-28 14:36:19,861:INFO:Defining folds
2024-11-28 14:36:19,861:INFO:Declaring metric variables
2024-11-28 14:36:19,869:INFO:Importing untrained model
2024-11-28 14:36:19,869:INFO:Decision Tree Classifier Imported successfully
2024-11-28 14:36:19,869:INFO:Starting cross validation
2024-11-28 14:36:19,882:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:20,267:INFO:Calculating mean and std
2024-11-28 14:36:20,267:INFO:Creating metrics dataframe
2024-11-28 14:36:20,267:INFO:Uploading results into container
2024-11-28 14:36:20,267:INFO:Uploading model into container now
2024-11-28 14:36:20,267:INFO:_master_model_container: 4
2024-11-28 14:36:20,267:INFO:_display_container: 2
2024-11-28 14:36:20,267:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 14:36:20,267:INFO:create_model() successfully completed......................................
2024-11-28 14:36:20,366:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:20,366:INFO:Creating metrics dataframe
2024-11-28 14:36:20,379:INFO:Initializing SVM - Linear Kernel
2024-11-28 14:36:20,379:INFO:Total runtime is 0.12879135608673095 minutes
2024-11-28 14:36:20,384:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:20,384:INFO:Initializing create_model()
2024-11-28 14:36:20,384:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:20,384:INFO:Checking exceptions
2024-11-28 14:36:20,384:INFO:Importing libraries
2024-11-28 14:36:20,384:INFO:Copying training dataset
2024-11-28 14:36:20,384:INFO:Defining folds
2024-11-28 14:36:20,384:INFO:Declaring metric variables
2024-11-28 14:36:20,384:INFO:Importing untrained model
2024-11-28 14:36:20,396:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 14:36:20,422:INFO:Starting cross validation
2024-11-28 14:36:20,425:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:20,897:INFO:Calculating mean and std
2024-11-28 14:36:20,897:INFO:Creating metrics dataframe
2024-11-28 14:36:20,897:INFO:Uploading results into container
2024-11-28 14:36:20,897:INFO:Uploading model into container now
2024-11-28 14:36:20,897:INFO:_master_model_container: 5
2024-11-28 14:36:20,897:INFO:_display_container: 2
2024-11-28 14:36:20,902:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 14:36:20,902:INFO:create_model() successfully completed......................................
2024-11-28 14:36:21,016:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:21,016:INFO:Creating metrics dataframe
2024-11-28 14:36:21,016:INFO:Initializing Ridge Classifier
2024-11-28 14:36:21,016:INFO:Total runtime is 0.13940333127975463 minutes
2024-11-28 14:36:21,016:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:21,016:INFO:Initializing create_model()
2024-11-28 14:36:21,016:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:21,016:INFO:Checking exceptions
2024-11-28 14:36:21,016:INFO:Importing libraries
2024-11-28 14:36:21,016:INFO:Copying training dataset
2024-11-28 14:36:21,031:INFO:Defining folds
2024-11-28 14:36:21,031:INFO:Declaring metric variables
2024-11-28 14:36:21,036:INFO:Importing untrained model
2024-11-28 14:36:21,036:INFO:Ridge Classifier Imported successfully
2024-11-28 14:36:21,048:INFO:Starting cross validation
2024-11-28 14:36:21,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:21,475:INFO:Calculating mean and std
2024-11-28 14:36:21,476:INFO:Creating metrics dataframe
2024-11-28 14:36:21,479:INFO:Uploading results into container
2024-11-28 14:36:21,479:INFO:Uploading model into container now
2024-11-28 14:36:21,481:INFO:_master_model_container: 6
2024-11-28 14:36:21,481:INFO:_display_container: 2
2024-11-28 14:36:21,481:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 14:36:21,481:INFO:create_model() successfully completed......................................
2024-11-28 14:36:21,581:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:21,581:INFO:Creating metrics dataframe
2024-11-28 14:36:21,595:INFO:Initializing Random Forest Classifier
2024-11-28 14:36:21,595:INFO:Total runtime is 0.14906351566314696 minutes
2024-11-28 14:36:21,598:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:21,598:INFO:Initializing create_model()
2024-11-28 14:36:21,598:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:21,599:INFO:Checking exceptions
2024-11-28 14:36:21,599:INFO:Importing libraries
2024-11-28 14:36:21,599:INFO:Copying training dataset
2024-11-28 14:36:21,603:INFO:Defining folds
2024-11-28 14:36:21,603:INFO:Declaring metric variables
2024-11-28 14:36:21,606:INFO:Importing untrained model
2024-11-28 14:36:21,611:INFO:Random Forest Classifier Imported successfully
2024-11-28 14:36:21,617:INFO:Starting cross validation
2024-11-28 14:36:21,617:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:22,586:INFO:Calculating mean and std
2024-11-28 14:36:22,586:INFO:Creating metrics dataframe
2024-11-28 14:36:22,586:INFO:Uploading results into container
2024-11-28 14:36:22,590:INFO:Uploading model into container now
2024-11-28 14:36:22,590:INFO:_master_model_container: 7
2024-11-28 14:36:22,590:INFO:_display_container: 2
2024-11-28 14:36:22,591:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 14:36:22,591:INFO:create_model() successfully completed......................................
2024-11-28 14:36:22,687:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:22,687:INFO:Creating metrics dataframe
2024-11-28 14:36:22,695:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 14:36:22,695:INFO:Total runtime is 0.16738967100779215 minutes
2024-11-28 14:36:22,695:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:22,695:INFO:Initializing create_model()
2024-11-28 14:36:22,701:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:22,701:INFO:Checking exceptions
2024-11-28 14:36:22,701:INFO:Importing libraries
2024-11-28 14:36:22,701:INFO:Copying training dataset
2024-11-28 14:36:22,704:INFO:Defining folds
2024-11-28 14:36:22,704:INFO:Declaring metric variables
2024-11-28 14:36:22,708:INFO:Importing untrained model
2024-11-28 14:36:22,711:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 14:36:22,711:INFO:Starting cross validation
2024-11-28 14:36:22,711:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:22,895:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,895:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,895:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,895:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,895:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,895:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,911:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:22,924:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:23,046:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:23,048:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 14:36:23,087:INFO:Calculating mean and std
2024-11-28 14:36:23,087:INFO:Creating metrics dataframe
2024-11-28 14:36:23,087:INFO:Uploading results into container
2024-11-28 14:36:23,087:INFO:Uploading model into container now
2024-11-28 14:36:23,087:INFO:_master_model_container: 8
2024-11-28 14:36:23,087:INFO:_display_container: 2
2024-11-28 14:36:23,087:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 14:36:23,087:INFO:create_model() successfully completed......................................
2024-11-28 14:36:23,176:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:23,176:INFO:Creating metrics dataframe
2024-11-28 14:36:23,191:INFO:Initializing Ada Boost Classifier
2024-11-28 14:36:23,191:INFO:Total runtime is 0.17565008401870727 minutes
2024-11-28 14:36:23,191:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:23,191:INFO:Initializing create_model()
2024-11-28 14:36:23,191:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:23,191:INFO:Checking exceptions
2024-11-28 14:36:23,191:INFO:Importing libraries
2024-11-28 14:36:23,191:INFO:Copying training dataset
2024-11-28 14:36:23,207:INFO:Defining folds
2024-11-28 14:36:23,207:INFO:Declaring metric variables
2024-11-28 14:36:23,207:INFO:Importing untrained model
2024-11-28 14:36:23,207:INFO:Ada Boost Classifier Imported successfully
2024-11-28 14:36:23,221:INFO:Starting cross validation
2024-11-28 14:36:23,224:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:23,392:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,392:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,393:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,401:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,404:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,404:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,420:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,452:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,761:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,761:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 14:36:23,886:INFO:Calculating mean and std
2024-11-28 14:36:23,886:INFO:Creating metrics dataframe
2024-11-28 14:36:23,886:INFO:Uploading results into container
2024-11-28 14:36:23,886:INFO:Uploading model into container now
2024-11-28 14:36:23,886:INFO:_master_model_container: 9
2024-11-28 14:36:23,886:INFO:_display_container: 2
2024-11-28 14:36:23,886:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 14:36:23,886:INFO:create_model() successfully completed......................................
2024-11-28 14:36:23,974:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:23,974:INFO:Creating metrics dataframe
2024-11-28 14:36:23,990:INFO:Initializing Gradient Boosting Classifier
2024-11-28 14:36:23,990:INFO:Total runtime is 0.18897156715393065 minutes
2024-11-28 14:36:23,994:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:23,994:INFO:Initializing create_model()
2024-11-28 14:36:23,994:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:23,994:INFO:Checking exceptions
2024-11-28 14:36:23,994:INFO:Importing libraries
2024-11-28 14:36:23,994:INFO:Copying training dataset
2024-11-28 14:36:23,994:INFO:Defining folds
2024-11-28 14:36:23,994:INFO:Declaring metric variables
2024-11-28 14:36:23,994:INFO:Importing untrained model
2024-11-28 14:36:24,006:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:36:24,006:INFO:Starting cross validation
2024-11-28 14:36:24,006:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:24,738:INFO:Calculating mean and std
2024-11-28 14:36:24,738:INFO:Creating metrics dataframe
2024-11-28 14:36:24,738:INFO:Uploading results into container
2024-11-28 14:36:24,738:INFO:Uploading model into container now
2024-11-28 14:36:24,738:INFO:_master_model_container: 10
2024-11-28 14:36:24,738:INFO:_display_container: 2
2024-11-28 14:36:24,738:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:36:24,738:INFO:create_model() successfully completed......................................
2024-11-28 14:36:24,823:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:24,823:INFO:Creating metrics dataframe
2024-11-28 14:36:24,842:INFO:Initializing Linear Discriminant Analysis
2024-11-28 14:36:24,842:INFO:Total runtime is 0.2031781196594238 minutes
2024-11-28 14:36:24,842:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:24,842:INFO:Initializing create_model()
2024-11-28 14:36:24,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:24,842:INFO:Checking exceptions
2024-11-28 14:36:24,842:INFO:Importing libraries
2024-11-28 14:36:24,842:INFO:Copying training dataset
2024-11-28 14:36:24,853:INFO:Defining folds
2024-11-28 14:36:24,853:INFO:Declaring metric variables
2024-11-28 14:36:24,853:INFO:Importing untrained model
2024-11-28 14:36:24,853:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 14:36:24,869:INFO:Starting cross validation
2024-11-28 14:36:24,870:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:25,274:INFO:Calculating mean and std
2024-11-28 14:36:25,274:INFO:Creating metrics dataframe
2024-11-28 14:36:25,274:INFO:Uploading results into container
2024-11-28 14:36:25,274:INFO:Uploading model into container now
2024-11-28 14:36:25,274:INFO:_master_model_container: 11
2024-11-28 14:36:25,274:INFO:_display_container: 2
2024-11-28 14:36:25,274:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 14:36:25,274:INFO:create_model() successfully completed......................................
2024-11-28 14:36:25,369:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:25,369:INFO:Creating metrics dataframe
2024-11-28 14:36:25,369:INFO:Initializing Extra Trees Classifier
2024-11-28 14:36:25,369:INFO:Total runtime is 0.2119485298792521 minutes
2024-11-28 14:36:25,385:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:25,385:INFO:Initializing create_model()
2024-11-28 14:36:25,385:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:25,385:INFO:Checking exceptions
2024-11-28 14:36:25,385:INFO:Importing libraries
2024-11-28 14:36:25,385:INFO:Copying training dataset
2024-11-28 14:36:25,385:INFO:Defining folds
2024-11-28 14:36:25,385:INFO:Declaring metric variables
2024-11-28 14:36:25,385:INFO:Importing untrained model
2024-11-28 14:36:25,398:INFO:Extra Trees Classifier Imported successfully
2024-11-28 14:36:25,405:INFO:Starting cross validation
2024-11-28 14:36:25,407:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:26,265:INFO:Calculating mean and std
2024-11-28 14:36:26,265:INFO:Creating metrics dataframe
2024-11-28 14:36:26,265:INFO:Uploading results into container
2024-11-28 14:36:26,265:INFO:Uploading model into container now
2024-11-28 14:36:26,265:INFO:_master_model_container: 12
2024-11-28 14:36:26,265:INFO:_display_container: 2
2024-11-28 14:36:26,265:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 14:36:26,265:INFO:create_model() successfully completed......................................
2024-11-28 14:36:26,354:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:26,354:INFO:Creating metrics dataframe
2024-11-28 14:36:26,364:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 14:36:26,364:INFO:Total runtime is 0.228541100025177 minutes
2024-11-28 14:36:26,364:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:26,364:INFO:Initializing create_model()
2024-11-28 14:36:26,364:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:26,364:INFO:Checking exceptions
2024-11-28 14:36:26,364:INFO:Importing libraries
2024-11-28 14:36:26,364:INFO:Copying training dataset
2024-11-28 14:36:26,364:INFO:Defining folds
2024-11-28 14:36:26,364:INFO:Declaring metric variables
2024-11-28 14:36:26,380:INFO:Importing untrained model
2024-11-28 14:36:26,380:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:36:26,380:INFO:Starting cross validation
2024-11-28 14:36:26,380:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:27,716:INFO:Calculating mean and std
2024-11-28 14:36:27,716:INFO:Creating metrics dataframe
2024-11-28 14:36:27,716:INFO:Uploading results into container
2024-11-28 14:36:27,716:INFO:Uploading model into container now
2024-11-28 14:36:27,716:INFO:_master_model_container: 13
2024-11-28 14:36:27,716:INFO:_display_container: 2
2024-11-28 14:36:27,723:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:27,723:INFO:create_model() successfully completed......................................
2024-11-28 14:36:27,848:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:27,848:INFO:Creating metrics dataframe
2024-11-28 14:36:27,855:INFO:Initializing Dummy Classifier
2024-11-28 14:36:27,855:INFO:Total runtime is 0.253385321299235 minutes
2024-11-28 14:36:27,864:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:27,864:INFO:Initializing create_model()
2024-11-28 14:36:27,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3E8B84C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:27,864:INFO:Checking exceptions
2024-11-28 14:36:27,864:INFO:Importing libraries
2024-11-28 14:36:27,864:INFO:Copying training dataset
2024-11-28 14:36:27,864:INFO:Defining folds
2024-11-28 14:36:27,864:INFO:Declaring metric variables
2024-11-28 14:36:27,864:INFO:Importing untrained model
2024-11-28 14:36:27,864:INFO:Dummy Classifier Imported successfully
2024-11-28 14:36:27,883:INFO:Starting cross validation
2024-11-28 14:36:27,886:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:28,092:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,208:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,233:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,234:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,241:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,258:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,307:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,319:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,341:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,358:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 14:36:28,374:INFO:Calculating mean and std
2024-11-28 14:36:28,374:INFO:Creating metrics dataframe
2024-11-28 14:36:28,374:INFO:Uploading results into container
2024-11-28 14:36:28,374:INFO:Uploading model into container now
2024-11-28 14:36:28,374:INFO:_master_model_container: 14
2024-11-28 14:36:28,374:INFO:_display_container: 2
2024-11-28 14:36:28,374:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 14:36:28,374:INFO:create_model() successfully completed......................................
2024-11-28 14:36:28,474:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:28,474:INFO:Creating metrics dataframe
2024-11-28 14:36:28,477:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 14:36:28,493:INFO:Initializing create_model()
2024-11-28 14:36:28,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:28,493:INFO:Checking exceptions
2024-11-28 14:36:28,494:INFO:Importing libraries
2024-11-28 14:36:28,494:INFO:Copying training dataset
2024-11-28 14:36:28,494:INFO:Defining folds
2024-11-28 14:36:28,494:INFO:Declaring metric variables
2024-11-28 14:36:28,494:INFO:Importing untrained model
2024-11-28 14:36:28,494:INFO:Declaring custom model
2024-11-28 14:36:28,494:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:36:28,494:INFO:Cross validation set to False
2024-11-28 14:36:28,494:INFO:Fitting Model
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:36:28,574:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:36:28,574:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.
2024-11-28 14:36:28,574:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:36:28,574:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:36:28,574:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:36:28,574:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:36:28,574:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:36:28,574:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:28,620:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:28,620:INFO:create_model() successfully completed......................................
2024-11-28 14:36:28,764:INFO:_master_model_container: 14
2024-11-28 14:36:28,764:INFO:_display_container: 2
2024-11-28 14:36:28,764:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:28,764:INFO:compare_models() successfully completed......................................
2024-11-28 14:36:28,850:INFO:Initializing tune_model()
2024-11-28 14:36:28,850:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>)
2024-11-28 14:36:28,850:INFO:Checking exceptions
2024-11-28 14:36:28,873:INFO:Copying training dataset
2024-11-28 14:36:28,880:INFO:Checking base model
2024-11-28 14:36:28,881:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 14:36:28,884:INFO:Declaring metric variables
2024-11-28 14:36:28,887:INFO:Defining Hyperparameters
2024-11-28 14:36:28,987:INFO:Tuning with n_jobs=-1
2024-11-28 14:36:28,987:INFO:Initializing RandomizedSearchCV
2024-11-28 14:36:37,317:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 14:36:37,318:INFO:Hyperparameter search completed
2024-11-28 14:36:37,318:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:37,319:INFO:Initializing create_model()
2024-11-28 14:36:37,319:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3D30BEB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 14:36:37,320:INFO:Checking exceptions
2024-11-28 14:36:37,320:INFO:Importing libraries
2024-11-28 14:36:37,320:INFO:Copying training dataset
2024-11-28 14:36:37,328:INFO:Defining folds
2024-11-28 14:36:37,328:INFO:Declaring metric variables
2024-11-28 14:36:37,334:INFO:Importing untrained model
2024-11-28 14:36:37,334:INFO:Declaring custom model
2024-11-28 14:36:37,336:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:36:37,336:INFO:Starting cross validation
2024-11-28 14:36:37,353:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:38,150:INFO:Calculating mean and std
2024-11-28 14:36:38,150:INFO:Creating metrics dataframe
2024-11-28 14:36:38,150:INFO:Finalizing model
2024-11-28 14:36:38,270:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:36:38,271:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:36:38,271:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:36:38,272:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:36:38,273:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:36:38,273:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:36:38,273:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:36:38,273:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:36:38,273:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000269 seconds.
2024-11-28 14:36:38,273:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:36:38,273:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:36:38,273:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 14:36:38,273:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 14:36:38,275:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:36:38,275:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:36:38,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,287:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,289:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,294:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,296:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,297:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,298:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:38,300:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:36:38,315:INFO:Uploading results into container
2024-11-28 14:36:38,317:INFO:Uploading model into container now
2024-11-28 14:36:38,317:INFO:_master_model_container: 15
2024-11-28 14:36:38,317:INFO:_display_container: 3
2024-11-28 14:36:38,317:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:38,317:INFO:create_model() successfully completed......................................
2024-11-28 14:36:38,430:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:38,430:INFO:choose_better activated
2024-11-28 14:36:38,446:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:38,446:INFO:Initializing create_model()
2024-11-28 14:36:38,446:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:38,446:INFO:Checking exceptions
2024-11-28 14:36:38,446:INFO:Importing libraries
2024-11-28 14:36:38,446:INFO:Copying training dataset
2024-11-28 14:36:38,446:INFO:Defining folds
2024-11-28 14:36:38,446:INFO:Declaring metric variables
2024-11-28 14:36:38,446:INFO:Importing untrained model
2024-11-28 14:36:38,446:INFO:Declaring custom model
2024-11-28 14:36:38,446:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:36:38,446:INFO:Starting cross validation
2024-11-28 14:36:38,446:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:39,741:INFO:Calculating mean and std
2024-11-28 14:36:39,742:INFO:Creating metrics dataframe
2024-11-28 14:36:39,743:INFO:Finalizing model
2024-11-28 14:36:39,837:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:36:39,837:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:36:39,837:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.
2024-11-28 14:36:39,837:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:36:39,837:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:36:39,837:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:36:39,837:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:36:39,837:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:36:39,837:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:36:39,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:39,916:INFO:Uploading results into container
2024-11-28 14:36:39,916:INFO:Uploading model into container now
2024-11-28 14:36:39,931:INFO:_master_model_container: 16
2024-11-28 14:36:39,931:INFO:_display_container: 4
2024-11-28 14:36:39,931:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:39,931:INFO:create_model() successfully completed......................................
2024-11-28 14:36:40,055:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:40,055:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 14:36:40,055:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 14:36:40,055:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 14:36:40,055:INFO:choose_better completed
2024-11-28 14:36:40,055:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 14:36:40,055:INFO:_master_model_container: 16
2024-11-28 14:36:40,055:INFO:_display_container: 3
2024-11-28 14:36:40,055:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:40,055:INFO:tune_model() successfully completed......................................
2024-11-28 14:36:40,180:INFO:Initializing evaluate_model()
2024-11-28 14:36:40,180:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-28 14:36:40,190:INFO:Initializing plot_model()
2024-11-28 14:36:40,190:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, system=True)
2024-11-28 14:36:40,190:INFO:Checking exceptions
2024-11-28 14:36:40,193:INFO:Preloading libraries
2024-11-28 14:36:40,205:INFO:Copying training dataset
2024-11-28 14:36:40,205:INFO:Plot type: pipeline
2024-11-28 14:36:40,358:INFO:Visual Rendered Successfully
2024-11-28 14:36:40,461:INFO:plot_model() successfully completed......................................
2024-11-28 14:36:40,506:INFO:Initializing plot_model()
2024-11-28 14:36:40,506:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, system=True)
2024-11-28 14:36:40,506:INFO:Checking exceptions
2024-11-28 14:36:40,511:INFO:Preloading libraries
2024-11-28 14:36:40,521:INFO:Copying training dataset
2024-11-28 14:36:40,521:INFO:Plot type: feature
2024-11-28 14:36:40,522:WARNING:No coef_ found. Trying feature_importances_
2024-11-28 14:36:40,737:INFO:Visual Rendered Successfully
2024-11-28 14:36:40,830:INFO:plot_model() successfully completed......................................
2024-11-28 14:36:40,857:INFO:Initializing create_model()
2024-11-28 14:36:40,857:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:40,858:INFO:Checking exceptions
2024-11-28 14:36:40,876:INFO:Importing libraries
2024-11-28 14:36:40,876:INFO:Copying training dataset
2024-11-28 14:36:40,881:INFO:Defining folds
2024-11-28 14:36:40,881:INFO:Declaring metric variables
2024-11-28 14:36:40,885:INFO:Importing untrained model
2024-11-28 14:36:40,887:INFO:Logistic Regression Imported successfully
2024-11-28 14:36:40,894:INFO:Starting cross validation
2024-11-28 14:36:40,895:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:41,422:INFO:Calculating mean and std
2024-11-28 14:36:41,422:INFO:Creating metrics dataframe
2024-11-28 14:36:41,422:INFO:Finalizing model
2024-11-28 14:36:41,521:INFO:Uploading results into container
2024-11-28 14:36:41,521:INFO:Uploading model into container now
2024-11-28 14:36:41,521:INFO:_master_model_container: 17
2024-11-28 14:36:41,521:INFO:_display_container: 4
2024-11-28 14:36:41,521:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 14:36:41,521:INFO:create_model() successfully completed......................................
2024-11-28 14:36:41,615:INFO:Initializing create_model()
2024-11-28 14:36:41,615:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:41,615:INFO:Checking exceptions
2024-11-28 14:36:41,631:INFO:Importing libraries
2024-11-28 14:36:41,631:INFO:Copying training dataset
2024-11-28 14:36:41,641:INFO:Defining folds
2024-11-28 14:36:41,641:INFO:Declaring metric variables
2024-11-28 14:36:41,644:INFO:Importing untrained model
2024-11-28 14:36:41,646:INFO:Random Forest Classifier Imported successfully
2024-11-28 14:36:41,653:INFO:Starting cross validation
2024-11-28 14:36:41,656:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:42,702:INFO:Calculating mean and std
2024-11-28 14:36:42,703:INFO:Creating metrics dataframe
2024-11-28 14:36:42,710:INFO:Finalizing model
2024-11-28 14:36:42,921:INFO:Uploading results into container
2024-11-28 14:36:42,921:INFO:Uploading model into container now
2024-11-28 14:36:42,921:INFO:_master_model_container: 18
2024-11-28 14:36:42,921:INFO:_display_container: 5
2024-11-28 14:36:42,921:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 14:36:42,921:INFO:create_model() successfully completed......................................
2024-11-28 14:36:43,017:INFO:Initializing create_model()
2024-11-28 14:36:43,017:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:43,017:INFO:Checking exceptions
2024-11-28 14:36:43,017:INFO:Importing libraries
2024-11-28 14:36:43,017:INFO:Copying training dataset
2024-11-28 14:36:43,038:INFO:Defining folds
2024-11-28 14:36:43,038:INFO:Declaring metric variables
2024-11-28 14:36:43,042:INFO:Importing untrained model
2024-11-28 14:36:43,045:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:36:43,053:INFO:Starting cross validation
2024-11-28 14:36:43,054:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:43,914:INFO:Calculating mean and std
2024-11-28 14:36:43,914:INFO:Creating metrics dataframe
2024-11-28 14:36:43,915:INFO:Finalizing model
2024-11-28 14:36:44,103:INFO:Uploading results into container
2024-11-28 14:36:44,104:INFO:Uploading model into container now
2024-11-28 14:36:44,105:INFO:_master_model_container: 19
2024-11-28 14:36:44,105:INFO:_display_container: 6
2024-11-28 14:36:44,105:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:36:44,105:INFO:create_model() successfully completed......................................
2024-11-28 14:36:44,214:INFO:Initializing create_model()
2024-11-28 14:36:44,214:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:44,214:INFO:Checking exceptions
2024-11-28 14:36:44,214:INFO:Importing libraries
2024-11-28 14:36:44,214:INFO:Copying training dataset
2024-11-28 14:36:44,231:INFO:Defining folds
2024-11-28 14:36:44,231:INFO:Declaring metric variables
2024-11-28 14:36:44,235:INFO:Importing untrained model
2024-11-28 14:36:44,238:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:36:44,246:INFO:Starting cross validation
2024-11-28 14:36:44,249:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:45,567:INFO:Calculating mean and std
2024-11-28 14:36:45,567:INFO:Creating metrics dataframe
2024-11-28 14:36:45,575:INFO:Finalizing model
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:36:45,680:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:36:45,680:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000222 seconds.
2024-11-28 14:36:45,680:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:36:45,680:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:36:45,680:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:36:45,680:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:36:45,680:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:36:45,680:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:36:45,732:INFO:Uploading results into container
2024-11-28 14:36:45,732:INFO:Uploading model into container now
2024-11-28 14:36:45,744:INFO:_master_model_container: 20
2024-11-28 14:36:45,744:INFO:_display_container: 7
2024-11-28 14:36:45,746:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:36:45,746:INFO:create_model() successfully completed......................................
2024-11-28 14:36:45,902:INFO:Initializing tune_model()
2024-11-28 14:36:45,902:INFO:tune_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>)
2024-11-28 14:36:45,902:INFO:Checking exceptions
2024-11-28 14:36:45,921:INFO:Copying training dataset
2024-11-28 14:36:45,927:INFO:Checking base model
2024-11-28 14:36:45,927:INFO:Base model : Gradient Boosting Classifier
2024-11-28 14:36:45,930:INFO:Declaring metric variables
2024-11-28 14:36:45,934:INFO:Defining Hyperparameters
2024-11-28 14:36:46,026:INFO:Tuning with n_jobs=-1
2024-11-28 14:36:46,026:INFO:Initializing RandomizedSearchCV
2024-11-28 14:36:53,984:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__n_estimators': 270, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.3, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 6, 'actual_estimator__learning_rate': 0.05}
2024-11-28 14:36:53,984:INFO:Hyperparameter search completed
2024-11-28 14:36:53,984:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:53,984:INFO:Initializing create_model()
2024-11-28 14:36:53,984:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3D3401F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'n_estimators': 270, 'min_samples_split': 10, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.3, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.05})
2024-11-28 14:36:53,984:INFO:Checking exceptions
2024-11-28 14:36:53,984:INFO:Importing libraries
2024-11-28 14:36:53,984:INFO:Copying training dataset
2024-11-28 14:36:53,989:INFO:Defining folds
2024-11-28 14:36:53,989:INFO:Declaring metric variables
2024-11-28 14:36:53,989:INFO:Importing untrained model
2024-11-28 14:36:53,989:INFO:Declaring custom model
2024-11-28 14:36:53,989:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:36:54,000:INFO:Starting cross validation
2024-11-28 14:36:54,000:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:55,033:INFO:Calculating mean and std
2024-11-28 14:36:55,034:INFO:Creating metrics dataframe
2024-11-28 14:36:55,040:INFO:Finalizing model
2024-11-28 14:36:55,282:INFO:Uploading results into container
2024-11-28 14:36:55,283:INFO:Uploading model into container now
2024-11-28 14:36:55,283:INFO:_master_model_container: 21
2024-11-28 14:36:55,284:INFO:_display_container: 8
2024-11-28 14:36:55,284:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:36:55,284:INFO:create_model() successfully completed......................................
2024-11-28 14:36:55,383:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:55,384:INFO:choose_better activated
2024-11-28 14:36:55,385:INFO:SubProcess create_model() called ==================================
2024-11-28 14:36:55,385:INFO:Initializing create_model()
2024-11-28 14:36:55,385:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:36:55,385:INFO:Checking exceptions
2024-11-28 14:36:55,385:INFO:Importing libraries
2024-11-28 14:36:55,385:INFO:Copying training dataset
2024-11-28 14:36:55,391:INFO:Defining folds
2024-11-28 14:36:55,391:INFO:Declaring metric variables
2024-11-28 14:36:55,391:INFO:Importing untrained model
2024-11-28 14:36:55,391:INFO:Declaring custom model
2024-11-28 14:36:55,391:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 14:36:55,391:INFO:Starting cross validation
2024-11-28 14:36:55,391:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:36:56,150:INFO:Calculating mean and std
2024-11-28 14:36:56,150:INFO:Creating metrics dataframe
2024-11-28 14:36:56,150:INFO:Finalizing model
2024-11-28 14:36:56,319:INFO:Uploading results into container
2024-11-28 14:36:56,319:INFO:Uploading model into container now
2024-11-28 14:36:56,319:INFO:_master_model_container: 22
2024-11-28 14:36:56,319:INFO:_display_container: 9
2024-11-28 14:36:56,319:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:36:56,319:INFO:create_model() successfully completed......................................
2024-11-28 14:36:56,413:INFO:SubProcess create_model() end ==================================
2024-11-28 14:36:56,413:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8234
2024-11-28 14:36:56,413:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8395
2024-11-28 14:36:56,413:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2024-11-28 14:36:56,413:INFO:choose_better completed
2024-11-28 14:36:56,436:INFO:_master_model_container: 22
2024-11-28 14:36:56,436:INFO:_display_container: 8
2024-11-28 14:36:56,436:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 14:36:56,436:INFO:tune_model() successfully completed......................................
2024-11-28 14:36:56,540:INFO:Initializing tune_model()
2024-11-28 14:36:56,540:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>)
2024-11-28 14:36:56,540:INFO:Checking exceptions
2024-11-28 14:36:56,554:INFO:Copying training dataset
2024-11-28 14:36:56,559:INFO:Checking base model
2024-11-28 14:36:56,559:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 14:36:56,563:INFO:Declaring metric variables
2024-11-28 14:36:56,568:INFO:Defining Hyperparameters
2024-11-28 14:36:56,684:INFO:Tuning with n_jobs=-1
2024-11-28 14:36:56,684:INFO:Initializing RandomizedSearchCV
2024-11-28 14:37:05,166:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 14:37:05,166:INFO:Hyperparameter search completed
2024-11-28 14:37:05,166:INFO:SubProcess create_model() called ==================================
2024-11-28 14:37:05,166:INFO:Initializing create_model()
2024-11-28 14:37:05,166:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3D30BEB0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 14:37:05,166:INFO:Checking exceptions
2024-11-28 14:37:05,171:INFO:Importing libraries
2024-11-28 14:37:05,171:INFO:Copying training dataset
2024-11-28 14:37:05,175:INFO:Defining folds
2024-11-28 14:37:05,175:INFO:Declaring metric variables
2024-11-28 14:37:05,175:INFO:Importing untrained model
2024-11-28 14:37:05,175:INFO:Declaring custom model
2024-11-28 14:37:05,190:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:37:05,190:INFO:Starting cross validation
2024-11-28 14:37:05,190:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:37:05,997:INFO:Calculating mean and std
2024-11-28 14:37:05,997:INFO:Creating metrics dataframe
2024-11-28 14:37:06,007:INFO:Finalizing model
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 14:37:06,111:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:37:06,111:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000187 seconds.
2024-11-28 14:37:06,111:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:37:06,111:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:37:06,111:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 14:37:06,111:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 14:37:06,111:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:37:06,111:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:06,126:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 14:37:06,149:INFO:Uploading results into container
2024-11-28 14:37:06,150:INFO:Uploading model into container now
2024-11-28 14:37:06,150:INFO:_master_model_container: 23
2024-11-28 14:37:06,150:INFO:_display_container: 9
2024-11-28 14:37:06,150:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:37:06,150:INFO:create_model() successfully completed......................................
2024-11-28 14:37:06,270:INFO:SubProcess create_model() end ==================================
2024-11-28 14:37:06,270:INFO:choose_better activated
2024-11-28 14:37:06,270:INFO:SubProcess create_model() called ==================================
2024-11-28 14:37:06,270:INFO:Initializing create_model()
2024-11-28 14:37:06,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:37:06,270:INFO:Checking exceptions
2024-11-28 14:37:06,285:INFO:Importing libraries
2024-11-28 14:37:06,285:INFO:Copying training dataset
2024-11-28 14:37:06,289:INFO:Defining folds
2024-11-28 14:37:06,289:INFO:Declaring metric variables
2024-11-28 14:37:06,289:INFO:Importing untrained model
2024-11-28 14:37:06,289:INFO:Declaring custom model
2024-11-28 14:37:06,290:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:37:06,290:INFO:Starting cross validation
2024-11-28 14:37:06,291:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 14:37:07,508:INFO:Calculating mean and std
2024-11-28 14:37:07,508:INFO:Creating metrics dataframe
2024-11-28 14:37:07,508:INFO:Finalizing model
2024-11-28 14:37:07,606:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:37:07,606:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 14:37:07,622:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000232 seconds.
2024-11-28 14:37:07,622:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:37:07,622:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:37:07,622:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 14:37:07,622:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 14:37:07,622:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 14:37:07,622:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:07,707:INFO:Uploading results into container
2024-11-28 14:37:07,707:INFO:Uploading model into container now
2024-11-28 14:37:07,707:INFO:_master_model_container: 24
2024-11-28 14:37:07,707:INFO:_display_container: 10
2024-11-28 14:37:07,707:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:37:07,707:INFO:create_model() successfully completed......................................
2024-11-28 14:37:07,839:INFO:SubProcess create_model() end ==================================
2024-11-28 14:37:07,839:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 14:37:07,839:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 14:37:07,839:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 14:37:07,839:INFO:choose_better completed
2024-11-28 14:37:07,839:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 14:37:07,854:INFO:_master_model_container: 24
2024-11-28 14:37:07,854:INFO:_display_container: 9
2024-11-28 14:37:07,854:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:37:07,854:INFO:tune_model() successfully completed......................................
2024-11-28 14:37:08,003:INFO:Initializing predict_model()
2024-11-28 14:37:08,003:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE3FCB4E50>)
2024-11-28 14:37:08,003:INFO:Checking exceptions
2024-11-28 14:37:08,003:INFO:Preloading libraries
2024-11-28 14:37:08,005:INFO:Set up data.
2024-11-28 14:37:08,005:INFO:Set up index.
2024-11-28 14:37:08,237:INFO:Initializing finalize_model()
2024-11-28 14:37:08,237:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-28 14:37:08,237:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 14:37:08,242:INFO:Initializing create_model()
2024-11-28 14:37:08,242:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 14:37:08,242:INFO:Checking exceptions
2024-11-28 14:37:08,242:INFO:Importing libraries
2024-11-28 14:37:08,242:INFO:Copying training dataset
2024-11-28 14:37:08,242:INFO:Defining folds
2024-11-28 14:37:08,242:INFO:Declaring metric variables
2024-11-28 14:37:08,242:INFO:Importing untrained model
2024-11-28 14:37:08,242:INFO:Declaring custom model
2024-11-28 14:37:08,242:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 14:37:08,248:INFO:Cross validation set to False
2024-11-28 14:37:08,248:INFO:Fitting Model
2024-11-28 14:37:08,329:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 14:37:08,330:INFO:[LightGBM] [Info] Number of positive: 342, number of negative: 549
2024-11-28 14:37:08,330:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000232 seconds.
2024-11-28 14:37:08,330:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 14:37:08,330:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 14:37:08,330:INFO:[LightGBM] [Info] Total Bins 423
2024-11-28 14:37:08,331:INFO:[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 23
2024-11-28 14:37:08,331:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288
2024-11-28 14:37:08,331:INFO:[LightGBM] [Info] Start training from score -0.473288
2024-11-28 14:37:08,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 14:37:08,398:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:37:08,398:INFO:create_model() successfully completed......................................
2024-11-28 14:37:08,520:INFO:_master_model_container: 24
2024-11-28 14:37:08,520:INFO:_display_container: 10
2024-11-28 14:37:08,520:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:37:08,520:INFO:finalize_model() successfully completed......................................
2024-11-28 14:37:08,630:INFO:Initializing save_model()
2024-11-28 14:37:08,630:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), model_name=../Titanic/data/titanic/final_tuned_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-28 14:37:08,630:INFO:Adding model into prep_pipe
2024-11-28 14:37:08,630:WARNING:Only Model saved as it was a pipeline.
2024-11-28 14:37:08,645:INFO:../Titanic/data/titanic/final_tuned_model.pkl saved in current working directory
2024-11-28 14:37:08,661:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 14:37:08,661:INFO:save_model() successfully completed......................................
2024-11-28 14:37:17,022:INFO:Initializing predict_model()
2024-11-28 14:37:17,024:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE3FCB4D30>)
2024-11-28 14:37:17,024:INFO:Checking exceptions
2024-11-28 14:37:17,024:INFO:Preloading libraries
2024-11-28 14:37:17,027:INFO:Set up data.
2024-11-28 14:37:17,033:INFO:Set up index.
2024-11-28 14:38:00,099:INFO:Initializing predict_model()
2024-11-28 14:38:00,099:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE3FCB5870>)
2024-11-28 14:38:00,099:INFO:Checking exceptions
2024-11-28 14:38:00,099:INFO:Preloading libraries
2024-11-28 14:38:00,099:INFO:Set up data.
2024-11-28 14:38:00,109:INFO:Set up index.
2024-11-28 15:56:21,538:INFO:Initializing predict_model()
2024-11-28 15:56:21,547:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FEB8580>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE3F389240>)
2024-11-28 15:56:21,547:INFO:Checking exceptions
2024-11-28 15:56:21,547:INFO:Preloading libraries
2024-11-28 15:56:21,551:INFO:Set up data.
2024-11-28 15:56:21,559:INFO:Set up index.
2024-11-28 16:58:14,802:INFO:PyCaret ClassificationExperiment
2024-11-28 16:58:14,803:INFO:Logging name: clf-default-name
2024-11-28 16:58:14,803:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 16:58:14,803:INFO:version 3.3.2
2024-11-28 16:58:14,803:INFO:Initializing setup()
2024-11-28 16:58:14,803:INFO:self.USI: 2fa6
2024-11-28 16:58:14,803:INFO:self._variable_keys: {'is_multiclass', 'exp_name_log', 'X_test', 'USI', 'y_train', 'n_jobs_param', 'html_param', 'logging_param', 'data', '_ml_usecase', 'gpu_param', 'gpu_n_jobs_param', 'y_test', 'memory', 'pipeline', 'log_plots_param', 'fix_imbalance', 'fold_shuffle_param', 'seed', 'fold_groups_param', 'target_param', 'X', 'idx', 'y', 'X_train', 'exp_id', 'fold_generator', '_available_plots'}
2024-11-28 16:58:14,803:INFO:Checking environment
2024-11-28 16:58:14,805:INFO:python_version: 3.10.7
2024-11-28 16:58:14,805:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 16:58:14,805:INFO:machine: AMD64
2024-11-28 16:58:14,805:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 16:58:14,814:INFO:Memory: svmem(total=16934494208, available=2177593344, percent=87.1, used=14756900864, free=2177593344)
2024-11-28 16:58:14,814:INFO:Physical Core: 4
2024-11-28 16:58:14,814:INFO:Logical Core: 8
2024-11-28 16:58:14,814:INFO:Checking libraries
2024-11-28 16:58:14,814:INFO:System:
2024-11-28 16:58:14,814:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 16:58:14,814:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 16:58:14,814:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 16:58:14,814:INFO:PyCaret required dependencies:
2024-11-28 16:58:14,817:INFO:                 pip: 22.2.2
2024-11-28 16:58:14,817:INFO:          setuptools: 63.2.0
2024-11-28 16:58:14,817:INFO:             pycaret: 3.3.2
2024-11-28 16:58:14,817:INFO:             IPython: 8.28.0
2024-11-28 16:58:14,817:INFO:          ipywidgets: 8.1.5
2024-11-28 16:58:14,817:INFO:                tqdm: 4.66.6
2024-11-28 16:58:14,817:INFO:               numpy: 1.26.4
2024-11-28 16:58:14,817:INFO:              pandas: 2.1.4
2024-11-28 16:58:14,817:INFO:              jinja2: 3.1.4
2024-11-28 16:58:14,817:INFO:               scipy: 1.11.4
2024-11-28 16:58:14,817:INFO:              joblib: 1.4.2
2024-11-28 16:58:14,817:INFO:             sklearn: 1.4.2
2024-11-28 16:58:14,817:INFO:                pyod: 2.0.2
2024-11-28 16:58:14,817:INFO:            imblearn: 0.12.4
2024-11-28 16:58:14,817:INFO:   category_encoders: 2.6.4
2024-11-28 16:58:14,817:INFO:            lightgbm: 4.5.0
2024-11-28 16:58:14,818:INFO:               numba: 0.60.0
2024-11-28 16:58:14,818:INFO:            requests: 2.32.3
2024-11-28 16:58:14,818:INFO:          matplotlib: 3.7.5
2024-11-28 16:58:14,818:INFO:          scikitplot: 0.3.7
2024-11-28 16:58:14,818:INFO:         yellowbrick: 1.5
2024-11-28 16:58:14,818:INFO:              plotly: 5.24.1
2024-11-28 16:58:14,818:INFO:    plotly-resampler: Not installed
2024-11-28 16:58:14,818:INFO:             kaleido: 0.2.1
2024-11-28 16:58:14,818:INFO:           schemdraw: 0.15
2024-11-28 16:58:14,818:INFO:         statsmodels: 0.14.4
2024-11-28 16:58:14,818:INFO:              sktime: 0.26.0
2024-11-28 16:58:14,818:INFO:               tbats: 1.1.3
2024-11-28 16:58:14,818:INFO:            pmdarima: 2.0.4
2024-11-28 16:58:14,818:INFO:              psutil: 6.0.0
2024-11-28 16:58:14,818:INFO:          markupsafe: 3.0.2
2024-11-28 16:58:14,818:INFO:             pickle5: Not installed
2024-11-28 16:58:14,818:INFO:         cloudpickle: 3.1.0
2024-11-28 16:58:14,818:INFO:         deprecation: 2.1.0
2024-11-28 16:58:14,818:INFO:              xxhash: 3.5.0
2024-11-28 16:58:14,818:INFO:           wurlitzer: Not installed
2024-11-28 16:58:14,818:INFO:PyCaret optional dependencies:
2024-11-28 16:58:14,818:INFO:                shap: Not installed
2024-11-28 16:58:14,818:INFO:           interpret: Not installed
2024-11-28 16:58:14,818:INFO:                umap: Not installed
2024-11-28 16:58:14,818:INFO:     ydata_profiling: Not installed
2024-11-28 16:58:14,818:INFO:  explainerdashboard: Not installed
2024-11-28 16:58:14,818:INFO:             autoviz: Not installed
2024-11-28 16:58:14,818:INFO:           fairlearn: Not installed
2024-11-28 16:58:14,818:INFO:          deepchecks: Not installed
2024-11-28 16:58:14,818:INFO:             xgboost: Not installed
2024-11-28 16:58:14,818:INFO:            catboost: Not installed
2024-11-28 16:58:14,818:INFO:              kmodes: Not installed
2024-11-28 16:58:14,818:INFO:             mlxtend: Not installed
2024-11-28 16:58:14,818:INFO:       statsforecast: Not installed
2024-11-28 16:58:14,818:INFO:        tune_sklearn: Not installed
2024-11-28 16:58:14,818:INFO:                 ray: Not installed
2024-11-28 16:58:14,818:INFO:            hyperopt: Not installed
2024-11-28 16:58:14,818:INFO:              optuna: Not installed
2024-11-28 16:58:14,818:INFO:               skopt: Not installed
2024-11-28 16:58:14,818:INFO:              mlflow: Not installed
2024-11-28 16:58:14,818:INFO:              gradio: Not installed
2024-11-28 16:58:14,819:INFO:             fastapi: Not installed
2024-11-28 16:58:14,819:INFO:             uvicorn: Not installed
2024-11-28 16:58:14,819:INFO:              m2cgen: Not installed
2024-11-28 16:58:14,819:INFO:           evidently: Not installed
2024-11-28 16:58:14,819:INFO:               fugue: Not installed
2024-11-28 16:58:14,819:INFO:           streamlit: Not installed
2024-11-28 16:58:14,819:INFO:             prophet: Not installed
2024-11-28 16:58:14,819:INFO:None
2024-11-28 16:58:14,819:INFO:Set up data.
2024-11-28 16:58:14,822:INFO:Set up folding strategy.
2024-11-28 16:58:14,822:INFO:Set up train/test split.
2024-11-28 16:58:14,845:INFO:Set up index.
2024-11-28 16:58:14,845:INFO:Assigning column types.
2024-11-28 16:58:14,849:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 16:58:14,883:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 16:58:14,883:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 16:58:14,914:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:14,914:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:14,945:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 16:58:14,961:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 16:58:14,984:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:14,984:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:14,985:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 16:58:15,016:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 16:58:15,032:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,032:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,088:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 16:58:15,107:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,107:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,107:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 16:58:15,166:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,166:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,227:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,227:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,227:INFO:Preparing preprocessing pipeline...
2024-11-28 16:58:15,227:INFO:Set up simple imputation.
2024-11-28 16:58:15,243:INFO:Set up encoding of categorical features.
2024-11-28 16:58:15,243:INFO:Set up column transformation.
2024-11-28 16:58:15,243:INFO:Set up feature normalization.
2024-11-28 16:58:15,400:INFO:Finished creating preprocessing pipeline.
2024-11-28 16:58:15,400:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 16:58:15,400:INFO:Creating final display dataframe.
2024-11-28 16:58:15,592:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 17)
4        Transformed data shape         (891, 24)
5   Transformed train set shape         (623, 24)
6    Transformed test set shape         (268, 24)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Transformation              True
17        Transformation method       yeo-johnson
18                    Normalize              True
19             Normalize method            zscore
20               Fold Generator   StratifiedKFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  clf-default-name
26                          USI              2fa6
2024-11-28 16:58:15,667:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,667:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,728:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,728:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 16:58:15,728:INFO:setup() successfully completed in 0.96s...............
2024-11-28 16:58:15,786:INFO:Initializing compare_models()
2024-11-28 16:58:15,786:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 16:58:15,786:INFO:Checking exceptions
2024-11-28 16:58:15,786:INFO:Preparing display monitor
2024-11-28 16:58:15,820:INFO:Initializing Logistic Regression
2024-11-28 16:58:15,820:INFO:Total runtime is 0.0 minutes
2024-11-28 16:58:15,824:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:15,825:INFO:Initializing create_model()
2024-11-28 16:58:15,825:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:15,825:INFO:Checking exceptions
2024-11-28 16:58:15,825:INFO:Importing libraries
2024-11-28 16:58:15,825:INFO:Copying training dataset
2024-11-28 16:58:15,830:INFO:Defining folds
2024-11-28 16:58:15,830:INFO:Declaring metric variables
2024-11-28 16:58:15,835:INFO:Importing untrained model
2024-11-28 16:58:15,853:INFO:Logistic Regression Imported successfully
2024-11-28 16:58:15,863:INFO:Starting cross validation
2024-11-28 16:58:15,868:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:23,692:INFO:Calculating mean and std
2024-11-28 16:58:23,692:INFO:Creating metrics dataframe
2024-11-28 16:58:23,692:INFO:Uploading results into container
2024-11-28 16:58:23,692:INFO:Uploading model into container now
2024-11-28 16:58:23,692:INFO:_master_model_container: 1
2024-11-28 16:58:23,692:INFO:_display_container: 2
2024-11-28 16:58:23,692:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 16:58:23,692:INFO:create_model() successfully completed......................................
2024-11-28 16:58:24,022:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:24,022:INFO:Creating metrics dataframe
2024-11-28 16:58:24,038:INFO:Initializing K Neighbors Classifier
2024-11-28 16:58:24,038:INFO:Total runtime is 0.1369635303815206 minutes
2024-11-28 16:58:24,038:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:24,038:INFO:Initializing create_model()
2024-11-28 16:58:24,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:24,038:INFO:Checking exceptions
2024-11-28 16:58:24,038:INFO:Importing libraries
2024-11-28 16:58:24,038:INFO:Copying training dataset
2024-11-28 16:58:24,038:INFO:Defining folds
2024-11-28 16:58:24,038:INFO:Declaring metric variables
2024-11-28 16:58:24,038:INFO:Importing untrained model
2024-11-28 16:58:24,054:INFO:K Neighbors Classifier Imported successfully
2024-11-28 16:58:24,054:INFO:Starting cross validation
2024-11-28 16:58:24,054:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:24,565:INFO:Calculating mean and std
2024-11-28 16:58:24,565:INFO:Creating metrics dataframe
2024-11-28 16:58:24,565:INFO:Uploading results into container
2024-11-28 16:58:24,565:INFO:Uploading model into container now
2024-11-28 16:58:24,565:INFO:_master_model_container: 2
2024-11-28 16:58:24,565:INFO:_display_container: 2
2024-11-28 16:58:24,565:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 16:58:24,565:INFO:create_model() successfully completed......................................
2024-11-28 16:58:24,660:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:24,660:INFO:Creating metrics dataframe
2024-11-28 16:58:24,660:INFO:Initializing Naive Bayes
2024-11-28 16:58:24,660:INFO:Total runtime is 0.14732397397359212 minutes
2024-11-28 16:58:24,675:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:24,675:INFO:Initializing create_model()
2024-11-28 16:58:24,675:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:24,675:INFO:Checking exceptions
2024-11-28 16:58:24,675:INFO:Importing libraries
2024-11-28 16:58:24,675:INFO:Copying training dataset
2024-11-28 16:58:24,675:INFO:Defining folds
2024-11-28 16:58:24,675:INFO:Declaring metric variables
2024-11-28 16:58:24,683:INFO:Importing untrained model
2024-11-28 16:58:24,683:INFO:Naive Bayes Imported successfully
2024-11-28 16:58:24,691:INFO:Starting cross validation
2024-11-28 16:58:24,691:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:25,078:INFO:Calculating mean and std
2024-11-28 16:58:25,079:INFO:Creating metrics dataframe
2024-11-28 16:58:25,080:INFO:Uploading results into container
2024-11-28 16:58:25,081:INFO:Uploading model into container now
2024-11-28 16:58:25,081:INFO:_master_model_container: 3
2024-11-28 16:58:25,081:INFO:_display_container: 2
2024-11-28 16:58:25,081:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 16:58:25,081:INFO:create_model() successfully completed......................................
2024-11-28 16:58:25,170:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:25,170:INFO:Creating metrics dataframe
2024-11-28 16:58:25,170:INFO:Initializing Decision Tree Classifier
2024-11-28 16:58:25,170:INFO:Total runtime is 0.1558334986368815 minutes
2024-11-28 16:58:25,184:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:25,184:INFO:Initializing create_model()
2024-11-28 16:58:25,184:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:25,184:INFO:Checking exceptions
2024-11-28 16:58:25,184:INFO:Importing libraries
2024-11-28 16:58:25,184:INFO:Copying training dataset
2024-11-28 16:58:25,189:INFO:Defining folds
2024-11-28 16:58:25,189:INFO:Declaring metric variables
2024-11-28 16:58:25,189:INFO:Importing untrained model
2024-11-28 16:58:25,189:INFO:Decision Tree Classifier Imported successfully
2024-11-28 16:58:25,189:INFO:Starting cross validation
2024-11-28 16:58:25,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:25,550:INFO:Calculating mean and std
2024-11-28 16:58:25,550:INFO:Creating metrics dataframe
2024-11-28 16:58:25,550:INFO:Uploading results into container
2024-11-28 16:58:25,550:INFO:Uploading model into container now
2024-11-28 16:58:25,550:INFO:_master_model_container: 4
2024-11-28 16:58:25,550:INFO:_display_container: 2
2024-11-28 16:58:25,550:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 16:58:25,550:INFO:create_model() successfully completed......................................
2024-11-28 16:58:25,644:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:25,644:INFO:Creating metrics dataframe
2024-11-28 16:58:25,644:INFO:Initializing SVM - Linear Kernel
2024-11-28 16:58:25,644:INFO:Total runtime is 0.16373496452967326 minutes
2024-11-28 16:58:25,660:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:25,660:INFO:Initializing create_model()
2024-11-28 16:58:25,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:25,660:INFO:Checking exceptions
2024-11-28 16:58:25,660:INFO:Importing libraries
2024-11-28 16:58:25,660:INFO:Copying training dataset
2024-11-28 16:58:25,660:INFO:Defining folds
2024-11-28 16:58:25,660:INFO:Declaring metric variables
2024-11-28 16:58:25,660:INFO:Importing untrained model
2024-11-28 16:58:25,660:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 16:58:25,676:INFO:Starting cross validation
2024-11-28 16:58:25,676:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:26,036:INFO:Calculating mean and std
2024-11-28 16:58:26,037:INFO:Creating metrics dataframe
2024-11-28 16:58:26,037:INFO:Uploading results into container
2024-11-28 16:58:26,037:INFO:Uploading model into container now
2024-11-28 16:58:26,037:INFO:_master_model_container: 5
2024-11-28 16:58:26,037:INFO:_display_container: 2
2024-11-28 16:58:26,037:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 16:58:26,037:INFO:create_model() successfully completed......................................
2024-11-28 16:58:26,120:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:26,120:INFO:Creating metrics dataframe
2024-11-28 16:58:26,137:INFO:Initializing Ridge Classifier
2024-11-28 16:58:26,137:INFO:Total runtime is 0.1719408392906189 minutes
2024-11-28 16:58:26,137:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:26,137:INFO:Initializing create_model()
2024-11-28 16:58:26,137:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:26,137:INFO:Checking exceptions
2024-11-28 16:58:26,137:INFO:Importing libraries
2024-11-28 16:58:26,137:INFO:Copying training dataset
2024-11-28 16:58:26,137:INFO:Defining folds
2024-11-28 16:58:26,137:INFO:Declaring metric variables
2024-11-28 16:58:26,137:INFO:Importing untrained model
2024-11-28 16:58:26,153:INFO:Ridge Classifier Imported successfully
2024-11-28 16:58:26,153:INFO:Starting cross validation
2024-11-28 16:58:26,153:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:26,500:INFO:Calculating mean and std
2024-11-28 16:58:26,502:INFO:Creating metrics dataframe
2024-11-28 16:58:26,504:INFO:Uploading results into container
2024-11-28 16:58:26,504:INFO:Uploading model into container now
2024-11-28 16:58:26,505:INFO:_master_model_container: 6
2024-11-28 16:58:26,505:INFO:_display_container: 2
2024-11-28 16:58:26,505:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 16:58:26,505:INFO:create_model() successfully completed......................................
2024-11-28 16:58:26,600:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:26,600:INFO:Creating metrics dataframe
2024-11-28 16:58:26,609:INFO:Initializing Random Forest Classifier
2024-11-28 16:58:26,609:INFO:Total runtime is 0.17980690399805704 minutes
2024-11-28 16:58:26,613:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:26,613:INFO:Initializing create_model()
2024-11-28 16:58:26,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:26,613:INFO:Checking exceptions
2024-11-28 16:58:26,613:INFO:Importing libraries
2024-11-28 16:58:26,613:INFO:Copying training dataset
2024-11-28 16:58:26,619:INFO:Defining folds
2024-11-28 16:58:26,619:INFO:Declaring metric variables
2024-11-28 16:58:26,623:INFO:Importing untrained model
2024-11-28 16:58:26,628:INFO:Random Forest Classifier Imported successfully
2024-11-28 16:58:26,634:INFO:Starting cross validation
2024-11-28 16:58:26,638:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:27,532:INFO:Calculating mean and std
2024-11-28 16:58:27,533:INFO:Creating metrics dataframe
2024-11-28 16:58:27,533:INFO:Uploading results into container
2024-11-28 16:58:27,533:INFO:Uploading model into container now
2024-11-28 16:58:27,533:INFO:_master_model_container: 7
2024-11-28 16:58:27,533:INFO:_display_container: 2
2024-11-28 16:58:27,533:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 16:58:27,533:INFO:create_model() successfully completed......................................
2024-11-28 16:58:27,616:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:27,616:INFO:Creating metrics dataframe
2024-11-28 16:58:27,632:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 16:58:27,632:INFO:Total runtime is 0.19687079588572184 minutes
2024-11-28 16:58:27,632:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:27,632:INFO:Initializing create_model()
2024-11-28 16:58:27,632:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:27,632:INFO:Checking exceptions
2024-11-28 16:58:27,632:INFO:Importing libraries
2024-11-28 16:58:27,632:INFO:Copying training dataset
2024-11-28 16:58:27,649:INFO:Defining folds
2024-11-28 16:58:27,649:INFO:Declaring metric variables
2024-11-28 16:58:27,649:INFO:Importing untrained model
2024-11-28 16:58:27,649:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 16:58:27,649:INFO:Starting cross validation
2024-11-28 16:58:27,665:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,847:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,981:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:27,981:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 16:58:28,031:INFO:Calculating mean and std
2024-11-28 16:58:28,031:INFO:Creating metrics dataframe
2024-11-28 16:58:28,031:INFO:Uploading results into container
2024-11-28 16:58:28,031:INFO:Uploading model into container now
2024-11-28 16:58:28,031:INFO:_master_model_container: 8
2024-11-28 16:58:28,031:INFO:_display_container: 2
2024-11-28 16:58:28,031:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 16:58:28,031:INFO:create_model() successfully completed......................................
2024-11-28 16:58:28,115:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:28,115:INFO:Creating metrics dataframe
2024-11-28 16:58:28,131:INFO:Initializing Ada Boost Classifier
2024-11-28 16:58:28,131:INFO:Total runtime is 0.20518399079640706 minutes
2024-11-28 16:58:28,131:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:28,131:INFO:Initializing create_model()
2024-11-28 16:58:28,131:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:28,131:INFO:Checking exceptions
2024-11-28 16:58:28,131:INFO:Importing libraries
2024-11-28 16:58:28,131:INFO:Copying training dataset
2024-11-28 16:58:28,131:INFO:Defining folds
2024-11-28 16:58:28,131:INFO:Declaring metric variables
2024-11-28 16:58:28,131:INFO:Importing untrained model
2024-11-28 16:58:28,131:INFO:Ada Boost Classifier Imported successfully
2024-11-28 16:58:28,148:INFO:Starting cross validation
2024-11-28 16:58:28,148:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,314:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,596:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,596:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 16:58:28,728:INFO:Calculating mean and std
2024-11-28 16:58:28,730:INFO:Creating metrics dataframe
2024-11-28 16:58:28,730:INFO:Uploading results into container
2024-11-28 16:58:28,730:INFO:Uploading model into container now
2024-11-28 16:58:28,730:INFO:_master_model_container: 9
2024-11-28 16:58:28,730:INFO:_display_container: 2
2024-11-28 16:58:28,730:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 16:58:28,730:INFO:create_model() successfully completed......................................
2024-11-28 16:58:28,813:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:28,813:INFO:Creating metrics dataframe
2024-11-28 16:58:28,829:INFO:Initializing Gradient Boosting Classifier
2024-11-28 16:58:28,829:INFO:Total runtime is 0.21681933005650839 minutes
2024-11-28 16:58:28,829:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:28,829:INFO:Initializing create_model()
2024-11-28 16:58:28,829:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:28,829:INFO:Checking exceptions
2024-11-28 16:58:28,829:INFO:Importing libraries
2024-11-28 16:58:28,829:INFO:Copying training dataset
2024-11-28 16:58:28,829:INFO:Defining folds
2024-11-28 16:58:28,829:INFO:Declaring metric variables
2024-11-28 16:58:28,844:INFO:Importing untrained model
2024-11-28 16:58:28,846:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 16:58:28,846:INFO:Starting cross validation
2024-11-28 16:58:28,846:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:29,576:INFO:Calculating mean and std
2024-11-28 16:58:29,577:INFO:Creating metrics dataframe
2024-11-28 16:58:29,577:INFO:Uploading results into container
2024-11-28 16:58:29,577:INFO:Uploading model into container now
2024-11-28 16:58:29,577:INFO:_master_model_container: 10
2024-11-28 16:58:29,577:INFO:_display_container: 2
2024-11-28 16:58:29,577:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 16:58:29,577:INFO:create_model() successfully completed......................................
2024-11-28 16:58:29,661:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:29,661:INFO:Creating metrics dataframe
2024-11-28 16:58:29,677:INFO:Initializing Linear Discriminant Analysis
2024-11-28 16:58:29,677:INFO:Total runtime is 0.23095046281814577 minutes
2024-11-28 16:58:29,677:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:29,677:INFO:Initializing create_model()
2024-11-28 16:58:29,677:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:29,677:INFO:Checking exceptions
2024-11-28 16:58:29,677:INFO:Importing libraries
2024-11-28 16:58:29,677:INFO:Copying training dataset
2024-11-28 16:58:29,677:INFO:Defining folds
2024-11-28 16:58:29,677:INFO:Declaring metric variables
2024-11-28 16:58:29,690:INFO:Importing untrained model
2024-11-28 16:58:29,694:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 16:58:29,694:INFO:Starting cross validation
2024-11-28 16:58:29,694:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:30,057:INFO:Calculating mean and std
2024-11-28 16:58:30,058:INFO:Creating metrics dataframe
2024-11-28 16:58:30,060:INFO:Uploading results into container
2024-11-28 16:58:30,060:INFO:Uploading model into container now
2024-11-28 16:58:30,060:INFO:_master_model_container: 11
2024-11-28 16:58:30,060:INFO:_display_container: 2
2024-11-28 16:58:30,060:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 16:58:30,060:INFO:create_model() successfully completed......................................
2024-11-28 16:58:30,151:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:30,151:INFO:Creating metrics dataframe
2024-11-28 16:58:30,159:INFO:Initializing Extra Trees Classifier
2024-11-28 16:58:30,159:INFO:Total runtime is 0.238984755674998 minutes
2024-11-28 16:58:30,159:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:30,159:INFO:Initializing create_model()
2024-11-28 16:58:30,159:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:30,159:INFO:Checking exceptions
2024-11-28 16:58:30,159:INFO:Importing libraries
2024-11-28 16:58:30,159:INFO:Copying training dataset
2024-11-28 16:58:30,159:INFO:Defining folds
2024-11-28 16:58:30,159:INFO:Declaring metric variables
2024-11-28 16:58:30,159:INFO:Importing untrained model
2024-11-28 16:58:30,174:INFO:Extra Trees Classifier Imported successfully
2024-11-28 16:58:30,175:INFO:Starting cross validation
2024-11-28 16:58:30,175:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:30,988:INFO:Calculating mean and std
2024-11-28 16:58:30,990:INFO:Creating metrics dataframe
2024-11-28 16:58:30,992:INFO:Uploading results into container
2024-11-28 16:58:30,992:INFO:Uploading model into container now
2024-11-28 16:58:30,992:INFO:_master_model_container: 12
2024-11-28 16:58:30,992:INFO:_display_container: 2
2024-11-28 16:58:30,992:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 16:58:30,992:INFO:create_model() successfully completed......................................
2024-11-28 16:58:31,073:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:31,073:INFO:Creating metrics dataframe
2024-11-28 16:58:31,092:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 16:58:31,092:INFO:Total runtime is 0.2545335094134013 minutes
2024-11-28 16:58:31,092:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:31,092:INFO:Initializing create_model()
2024-11-28 16:58:31,092:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:31,092:INFO:Checking exceptions
2024-11-28 16:58:31,092:INFO:Importing libraries
2024-11-28 16:58:31,092:INFO:Copying training dataset
2024-11-28 16:58:31,092:INFO:Defining folds
2024-11-28 16:58:31,092:INFO:Declaring metric variables
2024-11-28 16:58:31,106:INFO:Importing untrained model
2024-11-28 16:58:31,106:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 16:58:31,106:INFO:Starting cross validation
2024-11-28 16:58:31,106:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:32,253:INFO:Calculating mean and std
2024-11-28 16:58:32,253:INFO:Creating metrics dataframe
2024-11-28 16:58:32,253:INFO:Uploading results into container
2024-11-28 16:58:32,253:INFO:Uploading model into container now
2024-11-28 16:58:32,253:INFO:_master_model_container: 13
2024-11-28 16:58:32,253:INFO:_display_container: 2
2024-11-28 16:58:32,253:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:32,253:INFO:create_model() successfully completed......................................
2024-11-28 16:58:32,370:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:32,370:INFO:Creating metrics dataframe
2024-11-28 16:58:32,370:INFO:Initializing Dummy Classifier
2024-11-28 16:58:32,370:INFO:Total runtime is 0.275826374689738 minutes
2024-11-28 16:58:32,370:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:32,370:INFO:Initializing create_model()
2024-11-28 16:58:32,370:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE39227DF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:32,370:INFO:Checking exceptions
2024-11-28 16:58:32,370:INFO:Importing libraries
2024-11-28 16:58:32,370:INFO:Copying training dataset
2024-11-28 16:58:32,393:INFO:Defining folds
2024-11-28 16:58:32,393:INFO:Declaring metric variables
2024-11-28 16:58:32,402:INFO:Importing untrained model
2024-11-28 16:58:32,420:INFO:Dummy Classifier Imported successfully
2024-11-28 16:58:32,420:INFO:Starting cross validation
2024-11-28 16:58:32,436:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:32,685:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,685:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,685:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,685:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,693:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,700:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,793:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,803:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 16:58:32,817:INFO:Calculating mean and std
2024-11-28 16:58:32,818:INFO:Creating metrics dataframe
2024-11-28 16:58:32,819:INFO:Uploading results into container
2024-11-28 16:58:32,819:INFO:Uploading model into container now
2024-11-28 16:58:32,819:INFO:_master_model_container: 14
2024-11-28 16:58:32,819:INFO:_display_container: 2
2024-11-28 16:58:32,819:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 16:58:32,819:INFO:create_model() successfully completed......................................
2024-11-28 16:58:32,918:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:32,918:INFO:Creating metrics dataframe
2024-11-28 16:58:32,918:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 16:58:32,935:INFO:Initializing create_model()
2024-11-28 16:58:32,935:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:32,935:INFO:Checking exceptions
2024-11-28 16:58:32,935:INFO:Importing libraries
2024-11-28 16:58:32,935:INFO:Copying training dataset
2024-11-28 16:58:32,935:INFO:Defining folds
2024-11-28 16:58:32,935:INFO:Declaring metric variables
2024-11-28 16:58:32,935:INFO:Importing untrained model
2024-11-28 16:58:32,935:INFO:Declaring custom model
2024-11-28 16:58:32,935:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 16:58:32,935:INFO:Cross validation set to False
2024-11-28 16:58:32,935:INFO:Fitting Model
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 16:58:33,018:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 16:58:33,018:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.
2024-11-28 16:58:33,018:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 16:58:33,018:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 16:58:33,018:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 16:58:33,018:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 16:58:33,018:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 16:58:33,018:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:33,051:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:33,051:INFO:create_model() successfully completed......................................
2024-11-28 16:58:33,201:INFO:_master_model_container: 14
2024-11-28 16:58:33,201:INFO:_display_container: 2
2024-11-28 16:58:33,201:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:33,201:INFO:compare_models() successfully completed......................................
2024-11-28 16:58:33,267:INFO:Initializing tune_model()
2024-11-28 16:58:33,267:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>)
2024-11-28 16:58:33,267:INFO:Checking exceptions
2024-11-28 16:58:33,297:INFO:Copying training dataset
2024-11-28 16:58:33,303:INFO:Checking base model
2024-11-28 16:58:33,304:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 16:58:33,308:INFO:Declaring metric variables
2024-11-28 16:58:33,313:INFO:Defining Hyperparameters
2024-11-28 16:58:33,414:INFO:Tuning with n_jobs=-1
2024-11-28 16:58:33,414:INFO:Initializing RandomizedSearchCV
2024-11-28 16:58:41,050:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 16:58:41,050:INFO:Hyperparameter search completed
2024-11-28 16:58:41,050:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:41,050:INFO:Initializing create_model()
2024-11-28 16:58:41,050:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EE3955FA90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 16:58:41,050:INFO:Checking exceptions
2024-11-28 16:58:41,050:INFO:Importing libraries
2024-11-28 16:58:41,050:INFO:Copying training dataset
2024-11-28 16:58:41,050:INFO:Defining folds
2024-11-28 16:58:41,050:INFO:Declaring metric variables
2024-11-28 16:58:41,050:INFO:Importing untrained model
2024-11-28 16:58:41,050:INFO:Declaring custom model
2024-11-28 16:58:41,066:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 16:58:41,066:INFO:Starting cross validation
2024-11-28 16:58:41,081:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:41,803:INFO:Calculating mean and std
2024-11-28 16:58:41,803:INFO:Creating metrics dataframe
2024-11-28 16:58:41,810:INFO:Finalizing model
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 16:58:41,913:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 16:58:41,913:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000254 seconds.
2024-11-28 16:58:41,913:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-11-28 16:58:41,913:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 16:58:41,913:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 16:58:41,913:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 16:58:41,913:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:41,929:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 16:58:41,945:INFO:Uploading results into container
2024-11-28 16:58:41,945:INFO:Uploading model into container now
2024-11-28 16:58:41,945:INFO:_master_model_container: 15
2024-11-28 16:58:41,945:INFO:_display_container: 3
2024-11-28 16:58:41,945:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:41,945:INFO:create_model() successfully completed......................................
2024-11-28 16:58:42,070:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:42,070:INFO:choose_better activated
2024-11-28 16:58:42,070:INFO:SubProcess create_model() called ==================================
2024-11-28 16:58:42,070:INFO:Initializing create_model()
2024-11-28 16:58:42,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:42,070:INFO:Checking exceptions
2024-11-28 16:58:42,070:INFO:Importing libraries
2024-11-28 16:58:42,070:INFO:Copying training dataset
2024-11-28 16:58:42,070:INFO:Defining folds
2024-11-28 16:58:42,070:INFO:Declaring metric variables
2024-11-28 16:58:42,070:INFO:Importing untrained model
2024-11-28 16:58:42,070:INFO:Declaring custom model
2024-11-28 16:58:42,070:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 16:58:42,070:INFO:Starting cross validation
2024-11-28 16:58:42,070:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:43,278:INFO:Calculating mean and std
2024-11-28 16:58:43,278:INFO:Creating metrics dataframe
2024-11-28 16:58:43,278:INFO:Finalizing model
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 16:58:43,373:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 16:58:43,373:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000182 seconds.
2024-11-28 16:58:43,373:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 16:58:43,373:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 16:58:43,373:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 16:58:43,373:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 16:58:43,373:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 16:58:43,373:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:43,421:INFO:Uploading results into container
2024-11-28 16:58:43,421:INFO:Uploading model into container now
2024-11-28 16:58:43,421:INFO:_master_model_container: 16
2024-11-28 16:58:43,421:INFO:_display_container: 4
2024-11-28 16:58:43,421:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:43,421:INFO:create_model() successfully completed......................................
2024-11-28 16:58:43,539:INFO:SubProcess create_model() end ==================================
2024-11-28 16:58:43,539:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 16:58:43,539:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 16:58:43,539:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 16:58:43,539:INFO:choose_better completed
2024-11-28 16:58:43,539:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 16:58:43,555:INFO:_master_model_container: 16
2024-11-28 16:58:43,555:INFO:_display_container: 3
2024-11-28 16:58:43,555:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:43,555:INFO:tune_model() successfully completed......................................
2024-11-28 16:58:43,690:INFO:Initializing evaluate_model()
2024-11-28 16:58:43,690:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-28 16:58:43,690:INFO:Initializing plot_model()
2024-11-28 16:58:43,690:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, system=True)
2024-11-28 16:58:43,690:INFO:Checking exceptions
2024-11-28 16:58:43,690:INFO:Preloading libraries
2024-11-28 16:58:43,712:INFO:Copying training dataset
2024-11-28 16:58:43,712:INFO:Plot type: pipeline
2024-11-28 16:58:43,866:INFO:Visual Rendered Successfully
2024-11-28 16:58:43,954:INFO:plot_model() successfully completed......................................
2024-11-28 16:58:43,993:INFO:Initializing plot_model()
2024-11-28 16:58:43,993:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, system=True)
2024-11-28 16:58:43,993:INFO:Checking exceptions
2024-11-28 16:58:43,998:INFO:Preloading libraries
2024-11-28 16:58:44,038:INFO:Copying training dataset
2024-11-28 16:58:44,039:INFO:Plot type: feature
2024-11-28 16:58:44,039:WARNING:No coef_ found. Trying feature_importances_
2024-11-28 16:58:44,255:INFO:Visual Rendered Successfully
2024-11-28 16:58:44,354:INFO:plot_model() successfully completed......................................
2024-11-28 16:58:44,369:INFO:Initializing create_model()
2024-11-28 16:58:44,369:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:44,369:INFO:Checking exceptions
2024-11-28 16:58:44,397:INFO:Importing libraries
2024-11-28 16:58:44,397:INFO:Copying training dataset
2024-11-28 16:58:44,403:INFO:Defining folds
2024-11-28 16:58:44,403:INFO:Declaring metric variables
2024-11-28 16:58:44,408:INFO:Importing untrained model
2024-11-28 16:58:44,411:INFO:Logistic Regression Imported successfully
2024-11-28 16:58:44,421:INFO:Starting cross validation
2024-11-28 16:58:44,423:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:44,889:INFO:Calculating mean and std
2024-11-28 16:58:44,889:INFO:Creating metrics dataframe
2024-11-28 16:58:44,889:INFO:Finalizing model
2024-11-28 16:58:44,988:INFO:Uploading results into container
2024-11-28 16:58:44,988:INFO:Uploading model into container now
2024-11-28 16:58:44,993:INFO:_master_model_container: 17
2024-11-28 16:58:44,993:INFO:_display_container: 4
2024-11-28 16:58:44,993:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 16:58:44,993:INFO:create_model() successfully completed......................................
2024-11-28 16:58:45,084:INFO:Initializing create_model()
2024-11-28 16:58:45,084:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:45,084:INFO:Checking exceptions
2024-11-28 16:58:45,101:INFO:Importing libraries
2024-11-28 16:58:45,101:INFO:Copying training dataset
2024-11-28 16:58:45,112:INFO:Defining folds
2024-11-28 16:58:45,113:INFO:Declaring metric variables
2024-11-28 16:58:45,115:INFO:Importing untrained model
2024-11-28 16:58:45,121:INFO:Random Forest Classifier Imported successfully
2024-11-28 16:58:45,127:INFO:Starting cross validation
2024-11-28 16:58:45,130:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:46,120:INFO:Calculating mean and std
2024-11-28 16:58:46,120:INFO:Creating metrics dataframe
2024-11-28 16:58:46,120:INFO:Finalizing model
2024-11-28 16:58:46,389:INFO:Uploading results into container
2024-11-28 16:58:46,391:INFO:Uploading model into container now
2024-11-28 16:58:46,399:INFO:_master_model_container: 18
2024-11-28 16:58:46,399:INFO:_display_container: 5
2024-11-28 16:58:46,400:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 16:58:46,400:INFO:create_model() successfully completed......................................
2024-11-28 16:58:46,497:INFO:Initializing create_model()
2024-11-28 16:58:46,497:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:46,497:INFO:Checking exceptions
2024-11-28 16:58:46,497:INFO:Importing libraries
2024-11-28 16:58:46,497:INFO:Copying training dataset
2024-11-28 16:58:46,516:INFO:Defining folds
2024-11-28 16:58:46,516:INFO:Declaring metric variables
2024-11-28 16:58:46,520:INFO:Importing untrained model
2024-11-28 16:58:46,524:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 16:58:46,532:INFO:Starting cross validation
2024-11-28 16:58:46,535:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:47,335:INFO:Calculating mean and std
2024-11-28 16:58:47,335:INFO:Creating metrics dataframe
2024-11-28 16:58:47,335:INFO:Finalizing model
2024-11-28 16:58:47,519:INFO:Uploading results into container
2024-11-28 16:58:47,522:INFO:Uploading model into container now
2024-11-28 16:58:47,532:INFO:_master_model_container: 19
2024-11-28 16:58:47,532:INFO:_display_container: 6
2024-11-28 16:58:47,532:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 16:58:47,532:INFO:create_model() successfully completed......................................
2024-11-28 16:58:47,626:INFO:Initializing create_model()
2024-11-28 16:58:47,626:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:58:47,626:INFO:Checking exceptions
2024-11-28 16:58:47,642:INFO:Importing libraries
2024-11-28 16:58:47,642:INFO:Copying training dataset
2024-11-28 16:58:47,650:INFO:Defining folds
2024-11-28 16:58:47,650:INFO:Declaring metric variables
2024-11-28 16:58:47,654:INFO:Importing untrained model
2024-11-28 16:58:47,658:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 16:58:47,665:INFO:Starting cross validation
2024-11-28 16:58:47,667:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 16:58:48,981:INFO:Calculating mean and std
2024-11-28 16:58:48,981:INFO:Creating metrics dataframe
2024-11-28 16:58:48,981:INFO:Finalizing model
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 16:58:49,090:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 16:58:49,090:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.
2024-11-28 16:58:49,090:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 16:58:49,090:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 16:58:49,090:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 16:58:49,090:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 16:58:49,090:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 16:58:49,090:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:58:49,140:INFO:Uploading results into container
2024-11-28 16:58:49,141:INFO:Uploading model into container now
2024-11-28 16:58:49,143:INFO:_master_model_container: 20
2024-11-28 16:58:49,143:INFO:_display_container: 7
2024-11-28 16:58:49,143:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:58:49,143:INFO:create_model() successfully completed......................................
2024-11-28 16:59:04,124:INFO:Initializing predict_model()
2024-11-28 16:59:04,124:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE39403F40>)
2024-11-28 16:59:04,124:INFO:Checking exceptions
2024-11-28 16:59:04,124:INFO:Preloading libraries
2024-11-28 16:59:04,134:INFO:Set up data.
2024-11-28 16:59:04,141:INFO:Set up index.
2024-11-28 16:59:07,447:INFO:Initializing finalize_model()
2024-11-28 16:59:07,447:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-28 16:59:07,447:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 16:59:07,447:INFO:Initializing create_model()
2024-11-28 16:59:07,447:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 16:59:07,447:INFO:Checking exceptions
2024-11-28 16:59:07,459:INFO:Importing libraries
2024-11-28 16:59:07,459:INFO:Copying training dataset
2024-11-28 16:59:07,459:INFO:Defining folds
2024-11-28 16:59:07,459:INFO:Declaring metric variables
2024-11-28 16:59:07,460:INFO:Importing untrained model
2024-11-28 16:59:07,460:INFO:Declaring custom model
2024-11-28 16:59:07,460:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 16:59:07,462:INFO:Cross validation set to False
2024-11-28 16:59:07,462:INFO:Fitting Model
2024-11-28 16:59:07,524:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 16:59:07,524:INFO:[LightGBM] [Info] Number of positive: 342, number of negative: 549
2024-11-28 16:59:07,524:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000219 seconds.
2024-11-28 16:59:07,524:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 16:59:07,524:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 16:59:07,524:INFO:[LightGBM] [Info] Total Bins 423
2024-11-28 16:59:07,524:INFO:[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 23
2024-11-28 16:59:07,524:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288
2024-11-28 16:59:07,524:INFO:[LightGBM] [Info] Start training from score -0.473288
2024-11-28 16:59:07,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 16:59:07,599:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 16:59:07,599:INFO:create_model() successfully completed......................................
2024-11-28 16:59:07,717:INFO:_master_model_container: 20
2024-11-28 16:59:07,717:INFO:_display_container: 8
2024-11-28 16:59:07,723:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 16:59:07,723:INFO:finalize_model() successfully completed......................................
2024-11-28 16:59:07,823:INFO:Initializing save_model()
2024-11-28 16:59:07,823:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), model_name=../Titanic/data/titanic/final_tuned_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-28 16:59:07,823:INFO:Adding model into prep_pipe
2024-11-28 16:59:07,823:WARNING:Only Model saved as it was a pipeline.
2024-11-28 16:59:07,840:INFO:../Titanic/data/titanic/final_tuned_model.pkl saved in current working directory
2024-11-28 16:59:07,848:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 16:59:07,848:INFO:save_model() successfully completed......................................
2024-11-28 16:59:09,535:INFO:Initializing predict_model()
2024-11-28 16:59:09,535:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EE3FBB3E50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001EE400709D0>)
2024-11-28 16:59:09,535:INFO:Checking exceptions
2024-11-28 16:59:09,535:INFO:Preloading libraries
2024-11-28 16:59:09,535:INFO:Set up data.
2024-11-28 16:59:09,548:INFO:Set up index.
2024-11-28 17:28:43,938:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 17:28:43,939:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 17:28:43,939:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 17:28:43,939:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-28 17:28:44,835:INFO:PyCaret ClassificationExperiment
2024-11-28 17:28:44,835:INFO:Logging name: clf-default-name
2024-11-28 17:28:44,835:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-28 17:28:44,835:INFO:version 3.3.2
2024-11-28 17:28:44,835:INFO:Initializing setup()
2024-11-28 17:28:44,835:INFO:self.USI: 8e31
2024-11-28 17:28:44,835:INFO:self._variable_keys: {'exp_name_log', 'fold_groups_param', 'log_plots_param', 'exp_id', 'n_jobs_param', 'fold_shuffle_param', 'USI', 'target_param', 'is_multiclass', 'seed', '_available_plots', 'y_train', 'y', 'pipeline', 'X_test', 'idx', 'fix_imbalance', 'X', 'fold_generator', 'gpu_n_jobs_param', '_ml_usecase', 'html_param', 'gpu_param', 'data', 'y_test', 'X_train', 'memory', 'logging_param'}
2024-11-28 17:28:44,835:INFO:Checking environment
2024-11-28 17:28:44,835:INFO:python_version: 3.10.7
2024-11-28 17:28:44,835:INFO:python_build: ('tags/v3.10.7:6cc6b13', 'Sep  5 2022 14:08:36')
2024-11-28 17:28:44,836:INFO:machine: AMD64
2024-11-28 17:28:44,836:INFO:platform: Windows-10-10.0.22631-SP0
2024-11-28 17:28:44,848:INFO:Memory: svmem(total=16934494208, available=2498310144, percent=85.2, used=14436184064, free=2498310144)
2024-11-28 17:28:44,848:INFO:Physical Core: 4
2024-11-28 17:28:44,849:INFO:Logical Core: 8
2024-11-28 17:28:44,849:INFO:Checking libraries
2024-11-28 17:28:44,849:INFO:System:
2024-11-28 17:28:44,849:INFO:    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
2024-11-28 17:28:44,849:INFO:executable: c:\Users\alame\AppData\Local\Programs\Python\Python310\python.exe
2024-11-28 17:28:44,849:INFO:   machine: Windows-10-10.0.22631-SP0
2024-11-28 17:28:44,849:INFO:PyCaret required dependencies:
2024-11-28 17:28:44,879:INFO:                 pip: 22.2.2
2024-11-28 17:28:44,880:INFO:          setuptools: 63.2.0
2024-11-28 17:28:44,880:INFO:             pycaret: 3.3.2
2024-11-28 17:28:44,880:INFO:             IPython: 8.28.0
2024-11-28 17:28:44,880:INFO:          ipywidgets: 8.1.5
2024-11-28 17:28:44,880:INFO:                tqdm: 4.66.6
2024-11-28 17:28:44,880:INFO:               numpy: 1.26.4
2024-11-28 17:28:44,880:INFO:              pandas: 2.1.4
2024-11-28 17:28:44,880:INFO:              jinja2: 3.1.4
2024-11-28 17:28:44,880:INFO:               scipy: 1.11.4
2024-11-28 17:28:44,880:INFO:              joblib: 1.4.2
2024-11-28 17:28:44,880:INFO:             sklearn: 1.4.2
2024-11-28 17:28:44,880:INFO:                pyod: 2.0.2
2024-11-28 17:28:44,880:INFO:            imblearn: 0.12.4
2024-11-28 17:28:44,880:INFO:   category_encoders: 2.6.4
2024-11-28 17:28:44,880:INFO:            lightgbm: 4.5.0
2024-11-28 17:28:44,880:INFO:               numba: 0.60.0
2024-11-28 17:28:44,880:INFO:            requests: 2.32.3
2024-11-28 17:28:44,880:INFO:          matplotlib: 3.7.5
2024-11-28 17:28:44,880:INFO:          scikitplot: 0.3.7
2024-11-28 17:28:44,880:INFO:         yellowbrick: 1.5
2024-11-28 17:28:44,880:INFO:              plotly: 5.24.1
2024-11-28 17:28:44,880:INFO:    plotly-resampler: Not installed
2024-11-28 17:28:44,880:INFO:             kaleido: 0.2.1
2024-11-28 17:28:44,880:INFO:           schemdraw: 0.15
2024-11-28 17:28:44,880:INFO:         statsmodels: 0.14.4
2024-11-28 17:28:44,880:INFO:              sktime: 0.26.0
2024-11-28 17:28:44,880:INFO:               tbats: 1.1.3
2024-11-28 17:28:44,880:INFO:            pmdarima: 2.0.4
2024-11-28 17:28:44,880:INFO:              psutil: 6.0.0
2024-11-28 17:28:44,881:INFO:          markupsafe: 3.0.2
2024-11-28 17:28:44,881:INFO:             pickle5: Not installed
2024-11-28 17:28:44,881:INFO:         cloudpickle: 3.1.0
2024-11-28 17:28:44,881:INFO:         deprecation: 2.1.0
2024-11-28 17:28:44,881:INFO:              xxhash: 3.5.0
2024-11-28 17:28:44,881:INFO:           wurlitzer: Not installed
2024-11-28 17:28:44,881:INFO:PyCaret optional dependencies:
2024-11-28 17:28:44,892:INFO:                shap: Not installed
2024-11-28 17:28:44,892:INFO:           interpret: Not installed
2024-11-28 17:28:44,892:INFO:                umap: Not installed
2024-11-28 17:28:44,892:INFO:     ydata_profiling: Not installed
2024-11-28 17:28:44,892:INFO:  explainerdashboard: Not installed
2024-11-28 17:28:44,892:INFO:             autoviz: Not installed
2024-11-28 17:28:44,892:INFO:           fairlearn: Not installed
2024-11-28 17:28:44,892:INFO:          deepchecks: Not installed
2024-11-28 17:28:44,892:INFO:             xgboost: Not installed
2024-11-28 17:28:44,892:INFO:            catboost: Not installed
2024-11-28 17:28:44,892:INFO:              kmodes: Not installed
2024-11-28 17:28:44,893:INFO:             mlxtend: Not installed
2024-11-28 17:28:44,893:INFO:       statsforecast: Not installed
2024-11-28 17:28:44,893:INFO:        tune_sklearn: Not installed
2024-11-28 17:28:44,893:INFO:                 ray: Not installed
2024-11-28 17:28:44,893:INFO:            hyperopt: Not installed
2024-11-28 17:28:44,893:INFO:              optuna: Not installed
2024-11-28 17:28:44,893:INFO:               skopt: Not installed
2024-11-28 17:28:44,893:INFO:              mlflow: Not installed
2024-11-28 17:28:44,893:INFO:              gradio: Not installed
2024-11-28 17:28:44,893:INFO:             fastapi: Not installed
2024-11-28 17:28:44,893:INFO:             uvicorn: Not installed
2024-11-28 17:28:44,893:INFO:              m2cgen: Not installed
2024-11-28 17:28:44,893:INFO:           evidently: Not installed
2024-11-28 17:28:44,893:INFO:               fugue: Not installed
2024-11-28 17:28:44,893:INFO:           streamlit: Not installed
2024-11-28 17:28:44,893:INFO:             prophet: Not installed
2024-11-28 17:28:44,893:INFO:None
2024-11-28 17:28:44,893:INFO:Set up data.
2024-11-28 17:28:44,905:INFO:Set up folding strategy.
2024-11-28 17:28:44,905:INFO:Set up train/test split.
2024-11-28 17:28:44,913:INFO:Set up index.
2024-11-28 17:28:44,914:INFO:Assigning column types.
2024-11-28 17:28:44,917:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-28 17:28:44,952:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 17:28:44,955:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 17:28:44,987:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:44,987:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,022:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-28 17:28:45,022:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 17:28:45,046:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,047:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,047:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-28 17:28:45,086:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 17:28:45,110:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,111:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,149:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-28 17:28:45,170:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,170:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,170:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-28 17:28:45,225:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,226:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,286:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,286:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,287:INFO:Preparing preprocessing pipeline...
2024-11-28 17:28:45,289:INFO:Set up simple imputation.
2024-11-28 17:28:45,291:INFO:Set up encoding of categorical features.
2024-11-28 17:28:45,291:INFO:Set up column transformation.
2024-11-28 17:28:45,291:INFO:Set up feature normalization.
2024-11-28 17:28:45,392:INFO:Finished creating preprocessing pipeline.
2024-11-28 17:28:45,400:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2024-11-28 17:28:45,400:INFO:Creating final display dataframe.
2024-11-28 17:28:45,593:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          Survived
2                   Target type            Binary
3           Original data shape         (891, 17)
4        Transformed data shape         (891, 24)
5   Transformed train set shape         (623, 24)
6    Transformed test set shape         (268, 24)
7               Ignore features                 3
8              Numeric features                10
9          Categorical features                 3
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Transformation              True
17        Transformation method       yeo-johnson
18                    Normalize              True
19             Normalize method            zscore
20               Fold Generator   StratifiedKFold
21                  Fold Number                10
22                     CPU Jobs                -1
23                      Use GPU             False
24               Log Experiment             False
25              Experiment Name  clf-default-name
26                          USI              8e31
2024-11-28 17:28:45,663:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,664:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,732:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,732:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-28 17:28:45,733:INFO:setup() successfully completed in 0.9s...............
2024-11-28 17:28:45,761:INFO:Initializing compare_models()
2024-11-28 17:28:45,761:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-28 17:28:45,761:INFO:Checking exceptions
2024-11-28 17:28:45,766:INFO:Preparing display monitor
2024-11-28 17:28:45,798:INFO:Initializing Logistic Regression
2024-11-28 17:28:45,798:INFO:Total runtime is 0.0 minutes
2024-11-28 17:28:45,803:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:45,803:INFO:Initializing create_model()
2024-11-28 17:28:45,804:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:45,804:INFO:Checking exceptions
2024-11-28 17:28:45,804:INFO:Importing libraries
2024-11-28 17:28:45,804:INFO:Copying training dataset
2024-11-28 17:28:45,810:INFO:Defining folds
2024-11-28 17:28:45,810:INFO:Declaring metric variables
2024-11-28 17:28:45,820:INFO:Importing untrained model
2024-11-28 17:28:45,829:INFO:Logistic Regression Imported successfully
2024-11-28 17:28:45,843:INFO:Starting cross validation
2024-11-28 17:28:45,846:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:51,304:INFO:Calculating mean and std
2024-11-28 17:28:51,305:INFO:Creating metrics dataframe
2024-11-28 17:28:51,308:INFO:Uploading results into container
2024-11-28 17:28:51,309:INFO:Uploading model into container now
2024-11-28 17:28:51,310:INFO:_master_model_container: 1
2024-11-28 17:28:51,310:INFO:_display_container: 2
2024-11-28 17:28:51,310:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 17:28:51,310:INFO:create_model() successfully completed......................................
2024-11-28 17:28:51,373:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:51,373:INFO:Creating metrics dataframe
2024-11-28 17:28:51,380:INFO:Initializing K Neighbors Classifier
2024-11-28 17:28:51,380:INFO:Total runtime is 0.09303602377573648 minutes
2024-11-28 17:28:51,384:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:51,384:INFO:Initializing create_model()
2024-11-28 17:28:51,384:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:51,384:INFO:Checking exceptions
2024-11-28 17:28:51,384:INFO:Importing libraries
2024-11-28 17:28:51,384:INFO:Copying training dataset
2024-11-28 17:28:51,388:INFO:Defining folds
2024-11-28 17:28:51,388:INFO:Declaring metric variables
2024-11-28 17:28:51,392:INFO:Importing untrained model
2024-11-28 17:28:51,396:INFO:K Neighbors Classifier Imported successfully
2024-11-28 17:28:51,404:INFO:Starting cross validation
2024-11-28 17:28:51,408:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:51,951:INFO:Calculating mean and std
2024-11-28 17:28:51,952:INFO:Creating metrics dataframe
2024-11-28 17:28:51,955:INFO:Uploading results into container
2024-11-28 17:28:51,955:INFO:Uploading model into container now
2024-11-28 17:28:51,956:INFO:_master_model_container: 2
2024-11-28 17:28:51,956:INFO:_display_container: 2
2024-11-28 17:28:51,956:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-28 17:28:51,956:INFO:create_model() successfully completed......................................
2024-11-28 17:28:52,015:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:52,015:INFO:Creating metrics dataframe
2024-11-28 17:28:52,020:INFO:Initializing Naive Bayes
2024-11-28 17:28:52,020:INFO:Total runtime is 0.10370479424794514 minutes
2024-11-28 17:28:52,025:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:52,025:INFO:Initializing create_model()
2024-11-28 17:28:52,025:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:52,025:INFO:Checking exceptions
2024-11-28 17:28:52,025:INFO:Importing libraries
2024-11-28 17:28:52,025:INFO:Copying training dataset
2024-11-28 17:28:52,030:INFO:Defining folds
2024-11-28 17:28:52,030:INFO:Declaring metric variables
2024-11-28 17:28:52,032:INFO:Importing untrained model
2024-11-28 17:28:52,035:INFO:Naive Bayes Imported successfully
2024-11-28 17:28:52,041:INFO:Starting cross validation
2024-11-28 17:28:52,044:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:52,397:INFO:Calculating mean and std
2024-11-28 17:28:52,398:INFO:Creating metrics dataframe
2024-11-28 17:28:52,399:INFO:Uploading results into container
2024-11-28 17:28:52,400:INFO:Uploading model into container now
2024-11-28 17:28:52,400:INFO:_master_model_container: 3
2024-11-28 17:28:52,400:INFO:_display_container: 2
2024-11-28 17:28:52,400:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-28 17:28:52,400:INFO:create_model() successfully completed......................................
2024-11-28 17:28:52,465:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:52,465:INFO:Creating metrics dataframe
2024-11-28 17:28:52,471:INFO:Initializing Decision Tree Classifier
2024-11-28 17:28:52,472:INFO:Total runtime is 0.11121003627777098 minutes
2024-11-28 17:28:52,476:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:52,477:INFO:Initializing create_model()
2024-11-28 17:28:52,477:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:52,477:INFO:Checking exceptions
2024-11-28 17:28:52,477:INFO:Importing libraries
2024-11-28 17:28:52,477:INFO:Copying training dataset
2024-11-28 17:28:52,480:INFO:Defining folds
2024-11-28 17:28:52,480:INFO:Declaring metric variables
2024-11-28 17:28:52,483:INFO:Importing untrained model
2024-11-28 17:28:52,486:INFO:Decision Tree Classifier Imported successfully
2024-11-28 17:28:52,495:INFO:Starting cross validation
2024-11-28 17:28:52,496:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:52,878:INFO:Calculating mean and std
2024-11-28 17:28:52,879:INFO:Creating metrics dataframe
2024-11-28 17:28:52,881:INFO:Uploading results into container
2024-11-28 17:28:52,881:INFO:Uploading model into container now
2024-11-28 17:28:52,881:INFO:_master_model_container: 4
2024-11-28 17:28:52,881:INFO:_display_container: 2
2024-11-28 17:28:52,882:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2024-11-28 17:28:52,882:INFO:create_model() successfully completed......................................
2024-11-28 17:28:52,945:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:52,946:INFO:Creating metrics dataframe
2024-11-28 17:28:52,952:INFO:Initializing SVM - Linear Kernel
2024-11-28 17:28:52,953:INFO:Total runtime is 0.11924545367558796 minutes
2024-11-28 17:28:52,957:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:52,957:INFO:Initializing create_model()
2024-11-28 17:28:52,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:52,957:INFO:Checking exceptions
2024-11-28 17:28:52,957:INFO:Importing libraries
2024-11-28 17:28:52,958:INFO:Copying training dataset
2024-11-28 17:28:52,961:INFO:Defining folds
2024-11-28 17:28:52,961:INFO:Declaring metric variables
2024-11-28 17:28:52,964:INFO:Importing untrained model
2024-11-28 17:28:52,966:INFO:SVM - Linear Kernel Imported successfully
2024-11-28 17:28:52,972:INFO:Starting cross validation
2024-11-28 17:28:52,974:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:53,330:INFO:Calculating mean and std
2024-11-28 17:28:53,331:INFO:Creating metrics dataframe
2024-11-28 17:28:53,332:INFO:Uploading results into container
2024-11-28 17:28:53,333:INFO:Uploading model into container now
2024-11-28 17:28:53,333:INFO:_master_model_container: 5
2024-11-28 17:28:53,333:INFO:_display_container: 2
2024-11-28 17:28:53,333:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-28 17:28:53,333:INFO:create_model() successfully completed......................................
2024-11-28 17:28:53,391:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:53,391:INFO:Creating metrics dataframe
2024-11-28 17:28:53,399:INFO:Initializing Ridge Classifier
2024-11-28 17:28:53,399:INFO:Total runtime is 0.12667781909306844 minutes
2024-11-28 17:28:53,402:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:53,403:INFO:Initializing create_model()
2024-11-28 17:28:53,403:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:53,403:INFO:Checking exceptions
2024-11-28 17:28:53,403:INFO:Importing libraries
2024-11-28 17:28:53,403:INFO:Copying training dataset
2024-11-28 17:28:53,407:INFO:Defining folds
2024-11-28 17:28:53,407:INFO:Declaring metric variables
2024-11-28 17:28:53,409:INFO:Importing untrained model
2024-11-28 17:28:53,412:INFO:Ridge Classifier Imported successfully
2024-11-28 17:28:53,418:INFO:Starting cross validation
2024-11-28 17:28:53,420:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:53,805:INFO:Calculating mean and std
2024-11-28 17:28:53,806:INFO:Creating metrics dataframe
2024-11-28 17:28:53,808:INFO:Uploading results into container
2024-11-28 17:28:53,809:INFO:Uploading model into container now
2024-11-28 17:28:53,809:INFO:_master_model_container: 6
2024-11-28 17:28:53,809:INFO:_display_container: 2
2024-11-28 17:28:53,810:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2024-11-28 17:28:53,810:INFO:create_model() successfully completed......................................
2024-11-28 17:28:53,867:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:53,868:INFO:Creating metrics dataframe
2024-11-28 17:28:53,876:INFO:Initializing Random Forest Classifier
2024-11-28 17:28:53,876:INFO:Total runtime is 0.13462595144907633 minutes
2024-11-28 17:28:53,880:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:53,880:INFO:Initializing create_model()
2024-11-28 17:28:53,881:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:53,881:INFO:Checking exceptions
2024-11-28 17:28:53,881:INFO:Importing libraries
2024-11-28 17:28:53,881:INFO:Copying training dataset
2024-11-28 17:28:53,887:INFO:Defining folds
2024-11-28 17:28:53,887:INFO:Declaring metric variables
2024-11-28 17:28:53,891:INFO:Importing untrained model
2024-11-28 17:28:53,895:INFO:Random Forest Classifier Imported successfully
2024-11-28 17:28:53,902:INFO:Starting cross validation
2024-11-28 17:28:53,905:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:54,876:INFO:Calculating mean and std
2024-11-28 17:28:54,878:INFO:Creating metrics dataframe
2024-11-28 17:28:54,879:INFO:Uploading results into container
2024-11-28 17:28:54,880:INFO:Uploading model into container now
2024-11-28 17:28:54,880:INFO:_master_model_container: 7
2024-11-28 17:28:54,881:INFO:_display_container: 2
2024-11-28 17:28:54,881:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 17:28:54,882:INFO:create_model() successfully completed......................................
2024-11-28 17:28:54,940:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:54,941:INFO:Creating metrics dataframe
2024-11-28 17:28:54,946:INFO:Initializing Quadratic Discriminant Analysis
2024-11-28 17:28:54,946:INFO:Total runtime is 0.15247281392415363 minutes
2024-11-28 17:28:54,949:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:54,949:INFO:Initializing create_model()
2024-11-28 17:28:54,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:54,949:INFO:Checking exceptions
2024-11-28 17:28:54,949:INFO:Importing libraries
2024-11-28 17:28:54,950:INFO:Copying training dataset
2024-11-28 17:28:54,954:INFO:Defining folds
2024-11-28 17:28:54,955:INFO:Declaring metric variables
2024-11-28 17:28:54,958:INFO:Importing untrained model
2024-11-28 17:28:54,960:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-28 17:28:54,972:INFO:Starting cross validation
2024-11-28 17:28:54,983:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:55,176:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,176:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,187:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,189:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,189:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,189:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,220:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,325:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,325:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-28 17:28:55,375:INFO:Calculating mean and std
2024-11-28 17:28:55,376:INFO:Creating metrics dataframe
2024-11-28 17:28:55,377:INFO:Uploading results into container
2024-11-28 17:28:55,377:INFO:Uploading model into container now
2024-11-28 17:28:55,378:INFO:_master_model_container: 8
2024-11-28 17:28:55,378:INFO:_display_container: 2
2024-11-28 17:28:55,378:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-28 17:28:55,378:INFO:create_model() successfully completed......................................
2024-11-28 17:28:55,435:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:55,435:INFO:Creating metrics dataframe
2024-11-28 17:28:55,441:INFO:Initializing Ada Boost Classifier
2024-11-28 17:28:55,441:INFO:Total runtime is 0.16072184244791665 minutes
2024-11-28 17:28:55,445:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:55,445:INFO:Initializing create_model()
2024-11-28 17:28:55,445:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:55,446:INFO:Checking exceptions
2024-11-28 17:28:55,446:INFO:Importing libraries
2024-11-28 17:28:55,446:INFO:Copying training dataset
2024-11-28 17:28:55,451:INFO:Defining folds
2024-11-28 17:28:55,451:INFO:Declaring metric variables
2024-11-28 17:28:55,453:INFO:Importing untrained model
2024-11-28 17:28:55,456:INFO:Ada Boost Classifier Imported successfully
2024-11-28 17:28:55,462:INFO:Starting cross validation
2024-11-28 17:28:55,464:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:55,640:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,640:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,640:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,640:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,649:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,655:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,655:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,686:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,950:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:55,953:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-28 17:28:56,083:INFO:Calculating mean and std
2024-11-28 17:28:56,084:INFO:Creating metrics dataframe
2024-11-28 17:28:56,086:INFO:Uploading results into container
2024-11-28 17:28:56,086:INFO:Uploading model into container now
2024-11-28 17:28:56,086:INFO:_master_model_container: 9
2024-11-28 17:28:56,086:INFO:_display_container: 2
2024-11-28 17:28:56,087:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2024-11-28 17:28:56,087:INFO:create_model() successfully completed......................................
2024-11-28 17:28:56,139:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:56,140:INFO:Creating metrics dataframe
2024-11-28 17:28:56,150:INFO:Initializing Gradient Boosting Classifier
2024-11-28 17:28:56,150:INFO:Total runtime is 0.17252569595972694 minutes
2024-11-28 17:28:56,153:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:56,154:INFO:Initializing create_model()
2024-11-28 17:28:56,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:56,154:INFO:Checking exceptions
2024-11-28 17:28:56,154:INFO:Importing libraries
2024-11-28 17:28:56,154:INFO:Copying training dataset
2024-11-28 17:28:56,157:INFO:Defining folds
2024-11-28 17:28:56,157:INFO:Declaring metric variables
2024-11-28 17:28:56,160:INFO:Importing untrained model
2024-11-28 17:28:56,163:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 17:28:56,169:INFO:Starting cross validation
2024-11-28 17:28:56,171:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:56,920:INFO:Calculating mean and std
2024-11-28 17:28:56,922:INFO:Creating metrics dataframe
2024-11-28 17:28:56,924:INFO:Uploading results into container
2024-11-28 17:28:56,925:INFO:Uploading model into container now
2024-11-28 17:28:56,925:INFO:_master_model_container: 10
2024-11-28 17:28:56,925:INFO:_display_container: 2
2024-11-28 17:28:56,926:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 17:28:56,926:INFO:create_model() successfully completed......................................
2024-11-28 17:28:56,984:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:56,984:INFO:Creating metrics dataframe
2024-11-28 17:28:56,990:INFO:Initializing Linear Discriminant Analysis
2024-11-28 17:28:56,991:INFO:Total runtime is 0.18653165102005 minutes
2024-11-28 17:28:56,995:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:56,995:INFO:Initializing create_model()
2024-11-28 17:28:56,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:56,995:INFO:Checking exceptions
2024-11-28 17:28:56,995:INFO:Importing libraries
2024-11-28 17:28:56,995:INFO:Copying training dataset
2024-11-28 17:28:56,999:INFO:Defining folds
2024-11-28 17:28:56,999:INFO:Declaring metric variables
2024-11-28 17:28:57,002:INFO:Importing untrained model
2024-11-28 17:28:57,006:INFO:Linear Discriminant Analysis Imported successfully
2024-11-28 17:28:57,012:INFO:Starting cross validation
2024-11-28 17:28:57,015:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:57,414:INFO:Calculating mean and std
2024-11-28 17:28:57,415:INFO:Creating metrics dataframe
2024-11-28 17:28:57,416:INFO:Uploading results into container
2024-11-28 17:28:57,417:INFO:Uploading model into container now
2024-11-28 17:28:57,417:INFO:_master_model_container: 11
2024-11-28 17:28:57,417:INFO:_display_container: 2
2024-11-28 17:28:57,417:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-28 17:28:57,417:INFO:create_model() successfully completed......................................
2024-11-28 17:28:57,488:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:57,488:INFO:Creating metrics dataframe
2024-11-28 17:28:57,501:INFO:Initializing Extra Trees Classifier
2024-11-28 17:28:57,501:INFO:Total runtime is 0.19505599737167353 minutes
2024-11-28 17:28:57,505:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:57,506:INFO:Initializing create_model()
2024-11-28 17:28:57,506:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:57,506:INFO:Checking exceptions
2024-11-28 17:28:57,506:INFO:Importing libraries
2024-11-28 17:28:57,506:INFO:Copying training dataset
2024-11-28 17:28:57,511:INFO:Defining folds
2024-11-28 17:28:57,511:INFO:Declaring metric variables
2024-11-28 17:28:57,517:INFO:Importing untrained model
2024-11-28 17:28:57,520:INFO:Extra Trees Classifier Imported successfully
2024-11-28 17:28:57,532:INFO:Starting cross validation
2024-11-28 17:28:57,534:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:58,435:INFO:Calculating mean and std
2024-11-28 17:28:58,435:INFO:Creating metrics dataframe
2024-11-28 17:28:58,437:INFO:Uploading results into container
2024-11-28 17:28:58,438:INFO:Uploading model into container now
2024-11-28 17:28:58,439:INFO:_master_model_container: 12
2024-11-28 17:28:58,439:INFO:_display_container: 2
2024-11-28 17:28:58,439:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2024-11-28 17:28:58,439:INFO:create_model() successfully completed......................................
2024-11-28 17:28:58,496:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:58,496:INFO:Creating metrics dataframe
2024-11-28 17:28:58,505:INFO:Initializing Light Gradient Boosting Machine
2024-11-28 17:28:58,506:INFO:Total runtime is 0.21180321375528965 minutes
2024-11-28 17:28:58,509:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:58,509:INFO:Initializing create_model()
2024-11-28 17:28:58,509:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:58,509:INFO:Checking exceptions
2024-11-28 17:28:58,509:INFO:Importing libraries
2024-11-28 17:28:58,509:INFO:Copying training dataset
2024-11-28 17:28:58,513:INFO:Defining folds
2024-11-28 17:28:58,513:INFO:Declaring metric variables
2024-11-28 17:28:58,516:INFO:Importing untrained model
2024-11-28 17:28:58,518:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:28:58,526:INFO:Starting cross validation
2024-11-28 17:28:58,527:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:28:59,799:INFO:Calculating mean and std
2024-11-28 17:28:59,801:INFO:Creating metrics dataframe
2024-11-28 17:28:59,803:INFO:Uploading results into container
2024-11-28 17:28:59,804:INFO:Uploading model into container now
2024-11-28 17:28:59,804:INFO:_master_model_container: 13
2024-11-28 17:28:59,804:INFO:_display_container: 2
2024-11-28 17:28:59,804:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:28:59,804:INFO:create_model() successfully completed......................................
2024-11-28 17:28:59,867:INFO:SubProcess create_model() end ==================================
2024-11-28 17:28:59,867:INFO:Creating metrics dataframe
2024-11-28 17:28:59,894:INFO:Initializing Dummy Classifier
2024-11-28 17:28:59,894:INFO:Total runtime is 0.2349309086799621 minutes
2024-11-28 17:28:59,896:INFO:SubProcess create_model() called ==================================
2024-11-28 17:28:59,897:INFO:Initializing create_model()
2024-11-28 17:28:59,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3B35FF0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:28:59,897:INFO:Checking exceptions
2024-11-28 17:28:59,897:INFO:Importing libraries
2024-11-28 17:28:59,897:INFO:Copying training dataset
2024-11-28 17:28:59,903:INFO:Defining folds
2024-11-28 17:28:59,903:INFO:Declaring metric variables
2024-11-28 17:28:59,907:INFO:Importing untrained model
2024-11-28 17:28:59,942:INFO:Dummy Classifier Imported successfully
2024-11-28 17:28:59,958:INFO:Starting cross validation
2024-11-28 17:28:59,960:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:00,243:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,256:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,263:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,267:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,277:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,282:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,288:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,301:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,384:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,391:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-28 17:29:00,396:INFO:Calculating mean and std
2024-11-28 17:29:00,397:INFO:Creating metrics dataframe
2024-11-28 17:29:00,398:INFO:Uploading results into container
2024-11-28 17:29:00,399:INFO:Uploading model into container now
2024-11-28 17:29:00,400:INFO:_master_model_container: 14
2024-11-28 17:29:00,400:INFO:_display_container: 2
2024-11-28 17:29:00,400:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2024-11-28 17:29:00,400:INFO:create_model() successfully completed......................................
2024-11-28 17:29:00,457:INFO:SubProcess create_model() end ==================================
2024-11-28 17:29:00,458:INFO:Creating metrics dataframe
2024-11-28 17:29:00,472:WARNING:C:\Users\alame\AppData\Roaming\Python\Python310\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-28 17:29:00,479:INFO:Initializing create_model()
2024-11-28 17:29:00,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:00,479:INFO:Checking exceptions
2024-11-28 17:29:00,481:INFO:Importing libraries
2024-11-28 17:29:00,481:INFO:Copying training dataset
2024-11-28 17:29:00,486:INFO:Defining folds
2024-11-28 17:29:00,486:INFO:Declaring metric variables
2024-11-28 17:29:00,486:INFO:Importing untrained model
2024-11-28 17:29:00,486:INFO:Declaring custom model
2024-11-28 17:29:00,487:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:29:00,488:INFO:Cross validation set to False
2024-11-28 17:29:00,488:INFO:Fitting Model
2024-11-28 17:29:00,563:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:29:00,563:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 17:29:00,564:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000217 seconds.
2024-11-28 17:29:00,564:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:29:00,564:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:29:00,564:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 17:29:00,564:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 17:29:00,565:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 17:29:00,565:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 17:29:00,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,584:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:00,611:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:00,611:INFO:create_model() successfully completed......................................
2024-11-28 17:29:00,720:INFO:_master_model_container: 14
2024-11-28 17:29:00,720:INFO:_display_container: 2
2024-11-28 17:29:00,720:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:00,720:INFO:compare_models() successfully completed......................................
2024-11-28 17:29:00,779:INFO:Initializing tune_model()
2024-11-28 17:29:00,779:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>)
2024-11-28 17:29:00,779:INFO:Checking exceptions
2024-11-28 17:29:00,794:INFO:Copying training dataset
2024-11-28 17:29:00,798:INFO:Checking base model
2024-11-28 17:29:00,799:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 17:29:00,803:INFO:Declaring metric variables
2024-11-28 17:29:00,807:INFO:Defining Hyperparameters
2024-11-28 17:29:00,870:INFO:Tuning with n_jobs=-1
2024-11-28 17:29:00,870:INFO:Initializing RandomizedSearchCV
2024-11-28 17:29:09,330:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 17:29:09,330:INFO:Hyperparameter search completed
2024-11-28 17:29:09,330:INFO:SubProcess create_model() called ==================================
2024-11-28 17:29:09,330:INFO:Initializing create_model()
2024-11-28 17:29:09,330:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247C037D2D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 17:29:09,330:INFO:Checking exceptions
2024-11-28 17:29:09,330:INFO:Importing libraries
2024-11-28 17:29:09,330:INFO:Copying training dataset
2024-11-28 17:29:09,345:INFO:Defining folds
2024-11-28 17:29:09,345:INFO:Declaring metric variables
2024-11-28 17:29:09,345:INFO:Importing untrained model
2024-11-28 17:29:09,345:INFO:Declaring custom model
2024-11-28 17:29:09,345:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:29:09,363:INFO:Starting cross validation
2024-11-28 17:29:09,368:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:10,207:INFO:Calculating mean and std
2024-11-28 17:29:10,207:INFO:Creating metrics dataframe
2024-11-28 17:29:10,215:INFO:Finalizing model
2024-11-28 17:29:10,320:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 17:29:10,320:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 17:29:10,320:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 17:29:10,322:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:29:10,323:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 17:29:10,323:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 17:29:10,323:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 17:29:10,323:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 17:29:10,324:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000316 seconds.
2024-11-28 17:29:10,324:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:29:10,324:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:29:10,324:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 17:29:10,324:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 17:29:10,325:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 17:29:10,325:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 17:29:10,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,343:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,343:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,344:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,346:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,348:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,348:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,348:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,349:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,350:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,351:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,352:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,353:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,353:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:10,353:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:29:10,364:INFO:Uploading results into container
2024-11-28 17:29:10,365:INFO:Uploading model into container now
2024-11-28 17:29:10,367:INFO:_master_model_container: 15
2024-11-28 17:29:10,367:INFO:_display_container: 3
2024-11-28 17:29:10,368:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:10,368:INFO:create_model() successfully completed......................................
2024-11-28 17:29:10,470:INFO:SubProcess create_model() end ==================================
2024-11-28 17:29:10,470:INFO:choose_better activated
2024-11-28 17:29:10,477:INFO:SubProcess create_model() called ==================================
2024-11-28 17:29:10,477:INFO:Initializing create_model()
2024-11-28 17:29:10,478:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:10,478:INFO:Checking exceptions
2024-11-28 17:29:10,480:INFO:Importing libraries
2024-11-28 17:29:10,480:INFO:Copying training dataset
2024-11-28 17:29:10,485:INFO:Defining folds
2024-11-28 17:29:10,485:INFO:Declaring metric variables
2024-11-28 17:29:10,485:INFO:Importing untrained model
2024-11-28 17:29:10,485:INFO:Declaring custom model
2024-11-28 17:29:10,486:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:29:10,486:INFO:Starting cross validation
2024-11-28 17:29:10,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:11,870:INFO:Calculating mean and std
2024-11-28 17:29:11,870:INFO:Creating metrics dataframe
2024-11-28 17:29:11,870:INFO:Finalizing model
2024-11-28 17:29:11,980:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:29:11,981:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 17:29:11,982:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.
2024-11-28 17:29:11,982:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:29:11,982:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:29:11,982:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 17:29:11,982:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 17:29:11,983:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 17:29:11,983:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 17:29:11,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:11,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:12,033:INFO:Uploading results into container
2024-11-28 17:29:12,034:INFO:Uploading model into container now
2024-11-28 17:29:12,035:INFO:_master_model_container: 16
2024-11-28 17:29:12,035:INFO:_display_container: 4
2024-11-28 17:29:12,035:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:12,035:INFO:create_model() successfully completed......................................
2024-11-28 17:29:12,115:INFO:SubProcess create_model() end ==================================
2024-11-28 17:29:12,116:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 17:29:12,116:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 17:29:12,117:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 17:29:12,117:INFO:choose_better completed
2024-11-28 17:29:12,118:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 17:29:12,129:INFO:_master_model_container: 16
2024-11-28 17:29:12,129:INFO:_display_container: 3
2024-11-28 17:29:12,130:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:12,130:INFO:tune_model() successfully completed......................................
2024-11-28 17:29:12,207:INFO:Initializing evaluate_model()
2024-11-28 17:29:12,207:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-11-28 17:29:12,226:INFO:Initializing plot_model()
2024-11-28 17:29:12,226:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, system=True)
2024-11-28 17:29:12,227:INFO:Checking exceptions
2024-11-28 17:29:12,229:INFO:Preloading libraries
2024-11-28 17:29:12,240:INFO:Copying training dataset
2024-11-28 17:29:12,240:INFO:Plot type: pipeline
2024-11-28 17:29:12,445:INFO:Visual Rendered Successfully
2024-11-28 17:29:12,503:INFO:plot_model() successfully completed......................................
2024-11-28 17:29:12,537:INFO:Initializing plot_model()
2024-11-28 17:29:12,538:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, system=True)
2024-11-28 17:29:12,538:INFO:Checking exceptions
2024-11-28 17:29:12,557:INFO:Preloading libraries
2024-11-28 17:29:12,593:INFO:Copying training dataset
2024-11-28 17:29:12,593:INFO:Plot type: feature
2024-11-28 17:29:12,594:WARNING:No coef_ found. Trying feature_importances_
2024-11-28 17:29:12,829:INFO:Visual Rendered Successfully
2024-11-28 17:29:12,886:INFO:plot_model() successfully completed......................................
2024-11-28 17:29:12,914:INFO:Initializing create_model()
2024-11-28 17:29:12,915:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=lr, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:12,915:INFO:Checking exceptions
2024-11-28 17:29:12,930:INFO:Importing libraries
2024-11-28 17:29:12,931:INFO:Copying training dataset
2024-11-28 17:29:12,937:INFO:Defining folds
2024-11-28 17:29:12,937:INFO:Declaring metric variables
2024-11-28 17:29:12,941:INFO:Importing untrained model
2024-11-28 17:29:12,945:INFO:Logistic Regression Imported successfully
2024-11-28 17:29:12,953:INFO:Starting cross validation
2024-11-28 17:29:12,955:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:13,376:INFO:Calculating mean and std
2024-11-28 17:29:13,376:INFO:Creating metrics dataframe
2024-11-28 17:29:13,380:INFO:Finalizing model
2024-11-28 17:29:13,471:INFO:Uploading results into container
2024-11-28 17:29:13,472:INFO:Uploading model into container now
2024-11-28 17:29:13,481:INFO:_master_model_container: 17
2024-11-28 17:29:13,481:INFO:_display_container: 4
2024-11-28 17:29:13,481:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-28 17:29:13,481:INFO:create_model() successfully completed......................................
2024-11-28 17:29:13,542:INFO:Initializing create_model()
2024-11-28 17:29:13,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:13,542:INFO:Checking exceptions
2024-11-28 17:29:13,555:INFO:Importing libraries
2024-11-28 17:29:13,555:INFO:Copying training dataset
2024-11-28 17:29:13,562:INFO:Defining folds
2024-11-28 17:29:13,562:INFO:Declaring metric variables
2024-11-28 17:29:13,571:INFO:Importing untrained model
2024-11-28 17:29:13,582:INFO:Random Forest Classifier Imported successfully
2024-11-28 17:29:13,604:INFO:Starting cross validation
2024-11-28 17:29:13,605:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:14,628:INFO:Calculating mean and std
2024-11-28 17:29:14,630:INFO:Creating metrics dataframe
2024-11-28 17:29:14,635:INFO:Finalizing model
2024-11-28 17:29:14,851:INFO:Uploading results into container
2024-11-28 17:29:14,852:INFO:Uploading model into container now
2024-11-28 17:29:14,861:INFO:_master_model_container: 18
2024-11-28 17:29:14,861:INFO:_display_container: 5
2024-11-28 17:29:14,862:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2024-11-28 17:29:14,862:INFO:create_model() successfully completed......................................
2024-11-28 17:29:14,925:INFO:Initializing create_model()
2024-11-28 17:29:14,925:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=gbc, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:14,925:INFO:Checking exceptions
2024-11-28 17:29:14,937:INFO:Importing libraries
2024-11-28 17:29:14,937:INFO:Copying training dataset
2024-11-28 17:29:14,945:INFO:Defining folds
2024-11-28 17:29:14,945:INFO:Declaring metric variables
2024-11-28 17:29:14,948:INFO:Importing untrained model
2024-11-28 17:29:14,952:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 17:29:14,960:INFO:Starting cross validation
2024-11-28 17:29:14,962:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:15,792:INFO:Calculating mean and std
2024-11-28 17:29:15,793:INFO:Creating metrics dataframe
2024-11-28 17:29:15,799:INFO:Finalizing model
2024-11-28 17:29:15,989:INFO:Uploading results into container
2024-11-28 17:29:15,990:INFO:Uploading model into container now
2024-11-28 17:29:16,001:INFO:_master_model_container: 19
2024-11-28 17:29:16,001:INFO:_display_container: 6
2024-11-28 17:29:16,001:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 17:29:16,001:INFO:create_model() successfully completed......................................
2024-11-28 17:29:16,069:INFO:Initializing create_model()
2024-11-28 17:29:16,069:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:16,069:INFO:Checking exceptions
2024-11-28 17:29:16,082:INFO:Importing libraries
2024-11-28 17:29:16,082:INFO:Copying training dataset
2024-11-28 17:29:16,087:INFO:Defining folds
2024-11-28 17:29:16,088:INFO:Declaring metric variables
2024-11-28 17:29:16,093:INFO:Importing untrained model
2024-11-28 17:29:16,096:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:29:16,102:INFO:Starting cross validation
2024-11-28 17:29:16,104:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:29:17,571:INFO:Calculating mean and std
2024-11-28 17:29:17,573:INFO:Creating metrics dataframe
2024-11-28 17:29:17,582:INFO:Finalizing model
2024-11-28 17:29:17,687:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:29:17,687:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 17:29:17,688:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000290 seconds.
2024-11-28 17:29:17,688:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:29:17,688:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:29:17,688:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 17:29:17,689:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 17:29:17,689:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 17:29:17,689:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 17:29:17,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:17,751:INFO:Uploading results into container
2024-11-28 17:29:17,752:INFO:Uploading model into container now
2024-11-28 17:29:17,765:INFO:_master_model_container: 20
2024-11-28 17:29:17,766:INFO:_display_container: 7
2024-11-28 17:29:17,766:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:17,766:INFO:create_model() successfully completed......................................
2024-11-28 17:29:17,966:INFO:Initializing predict_model()
2024-11-28 17:29:17,966:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000247C035FB50>)
2024-11-28 17:29:17,966:INFO:Checking exceptions
2024-11-28 17:29:17,966:INFO:Preloading libraries
2024-11-28 17:29:17,969:INFO:Set up data.
2024-11-28 17:29:17,979:INFO:Set up index.
2024-11-28 17:29:26,985:INFO:Initializing predict_model()
2024-11-28 17:29:26,985:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000247E3D8F010>)
2024-11-28 17:29:26,985:INFO:Checking exceptions
2024-11-28 17:29:26,985:INFO:Preloading libraries
2024-11-28 17:29:26,988:INFO:Set up data.
2024-11-28 17:29:26,998:INFO:Set up index.
2024-11-28 17:29:36,394:INFO:Initializing finalize_model()
2024-11-28 17:29:36,394:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-11-28 17:29:36,396:INFO:Finalizing LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:29:36,399:INFO:Initializing create_model()
2024-11-28 17:29:36,399:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:29:36,399:INFO:Checking exceptions
2024-11-28 17:29:36,401:INFO:Importing libraries
2024-11-28 17:29:36,402:INFO:Copying training dataset
2024-11-28 17:29:36,402:INFO:Defining folds
2024-11-28 17:29:36,402:INFO:Declaring metric variables
2024-11-28 17:29:36,402:INFO:Importing untrained model
2024-11-28 17:29:36,402:INFO:Declaring custom model
2024-11-28 17:29:36,403:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:29:36,404:INFO:Cross validation set to False
2024-11-28 17:29:36,404:INFO:Fitting Model
2024-11-28 17:29:36,475:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:29:36,475:INFO:[LightGBM] [Info] Number of positive: 342, number of negative: 549
2024-11-28 17:29:36,476:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.
2024-11-28 17:29:36,476:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:29:36,476:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:29:36,476:INFO:[LightGBM] [Info] Total Bins 423
2024-11-28 17:29:36,476:INFO:[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 23
2024-11-28 17:29:36,476:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288
2024-11-28 17:29:36,476:INFO:[LightGBM] [Info] Start training from score -0.473288
2024-11-28 17:29:36,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:29:36,557:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 17:29:36,557:INFO:create_model() successfully completed......................................
2024-11-28 17:29:36,635:INFO:_master_model_container: 20
2024-11-28 17:29:36,636:INFO:_display_container: 9
2024-11-28 17:29:36,644:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 17:29:36,644:INFO:finalize_model() successfully completed......................................
2024-11-28 17:29:36,708:INFO:Initializing save_model()
2024-11-28 17:29:36,708:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), model_name=../Titanic/data/titanic/final_tuned_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\alame\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_value...
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-28 17:29:36,708:INFO:Adding model into prep_pipe
2024-11-28 17:29:36,708:WARNING:Only Model saved as it was a pipeline.
2024-11-28 17:29:36,726:INFO:../Titanic/data/titanic/final_tuned_model.pkl saved in current working directory
2024-11-28 17:29:36,734:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-11-28 17:29:36,734:INFO:save_model() successfully completed......................................
2024-11-28 17:29:40,247:INFO:Initializing predict_model()
2024-11-28 17:29:40,247:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Pclass', 'Sex', 'Age', 'SibSp',
                                             'Parch', 'Fare', 'Embarked',
                                             'Title', 'FamilySize',
                                             'FarePerClass'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_i...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000247E4118790>)
2024-11-28 17:29:40,248:INFO:Checking exceptions
2024-11-28 17:29:40,248:INFO:Preloading libraries
2024-11-28 17:29:40,251:INFO:Set up data.
2024-11-28 17:29:40,258:INFO:Set up index.
2024-11-28 17:30:07,868:INFO:Initializing tune_model()
2024-11-28 17:30:07,868:INFO:tune_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>)
2024-11-28 17:30:07,868:INFO:Checking exceptions
2024-11-28 17:30:07,886:INFO:Copying training dataset
2024-11-28 17:30:07,891:INFO:Checking base model
2024-11-28 17:30:07,891:INFO:Base model : Gradient Boosting Classifier
2024-11-28 17:30:07,894:INFO:Declaring metric variables
2024-11-28 17:30:07,896:INFO:Defining Hyperparameters
2024-11-28 17:30:07,970:INFO:Tuning with n_jobs=-1
2024-11-28 17:30:07,971:INFO:Initializing RandomizedSearchCV
2024-11-28 17:30:14,903:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__n_estimators': 270, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.3, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 6, 'actual_estimator__learning_rate': 0.05}
2024-11-28 17:30:14,903:INFO:Hyperparameter search completed
2024-11-28 17:30:14,904:INFO:SubProcess create_model() called ==================================
2024-11-28 17:30:14,904:INFO:Initializing create_model()
2024-11-28 17:30:14,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247C0510CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'n_estimators': 270, 'min_samples_split': 10, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.3, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.05})
2024-11-28 17:30:14,904:INFO:Checking exceptions
2024-11-28 17:30:14,904:INFO:Importing libraries
2024-11-28 17:30:14,904:INFO:Copying training dataset
2024-11-28 17:30:14,909:INFO:Defining folds
2024-11-28 17:30:14,909:INFO:Declaring metric variables
2024-11-28 17:30:14,913:INFO:Importing untrained model
2024-11-28 17:30:14,913:INFO:Declaring custom model
2024-11-28 17:30:14,917:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 17:30:14,924:INFO:Starting cross validation
2024-11-28 17:30:14,926:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:30:15,802:INFO:Calculating mean and std
2024-11-28 17:30:15,802:INFO:Creating metrics dataframe
2024-11-28 17:30:15,806:INFO:Finalizing model
2024-11-28 17:30:16,037:INFO:Uploading results into container
2024-11-28 17:30:16,038:INFO:Uploading model into container now
2024-11-28 17:30:16,038:INFO:_master_model_container: 21
2024-11-28 17:30:16,038:INFO:_display_container: 10
2024-11-28 17:30:16,039:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 17:30:16,039:INFO:create_model() successfully completed......................................
2024-11-28 17:30:16,104:INFO:SubProcess create_model() end ==================================
2024-11-28 17:30:16,104:INFO:choose_better activated
2024-11-28 17:30:16,107:INFO:SubProcess create_model() called ==================================
2024-11-28 17:30:16,107:INFO:Initializing create_model()
2024-11-28 17:30:16,107:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:30:16,107:INFO:Checking exceptions
2024-11-28 17:30:16,108:INFO:Importing libraries
2024-11-28 17:30:16,109:INFO:Copying training dataset
2024-11-28 17:30:16,114:INFO:Defining folds
2024-11-28 17:30:16,114:INFO:Declaring metric variables
2024-11-28 17:30:16,114:INFO:Importing untrained model
2024-11-28 17:30:16,114:INFO:Declaring custom model
2024-11-28 17:30:16,114:INFO:Gradient Boosting Classifier Imported successfully
2024-11-28 17:30:16,115:INFO:Starting cross validation
2024-11-28 17:30:16,116:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:30:16,836:INFO:Calculating mean and std
2024-11-28 17:30:16,836:INFO:Creating metrics dataframe
2024-11-28 17:30:16,838:INFO:Finalizing model
2024-11-28 17:30:17,064:INFO:Uploading results into container
2024-11-28 17:30:17,065:INFO:Uploading model into container now
2024-11-28 17:30:17,065:INFO:_master_model_container: 22
2024-11-28 17:30:17,065:INFO:_display_container: 11
2024-11-28 17:30:17,065:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 17:30:17,065:INFO:create_model() successfully completed......................................
2024-11-28 17:30:17,125:INFO:SubProcess create_model() end ==================================
2024-11-28 17:30:17,126:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8234
2024-11-28 17:30:17,126:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8395
2024-11-28 17:30:17,126:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2024-11-28 17:30:17,126:INFO:choose_better completed
2024-11-28 17:30:17,135:INFO:_master_model_container: 22
2024-11-28 17:30:17,135:INFO:_display_container: 10
2024-11-28 17:30:17,135:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=6,
                           max_features='sqrt', max_leaf_nodes=None,
                           min_impurity_decrease=0.3, min_samples_leaf=4,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=270, n_iter_no_change=None,
                           random_state=42, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-28 17:30:17,135:INFO:tune_model() successfully completed......................................
2024-11-28 17:30:17,211:INFO:Initializing tune_model()
2024-11-28 17:30:17,211:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>)
2024-11-28 17:30:17,211:INFO:Checking exceptions
2024-11-28 17:30:17,226:INFO:Copying training dataset
2024-11-28 17:30:17,230:INFO:Checking base model
2024-11-28 17:30:17,230:INFO:Base model : Light Gradient Boosting Machine
2024-11-28 17:30:17,234:INFO:Declaring metric variables
2024-11-28 17:30:17,238:INFO:Defining Hyperparameters
2024-11-28 17:30:17,316:INFO:Tuning with n_jobs=-1
2024-11-28 17:30:17,316:INFO:Initializing RandomizedSearchCV
2024-11-28 17:30:25,287:INFO:best_params: {'actual_estimator__reg_lambda': 5, 'actual_estimator__reg_alpha': 0.001, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 100, 'actual_estimator__min_split_gain': 0.6, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2024-11-28 17:30:25,287:INFO:Hyperparameter search completed
2024-11-28 17:30:25,287:INFO:SubProcess create_model() called ==================================
2024-11-28 17:30:25,287:INFO:Initializing create_model()
2024-11-28 17:30:25,287:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000247E3C9AE60>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 5, 'reg_alpha': 0.001, 'num_leaves': 30, 'n_estimators': 100, 'min_split_gain': 0.6, 'min_child_samples': 6, 'learning_rate': 0.2, 'feature_fraction': 0.8, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2024-11-28 17:30:25,287:INFO:Checking exceptions
2024-11-28 17:30:25,287:INFO:Importing libraries
2024-11-28 17:30:25,287:INFO:Copying training dataset
2024-11-28 17:30:25,297:INFO:Defining folds
2024-11-28 17:30:25,300:INFO:Declaring metric variables
2024-11-28 17:30:25,303:INFO:Importing untrained model
2024-11-28 17:30:25,303:INFO:Declaring custom model
2024-11-28 17:30:25,303:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:30:25,303:INFO:Starting cross validation
2024-11-28 17:30:25,320:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:30:26,049:INFO:Calculating mean and std
2024-11-28 17:30:26,051:INFO:Creating metrics dataframe
2024-11-28 17:30:26,057:INFO:Finalizing model
2024-11-28 17:30:26,178:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 17:30:26,178:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 17:30:26,178:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 17:30:26,181:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:30:26,182:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2024-11-28 17:30:26,182:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2024-11-28 17:30:26,182:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2024-11-28 17:30:26,182:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 17:30:26,183:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000342 seconds.
2024-11-28 17:30:26,183:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:30:26,183:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:30:26,183:INFO:[LightGBM] [Info] Total Bins 368
2024-11-28 17:30:26,183:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 23
2024-11-28 17:30:26,183:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 17:30:26,183:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 17:30:26,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,209:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,209:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,211:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,212:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,214:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,216:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:26,218:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-11-28 17:30:26,231:INFO:Uploading results into container
2024-11-28 17:30:26,232:INFO:Uploading model into container now
2024-11-28 17:30:26,233:INFO:_master_model_container: 23
2024-11-28 17:30:26,233:INFO:_display_container: 11
2024-11-28 17:30:26,234:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:30:26,234:INFO:create_model() successfully completed......................................
2024-11-28 17:30:26,323:INFO:SubProcess create_model() end ==================================
2024-11-28 17:30:26,323:INFO:choose_better activated
2024-11-28 17:30:26,325:INFO:SubProcess create_model() called ==================================
2024-11-28 17:30:26,325:INFO:Initializing create_model()
2024-11-28 17:30:26,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-28 17:30:26,326:INFO:Checking exceptions
2024-11-28 17:30:26,327:INFO:Importing libraries
2024-11-28 17:30:26,328:INFO:Copying training dataset
2024-11-28 17:30:26,331:INFO:Defining folds
2024-11-28 17:30:26,332:INFO:Declaring metric variables
2024-11-28 17:30:26,332:INFO:Importing untrained model
2024-11-28 17:30:26,332:INFO:Declaring custom model
2024-11-28 17:30:26,332:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-28 17:30:26,333:INFO:Starting cross validation
2024-11-28 17:30:26,334:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-28 17:30:27,629:INFO:Calculating mean and std
2024-11-28 17:30:27,629:INFO:Creating metrics dataframe
2024-11-28 17:30:27,629:INFO:Finalizing model
2024-11-28 17:30:27,736:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-11-28 17:30:27,736:INFO:[LightGBM] [Info] Number of positive: 239, number of negative: 384
2024-11-28 17:30:27,737:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000224 seconds.
2024-11-28 17:30:27,737:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-11-28 17:30:27,737:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-11-28 17:30:27,737:INFO:[LightGBM] [Info] Total Bins 362
2024-11-28 17:30:27,737:INFO:[LightGBM] [Info] Number of data points in the train set: 623, number of used features: 21
2024-11-28 17:30:27,738:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383628 -> initscore=-0.474179
2024-11-28 17:30:27,738:INFO:[LightGBM] [Info] Start training from score -0.474179
2024-11-28 17:30:27,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-11-28 17:30:27,836:INFO:Uploading results into container
2024-11-28 17:30:27,837:INFO:Uploading model into container now
2024-11-28 17:30:27,838:INFO:_master_model_container: 24
2024-11-28 17:30:27,838:INFO:_display_container: 12
2024-11-28 17:30:27,838:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:30:27,838:INFO:create_model() successfully completed......................................
2024-11-28 17:30:27,921:INFO:SubProcess create_model() end ==================================
2024-11-28 17:30:27,922:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 17:30:27,923:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.6,
               n_estimators=100, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.001, reg_lambda=5, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8379
2024-11-28 17:30:27,923:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-11-28 17:30:27,923:INFO:choose_better completed
2024-11-28 17:30:27,923:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-11-28 17:30:27,935:INFO:_master_model_container: 24
2024-11-28 17:30:27,935:INFO:_display_container: 11
2024-11-28 17:30:27,936:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-28 17:30:27,936:INFO:tune_model() successfully completed......................................
2024-11-28 17:30:28,021:INFO:Initializing predict_model()
2024-11-28 17:30:28,021:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000247C0510EE0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000247E3F55EA0>)
2024-11-28 17:30:28,021:INFO:Checking exceptions
2024-11-28 17:30:28,021:INFO:Preloading libraries
2024-11-28 17:30:28,023:INFO:Set up data.
2024-11-28 17:30:28,032:INFO:Set up index.
